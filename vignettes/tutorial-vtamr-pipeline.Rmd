---
title: "Tutorial - vtamR pipeline"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Tutorial - vtamR pipeline}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  echo=TRUE,
  eval=TRUE,
  fig.width = 6,
  out.width = '100%',
  comment = "#>"
)
```

# Summary

**vtamR** is a revised, completed version of 
[VTAM](https://www.csbj.org/article/S2001-0370(23)00034-X/fulltext) 
(Validation and Taxonomic Assignation of Metabarcoding Data) 
rewritten in R. It is a complete metabarcoding pipeline:

* Sequence analyses from raw fastq files of amplicon sequences till 
Amplicon Sequence Variant ([ASV](#glossary)) table of validated ASVs assigned 
to taxonomic groups.
* Handles technical or biological replicates of the same sample
* Uses positive and negative control samples to fine tune the filtering and 
reduce [false positive](#glossary) and [false negative](#glossary) occurrences.
* Can pool multiple data sets (results of earlier analyses)
* Can pool results from overlapping markers

**Novelties compared to VTAM:**

* As it is a series of R functions, `vtamR` is highly adaptable to 
include/exclude and order different steps of the analyses
* Includes swarm for denoising
* Graphic options
* Include functions to get statistics of each filtering steps 
(read and variant count etc.)
* The notion of marker and run has been dropped to simplify the analyses

# Installation

Please, follow the [Installation](installation.html) instructions.

# Tutorial

## Set up

*Load library*
```{r setup}
library(vtamR)
```

*Set path to third party programs*

```{r set_path_win, eval=FALSE}
# Example for Windows
cutadapt_path <- "C:/Users/Public/cutadapt"
vsearch_path <- "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/vsearch"
blast_path <- "C:/Users/Public/blast-2.16.0+/bin/blastn"
swarm_path <- "C:/Users/Public/swarm-3.1.5-win-x86_64/bin/swarm"
```

```{r set_path_linux}
#  Example for Linux
cutadapt_path <- "~/miniconda3/envs/vtam/bin/cutadapt"
vsearch_path <- "~/miniconda3/envs/vtam/bin/vsearch"
blast_path <- "~/miniconda3/envs/vtam/bin/blastn"
swarm_path <- "swarm"
```

* **Adapt the path** to third party programs according to your installation
(See [Installation](installation.html)).

*Set general parameters*

```{r general-parameters}
num_threads <- 8
sep <- ","
outdir <- "vtamR_demo_out"
```

* `num_threads`: Number of CPUs for multithreaded programs
* `sep`: Separator used in csv files
* `outdir`: Name of the output directory.
Make sure there is **no space in the path**.


*Input Data*

The demo files below are included with the `vtamR` package, which is why we
use `system.file()` to access them in this tutorial. 
When using your own data, simply provide the file and directory names (e.g. `~/vtamR/fastq`)

```{r set-demo-file}
fastq_dir <- system.file("extdata/demo/fastq", package = "vtamR")
fastqinfo <-  system.file("extdata/demo/fastqinfo.csv", package = "vtamR")
mock_composition <-  system.file("extdata/demo/mock_composition.csv", package = "vtamR")
asv_list <-  system.file("extdata/demo/asv_list.csv", package = "vtamR")
```

* `fastq_dir`: Directory containing the input fastq files.
* [fastqinfo](#fastqinfo): CSV file with information on input files, primers, 
tags, samples.
* [mock_composition](#mock_composition): CSV file with expected ASVs in mock samples. 
See [How to make a mock composition file](make-mock-composition-file.html) on
how to create this file.
* [asv_list](#asv_list): CSV file with ASVs from earlier data sets. Optional.


*Taxassign database*

Set the path to the database for taxonomic assignment 
and to the accompanying taxonomy file.
For this example, we use a very small data base included in the `vtamR` package.
To see how to get a real data base visit [TaxAssign reference data base](installation.html#taxassign-reference-data-base).
When using your own data just enter the file names, without using the 
`system.file` function.

```{r set_db}
taxonomy <- system.file("extdata/db_test/taxonomy_reduced.tsv", package = "vtamR")
blast_db <- system.file("extdata/db_test", package = "vtamR")
blast_db <- file.path(blast_db, "COInr_reduced")
```

* `taxonomy`: CSV file with taxonomic information
* `blast_db`: BLAST database

Details are in [Reference database for taxonomic assignments section](#reference-database-for-taxonomic-assignments)


*Check the format of input files*

The `CheckFileinfo` function tests if all obligatory columns are present, 
and makes some sanity checks. This can be helpful, since these files are produced 
by the users, and may contain errors difficult to spot by eye.

```{r check-fileinfo}
CheckFileinfo(file=fastqinfo, dir=fastq_dir, file_type="fastqinfo")
CheckFileinfo(file=mock_composition, file_type="mock_composition")
CheckFileinfo(file=asv_list, file_type="asv_list")
```

## Merge - Demultiplex

According to your wetlab and sequencing protocol each fastq file can 
contain one or more [sample-replicates](#glossary), and sequences may or may 
not contain [tags](#glossary) (for [demultiplexing](#glossary)) and primer sequences. 

In this tutorial, the fastq files contain reads from several samples, so read pairs 
must be 

- merged and quality filtered (`Merge`)
- demultiplexed and tags and primers trimmed off (`SortReads`)

See the [From fastq to data frame](from-fastq-to-df.html) on
how to deal with other experimental design.

**Merge and quality filter**

* [fastqinfo](#fastqinfo): Either a csv file, or a data frame. 
* `fastq_dir`: Directory containing the input fastq files.
* [fastainfo_df](#fastainfo): is the output of `Merge`. 
It is the updated version of `fastqinfo`, where fastq file names have been 
replaced by fasta file names.
* `fasta_dir`: Directory containing the input fasta files for `SortReads`. 
This directory is created by `Merge`.

See `?Merge` for the options of quality filtering.

```{r merge}
merged_dir <- file.path(outdir, "merged")
fastainfo_df <- Merge(fastqinfo, 
                      fastq_dir=fastq_dir, 
                      vsearch_path=vsearch_path, 
                      outdir=merged_dir,
                      fastq_maxee=1,
                      fastq_maxns=0,
                      fastq_allowmergestagger=F
                      )
```

**Demultiplex and Trim**

The `SortReads` function will [demultiplex](#glossary) the fasta 
files according to the [tag](#glossary) combinations and [trim](#glossary) 
the primers from the reads. 

See the help (`?SortReads`) for setting the 
correct parameters for demultiplexing and primer trimming:

* If `check_reverse` is TRUE, `SortReads` checks the reverse complementary stand as well.
* `sortedinfo_df` is  updated version of `fastainfo`. 
This data frame and the files listed in it are the input of the `Dereplicate`.


```{r demultiplex}
sorted_dir <- file.path(outdir, "sorted")
sortedinfo_df <- SortReads(fastainfo_df, 
                           fasta_dir=merged_dir, 
                           outdir=sorted_dir, 
                           check_reverse=TRUE, 
                           cutadapt_path=cutadapt_path, 
                           vsearch_path=vsearch_path
                           )
```

## Dereplicate

At this stage, you should have **one FASTA file per [sample-replicate](#glossary)**,
containing merged reads with tags and primers removed.

These FASTA files should now be **dereplicated**, 
and each unique sequence ([ASV](#glossary)) will be assigned a numerical ID 
([asv_id](#glossary)). The result is a data frame ([read_count_df](#read_count_df)) 
that can also be written to a **CSV file** if `outfile` is specified. 
Saving this file is recommended, as it serves as the starting point for filtering. 
You can test different filtering strategies from this file without 
needing to re-run the time-consuming [merging](#glossary), [demultiplexing](#glossary),
and [trimming](#glossary) steps.

If you wish to retain ASV IDs from a previous analysis, you can provide a 
reference file ([asv_list](#asv_list)) containing earlier ASVs and their IDs. 
The complete, updated list of ASVs will then be written to `updated_asv_list`.

If only `updated_asv_list` is provided (and `asv_list` is omitted), 
the function will write the full list of current ASVs with assigned IDs. 
This will be useful when analyzing further data sets to ensure consistent 
ASV IDs across studies.

```{r derelicate}
outfile <- file.path(outdir, "filter", "1_before_filter.csv")
updated_asv_list <- file.path(outdir, "ASV_list_with_IDs.csv")

read_count_df <- Dereplicate(sortedinfo_df, 
                             dir=sorted_dir, 
                             outfile=outfile, 
                             asv_list=asv_list, 
                             updated_asv_list=updated_asv_list
                             )
```


## Filter

`vtamR` has a large number of functions to **filter out ASVs** or **occurrences of ASVs**
in a given sample-replicate, which are the results of 
**technical or biological problems associated with metabarcoding**. 

The different filters 

- can be **applied to the original data set** and then the results pooled together 
by using the `PoolFilters` function (accepting only occurrences that pass all filters) 
- or can be **applied sequentially** in the order defined according to your needs.

In this section, I give an example using most filtering options. 
It is up to you to construct your own pipeline.

Each function returns a **data frame with the filtered output**. 
You can also write the results to a **csv file**. This can be useful for tracking 
the presence/absence of some of the ASVs/samples throughout the analyses 
using the [HistoryBy](#historyby) and [SummarizeBy](#summarizeby) functions. 

I will **start the names of the output files by a number** to keep the order 
of the different steps and allow the [SummarizeBy](#summarizeby) function 
to make final statistics at each step of the analyses.

We can also **follow the evolution of the number of reads and ASVs** remaining 
in the data set after each filtering steps. 
Let's define a data frame (`stat_df`), that will contain this information, 
and we will complete it after each step. 

```{r set-stat}
stat_df <- data.frame(parameters=character(),
                      asv_count=integer(),
                      read_count=integer(),
                      sample_count=integer(),
                      sample_replicate_count=integer())
```

Then we add a line with information on the unfiltered data set.

```{r get-initial-stat}
stat_df <- GetStat(read_count_df, stat_df, stage="Input", params=NA)
```

* `params`: String containing the major parameters of each filtering steps.
* `stage`: String referring to the filtering step.

### Denoising by swarm

**Swarm** ([Mahé et al., 2015](https://peerj.com/articles/1420/)) 
is a powerful and quick computer program to denoise the data set by clustering ASVs 
that are likely to be the result of sequencing errors to ASVs representing real 
biological sequences. It will considerably reduce the number of ASVs. 

By default, `vtamR` uses 1 as swam's `d` and the `fastidious` algorithm. 
See `?Swarm` on how to change this.
It is possible to run `Swarm` separately for each sample (`by_sample=TRUE`; default)
or at once for the whole data set. 
This second option is more efficient in reducing the number of ASVs but can 
cluster very similar real sequences appearing in different samples. 

If you are interested in intra-specific variability, I suggest `by_sample=TRUE` 
or skip this step, since `Swarm` can cluster together very similar real ASV.


```{r swarm}
by_sample <- TRUE
outfile <- file.path(outdir, "filter", "2_Swarm_by_sample.csv")

read_count_df <- Swarm(read_count_df, 
                       outfile=outfile, 
                       swarm_path=swarm_path, 
                       num_threads=num_threads, 
                       by_sample=by_sample
                       )

stat_df <- GetStat(read_count_df, stat_df, stage="Swarm", params=by_sample)
```

Let's check the reduction of the number of variants after running `swarm`.
```{r}
knitr::kable(stat_df, format = "markdown")
```

We can also plot the number of reads per variant.
```{r}
Histogram_ReadCountByVariant(read_count_df, binwidth=100)
```
As we can see, most ASV have very low read counts.

### LFNglobalReadCount

Although, swarm has reduced considerably the number of ASVs, there are still 
many ASVs with low read count. 
The `LFNglobalReadCount` function filter out all ASVs with total read count 
bellow a threshold.

Let's eliminate [singletons](#glossary) and see how the number of ASV and reads are have changed.

```{r lfn-global-readcount}
global_read_count_cutoff = 2
outfile <- file.path(outdir, "filter", "3_LFNglobalReadCount.csv")

read_count_df <- LFNglobalReadCount(read_count_df, 
                                    cutoff=global_read_count_cutoff, 
                                    outfile=outfile
                                    )

stat_df <- GetStat(read_count_df, 
                   stat_df, 
                   stage="LFNglobalReadCount", 
                   params=global_read_count_cutoff
                   )
knitr::kable(stat_df, format = "markdown")
```

### FilterIndel

This filter is **applicable only for coding sequences**. 
The idea is that if the length of the ASV differs from the most frequent ASV 
length by a number that is not a multiple of 3, it is likely to be an erroneous 
sequence or a pseudo-gene.

```{r filter-indel}
outfile <- file.path(outdir, "filter", "4_FilterIndel.csv")

read_count_df <- FilterIndel(read_count_df, outfile=outfile)

stat_df <- GetStat(read_count_df, stat_df, stage="FilterIndel")

knitr::kable(stat_df, format = "markdown")
```

### FilterCodonStop

This filter is **applicable only for coding sequences**. 
It checks the number of codon STOPs in all 3 reading frames, 
and eliminates ASVs with STOP in all of them.

The numerical id of the genetic code can be chosen from 
[NCBI](https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi?chapter=cgencodes). 
By default, it is the invertebrate mitochondrial genetic code (5), 
since the STOP codons of this genetic codes are also STOP codons in almost all genetic codes.


```{r filter-codon-stop}
outfile <- file.path(outdir, "filter", "5_FilterCodonStop.csv")
genetic_code = 5

read_count_df <- FilterCodonStop(read_count_df, outfile=outfile, genetic_code=genetic_code)

stat_df <- GetStat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)

knitr::kable(stat_df, format = "markdown")
```

### FilterChimera

This function will run the `uchime3_denovo` function implemented in vsearch. 
It can be run sample by sample (`by_sample=T`; default), or on the whole data set at once. 

FilterChimera will eliminate ASVs detected as chimeras when running on the 
whole dataset at once (`by_sample=F`). 
If `by_sample=T`, ASVs are eliminated if they have been identified as chimeras 
in at least `sample_prop` proportion of the samples where they are present.

* `abskew`: A chimera must be at least `abskew` times less frequent that the parental ASVs.


```{r filter-chimera}
abskew=2
by_sample = T
sample_prop = 0.8
outfile <- file.path(outdir, "filter", "6_FilterChimera.csv")

read_count_df <- FilterChimera(read_count_df, outfile=outfile, vsearch_path=vsearch_path, by_sample=by_sample, sample_prop=sample_prop, abskew=abskew)

params <- paste(abskew, by_sample, sample_prop, sep=";")
stat_df <- GetStat(read_count_df, stat_df, stage="FilterChimera", params=params)

knitr::kable(stat_df, format = "markdown")
```

### FilterRenkonen

Let's eliminate aberrant replicates that are not similar to other replicates of the same sample. 

We will calculate the renkonen distances among all pairs of replicates within sample.

```{r renkonen-dist}
renkonen_within_df <- MakeRenkonenDistances(read_count_df, compare_all=FALSE)
```

Let's make a density plot of the renkonen distances

```{r renkonen-dist-density-plot}
DensityPlot_RenkonenDistance(renkonen_within_df)
```

We can also make a barplot of the renkonen distances, and use different colors for different sample types (real/mock/negative).

**Note**:

- `sortedinfo_df` contains a `sample` and `sample_type` columns, but any data frame with these columns would do.
- If you have restarted the analyses after the `SortReads` function, and  
`sortedinfo_df` is not in your environment, just use the sortedinfo 
file (in the `vtamR_demo_out/sorted/` directory).

```{r renkonen-dist-barplot}
Barplot_RenkonenDistance(renkonen_within_df, sample_types=sortedinfo_df, x_axis_label_size=6)
```

In this example, we have only a few distances, so the density plot is not very relevant. 
The barplot indicates that the replicate 1 of the `tnegtag` sample (negative control) 
is distant from the other two replicates. Lets, establish the Renkonen cutoff 
value to 0.4. We will filter out replicates that have Renkonen distances above 
this cutoff to most other replicates of the sample. In this case, 
replicate 1 of the tnegtag sample will be eliminated.

```{r filer-renkonen}
outfile <- file.path(outdir, "filter", "7_FilterRenkonen.csv")
cutoff <- 0.4

read_count_df <- FilterRenkonen(read_count_df, outfile=outfile, cutoff=cutoff)

stat_df <- GetStat(read_count_df, stat_df, stage="FilerRenkonen", params=cutoff)

knitr::kable(stat_df, format = "markdown")
```

### FilterPCRerror

If you have done the denoising step by **swarm**, this function probably redundant.

`FilterPCRerror` filters out an ASV if it is very similar to another, more frequent ASV.
`max_mismatch` argument tells how many mismatches are allowed between a pair of ASV
to be evaluated for PCR error. If the proportion of the read_counts of two similar 
ASVs is less or equal to `pcr_error_var_prop`, the less abundant ASV is flagged 
as a PCR error.

**OptimizePCRerror**

The default value of `pcr_error_var_prop` (0.1) is arbitrary. 
To choose a value adapted to your data set, you can use the
`OptimizePCRerror` function.

`OptimizePCRerror`, will find all highly similar ASV pairs (`max_mismatch=1` by default)
within a mock sample, where one ASV is expected, and the other is not. 
Their `read_count` ratio is printed in the output file. 

The function considers only ASVs with more than `min_read_count` 
reads in the sample to avoid a ratio based on low read counts that are more 
influenced by stochastic events and will be probably filtered out anyway.

See details using the help: `?OptimizePCRerror`. 

**Note:** I use `max_mismatch=2` in this example, 
to have some output in my very small (and artificial) data set,
but I recommend you to use `max_mismatch=1`.

**Note**: If you do not know the exact sequences of the species present in your 
mock samples, and thus, you do not have the [mock_composition](#mock_composition)
file yet, read the [How to make a mock_composition file](make-mock-composition-file.html).

```{r optimize-pcr-error}
outfile <- file.path(outdir, "optimize", "OptimizePCRerror.csv")
OptimizePCRerror_df <- OptimizePCRerror(read_count_df, 
                                        mock_composition=mock_composition, 
                                        vsearch_path=vsearch_path, 
                                        outfile=outfile, 
                                        max_mismatch=2, 
                                        min_read_count=5
                                        )
```

Let's see the beginning of the output:

```{r optimize-pcr-error-results}
knitr::kable(head(OptimizePCRerror_df), format = "markdown")
```

**FilterPCRerror**

It seems that Swarm has done a good job. The highest read count ratio is 0.013 in the output of `OptimizePCRerror`. This should be taken as a lower limit to `pcr_error_var_prop`. Let's use 0.05 for filtering out PCR errors sample by sample.

See the help for more detail `?FilterPCRerror`.

**Note:** Again, I use `max_mismatch <- 2` in this example, since I have used this value
for `OptimizePCRerror`, but I recommend you to use `max_mismatch <- 1`.

```{r filter-pcr-error}
pcr_error_var_prop <- 0.05
max_mismatch <- 2
outfile <- file.path(outdir, "filter", "8_FilterPCRerror.csv")

read_count_df <- FilterPCRerror(read_count_df, 
                                outfile=outfile, 
                                vsearch_path=vsearch_path, 
                                pcr_error_var_prop=pcr_error_var_prop, 
                                max_mismatch=max_mismatch
                                )

params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sep=";")
stat_df <- GetStat(read_count_df, stat_df, stage="FilterPCRerror", params=params)

knitr::kable(stat_df, format = "markdown")
```

### LFNsampleReplicate

`LFNsampleReplicate` will eliminate occurrences with very low read counts 
compared to the total number of reads in the sample-replicate. 
The default cutoff proportion is 0.001. 
We can have an idea of the maximum value of this cutoff, 
by examining the proportions of expected ASVs in the mock samples using the 
`OptimizeLFNsampleReplicate` function.

**OptimizeLFNsampleReplicate**

**Note**: If you do not know the exact sequences of the species present in your 
mock samples, and thus, you do not have the [mock_composition](#mock_composition)
file yet, read the [How to make a mock_composition file](make-mock-composition-file.html).

```{r optimize-lfn-sample-replicate}
outfile = file.path(outdir, "optimize", "OptimizeLFNsampleReplicate.csv")
OptimizeLFNsampleReplicate_df <- OptimizeLFNsampleReplicate(read_count=read_count_df,
                                                            mock_composition=mock_composition,
                                                            outfile=outfile)
knitr::kable(head(OptimizeLFNsampleReplicate_df), format = "markdown")
```

The lowest proportion of read count of an expected ASV to the total read count
of its sample-relicate is 0.0049 
(Sequence of *Caenis pusilla* in the replicate 3 of tpos1 mock sample). 
A cutoff values for `LFNsampleReplicate` higher than this will eliminate some 
of the expected occurrences and create false negatives. 
Therefore, we choose 0.004 as a cutoff.

**LFNsampleReplicate**

```{r lfn-sample-replicate}
lfn_sample_replicate_cutoff <- 0.004
outfile <- file.path(outdir, "filter", "9_LFNsampleReplicate.csv")

read_count_df <- LFNsampleReplicate(read_count_df, 
                                    cutoff=lfn_sample_replicate_cutoff, 
                                    outfile=outfile
                                    )

stat_df <- GetStat(read_count_df, 
                   stat_df, 
                   stage="LFNsampleReplicate", 
                   params=lfn_sample_replicate_cutoff
                   )

knitr::kable(stat_df, format = "markdown")
```

### FilterMinReplicate 1

To ensure repeatability, we can accept occurrences if they are present in at 
least `min_replicate_number` replicates of the sample (2 by default).

```{r filter-min-replicate}
min_replicate_number <- 2
outfile <- file.path(outdir, "filter", "10_FilterMinReplicate.csv")

read_count_df <- FilterMinReplicate(read_count_df, 
                                    cutoff=min_replicate_number, 
                                    outfile=outfile
                                    )
stat_df <- GetStat(read_count_df, 
                   stat_df, 
                   stage="FilterMinReplicate", 
                   params=min_replicate_number
                   )

knitr::kable(stat_df, format = "markdown")
```

### LFNvariant and LFNreadCount

The `LFNvariant` filter will eliminate occurrences with very low read counts 
compared to the total number of reads of the ASV. 
The default cutoff proportion is 0.001. 
This filter is designed to filter out occurrences present in the data set due 
to **tag-jump** or light **inter sample contamination**.

The `LFNreadCount` filter simply eliminates occurrences with read counts bellow 
a `cutoff` (10 by default). 

To find the best cutoff values for these two filters, 
the `OptimizeLFNreadCountLFNvariant` will count the number of 
[false positive](#glossary), [false negative](#glossary) and
[true positive](#glossary) occurrences using a series of combination of cutoff 
values of these two filters. To run this function, we need a 
[known_occurrences_df](#known_occurrences) data frame that lists all 
occurrences that are known to be a FP, TP.

**MakeKnownOccurrences**

The `MakeKnownOccurrences` function will identify [false positive](#glossary), 
[false negative](#glossary) and [true positive](#glossary) 
occurrences in controls samples (mock and negative). 
Some false positives can also be identified in real samples if 
samples of different habitats are included in the data sets. 

The false positive and the true positive occurrences are written to the 
`known_occurrences` data frame (or file), false negatives to the 
`missing_occurrences` data frame (or file), and a `performance_metrics`
data frame is also produced with the count of these occurrences. 
For details see `?MakeKnownOccurrences`.

This function takes a `read_count_samples` data frame as an input,
where the replicates of the same sample have been pooled (see `?PoolReplicates`)

**Note**: 

- If you do not know the exact sequences of the species present in your 
mock samples, and thus, you do not have the [mock_composition](#mock_composition)
file yet, read the [How to make a mock_composition file](make-mock-composition-file.html).
- sortedinfo_df contains a `sample` and `sample_type` `replicate` and `habitat`
columns, but any data frame with these columns would do.
- If you have restarted the analyses after the `SortReads` function, and  
`sortedinfo_df` is not in your environment, just use the sortedinfo 
file (in the `vtamR_demo_out/sorted` directory).
 
```{r make-known-occurrences}
# Detect known occurrences
results <- MakeKnownOccurrences(read_count = read_count_df, 
                                sortedinfo=sortedinfo_df, 
                                mock_composition=mock_composition
                                )

# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
```


**OptimizeLFNreadCountLFNvariant**

The `LFNreadCount` and `LFNvariant` functions are run for a series of cutoff 
value combinations of the two filters, followed by `FilterMinReplicate`. 
For each parameter combination, the number of FN, TP, and FP is reported. 
Chose the parameter setting that minimizes, FN and FP.

Here we will use the default range of cutoffs to test, 
but you can set the minimum, the maximum and the increment for the cutoff 
values for both filters. (see `?OptimizeLFNreadCountLFNvariant`). 
We set `min_replicate_number` to 2, to eliminate non-repeatable 
occurrences among the three replicates of each sample.


```{r optimize-lfn-readcount-lfn-variant}
outfile = file.path(outdir, "optimize", "OptimizeLFNreadCountLFNvariant.csv")

OptimizeLFNreadCountLFNvariant_df <- OptimizeLFNreadCountLFNvariant(
  read_count_df,
  known_occurrences=known_occurrences_df,
  outfile= outfile, 
  min_replicate_number=2
  )

#head(OptimizeLFNreadCountLFNvariant_df)
knitr::kable(head(OptimizeLFNreadCountLFNvariant_df), format = "markdown")
```

**LFNvariant, LFNreadCount**

From the output, we choose 0.001 for the cutoff of `LFNvariant` 
and 10 for `LFNreadCount`, since this is the less stringent combination 
that keeps all expected occurrences (6 TP, 0 FN), and has less FP (4 FP).

We will run the two filters on the same input data frame 
(for which the parameters has been optimized), 
and pool the results by accepting only occurrences that pass both 
filters by the `PoolFilters` function.

See `?LFNvariant, ?LFNreadCount` and `?PoolFilters` for details.

LFNvariant
```{r lfn-variant}
lnf_variant_cutoff = 0.001
outfile <- file.path(outdir, "filter", "11_LFNvariant.csv")

read_count_df_lnf_variant <- LFNvariant(read_count_df, 
                                        cutoff=lnf_variant_cutoff, 
                                        outfile=outfile
                                        )
stat_df <- GetStat(read_count_df_lnf_variant, 
                   stat_df, 
                   stage="LFNvariant", 
                   params=lnf_variant_cutoff)
```

LFNreadCount
```{r lfn-readcount}
lfn_read_count_cutoff <- 10
outfile <- file.path(outdir, "filter", "12_LFNreadCount.csv")

read_count_df_lfn_read_count <- LFNreadCount(read_count_df, 
                                             cutoff=lfn_read_count_cutoff, 
                                             outfile=outfile
                                             )
stat_df <- GetStat(read_count_df_lfn_read_count, 
                   stat_df, stage="LFNreadCount", 
                   params=lfn_read_count_cutoff
                   )
```

Combine results
```{r combine-results}
outfile <- file.path(outdir, "filter", "13_poolLFN.csv")
read_count_df <- PoolFilters(read_count_df_lfn_read_count, 
                             read_count_df_lnf_variant, 
                             outfile=outfile
                             )
stat_df <- GetStat(read_count_df, 
                   stat_df, 
                   stage="FilterLFN"
                   )
# delete temporary data frames
rm(read_count_df_lfn_read_count)
rm(read_count_df_lnf_variant)

knitr::kable(stat_df, format = "markdown")
```

### FilterMinReplicate 2

Let's run again `FilterMinReplicate` to ensure repeatability among replicates 
of the sample (2 by default).

```{r filter-min-replicate2}
min_replicate_number <- 2
outfile <- file.path(outdir, "filter", "14_FilterMinReplicate.csv")

read_count_df <- FilterMinReplicate(read_count_df, 
                                    cutoff=min_replicate_number, 
                                    outfile=outfile
                                    )
stat_df <- GetStat(read_count_df, 
                   stat_df, 
                   stage="FilterMinReplicate", 
                   params=min_replicate_number
                   )

knitr::kable(stat_df, format = "markdown")
```

### Get performance metrics

Run `MakeKnownOccurrences` again to get performance metrics (FP, FN, FP). 
This time we will write the output data frames to files as well. 
The `performance_metrics` file will give you the count of FP, FN, TP, 
accuracy and sensitivity. 
You can find false negatives in `missing_occurrences`, 
and true and false positives in `known_occurrences`.

```{r performance-metrics}
missing_occurrences <- file.path(outdir, "Missing_occurrences.csv")
performance_metrics <- file.path(outdir, "Performance_metrics.csv")
known_occurrences <- file.path(outdir, "Known_occurrences.csv")

results <- MakeKnownOccurrences(read_count_df, 
                                sortedinfo=sortedinfo_df, 
                                mock_composition=mock_composition, 
                                known_occurrences=known_occurrences, 
                                missing_occurrences=missing_occurrences,
                                performance_metrics=performance_metrics
                                )
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]

knitr::kable(performance_metrics_df, format = "markdown")
```

## Pool Replicates, Make mOTU

### PoolReplicates

Replicates have been used to make sure the repetability and reduce experimental
fluctuations. We do not need them any more, so let's pool replicates of the same sample. 

`PoolReplicates` function will take the mean non-zero read counts of each ASV 
over replicates of the same sample.

```{r pool-replicates}
outfile <- file.path(outdir, "filter", "15_PoolReplicates.csv")
read_count_samples_df <- PoolReplicates(read_count_df, 
                                        outfile=outfile)

stat_df <- GetStat(read_count_samples_df, 
                   stat_df, 
                   stage="PoolReplicates")

knitr::kable(stat_df, format = "markdown")
```

### Make mOTU

If you are not interested in intra-specific variation, the ASV can be clustered to mOTUs.

**Option 1 - ClusterSize**

The simplest is to cluster all ASV to the same mOTU within a fixed radius using the
`cluster_size` algorithm of `vsearch`. 

```{r motu-clustersize}
outfile <- file.path(outdir, "filter", "16_mOTU_ClusterSize.csv")
identity <- 0.97 
read_count_samples_df_ClusterSize <- ClusterSize(read_count_samples_df,
                                     id=identity, 
                                     vsearch_path=vsearch_path, 
                                     outfile=outfile)

stat_df <- GetStat(read_count_samples_df_ClusterSize, 
                   stat_df, 
                   stage="ClusterSize_mOTU",
                   param=identity
                   )
```

**Option 2 - Swarm**

Alternatively, it is possible to run `Smarm` on the whole data set (`by_sample=FALSE`)
using a highest `d` than the default (`1`). In our earlier analyses, we found that `d=7`
gives a good proxy of clustering different ASV of the same species together for 
a 313 bp fragment of the COI gene. By no means it is a standard value. 
You should experiment using a series of different `d` values.


```{r motu-swarm}
outfile <- file.path(outdir, "filter", "17_mOTU_Swarm.csv")
by_sample <- FALSE
d = 7
read_count_samples_df_swarm <- Swarm(read_count_samples_df, 
                       swarm_path=swarm_path, 
                       swarm_d=d,
                       fastidious=FALSE,
                       num_threads=num_threads, 
                       by_sample=by_sample,
                       outfile= outfile)

stat_df <- GetStat(read_count_samples_df_swarm, stat_df, stage="Swarm_mOTU", params=d)

knitr::kable(stat_df, format = "markdown")
```

Apparently, the 2 different clusterings are doing a similar job in this case.

## TaxAssign

If you have already assigned ASV to taxa, 
([before making the mock_composition file](make-mock-composition-file.html))
you can use the same output (`asv_tax` data frame) and skip this step.

If you haven't assigned ASV to taxa yet, it is time to do it. You can either assign
the ASV (`read_count_samples_df`) as in this example, or the centroids of the 
mOTUS (`read_count_samples_df_swarm` or `read_count_samples_df_ClusterSize`)

See the brief description of the algorithm with `?TaxAssign`.

For the format of `taxonomy` and `blast_db` check the 
[Reference database for taxonomic assignments](#reference-database-for-taxonomic-assignments) 
section. You can also download a ready to use COI database 
(See [Installation](installation.html)).


```{r taxassign}
outfile <- file.path(outdir, "TaxAssign.csv")

asv_tax <- TaxAssign(asv=read_count_samples_df, 
                     taxonomy=taxonomy, 
                     blast_db=blast_db, 
                     blast_path=blast_path, 
                     outfile=outfile, 
                     num_threads=num_threads
                     )
```

## Print output

`WriteASVtable` will reorganize the `read_count_samples_df` data frame,
(or `read_count_samples_df_swarm` or `read_count_samples_df_ClusterSize`)
with samples in columns and ASV (or centroids of mOTUs) in lines 
(from [long format](#glossary) to [wide format](#glossary)).

It is possible to add supplementary information as well:

* Taxonomic assignment (`asv_tax`)
* Total number of reads and samples for each ASV (`add_sums_by_asv`)
* Total number of reads and ASVs in each sample (`add_sums_by_sample`)
* Supplementary column for each mock sample with expected occurrences 
in each of them (`add_expected_asv`)
* Supplementary column for each sample that has been filtered out (`add_empty_samples`)

For more information see `?WriteAsVtable`

```{r write-asv-table}
outfile=file.path(outdir, "Final_asvtable_with_TaxAssign.csv")

asv_table_df <- WriteASVtable(read_count_samples_df, 
                              outfile=outfile, 
                              asv_tax=asv_tax, 
                              sortedinfo=sortedinfo_df, 
                              add_empty_samples=T, 
                              add_sums_by_sample=T, 
                              add_sums_by_asv=T, 
                              add_expected_asv=T, 
                              mock_composition=mock_composition
                              )
```

Print the number of reads, ASVs, samples and replicates after each step to a CSV file.

```{r}
write.csv(stat_df, file = file.path(outdir, "Summary.csv"))
```

# Supplementary functions

## RandomSeq

Random select `n` sequences from each input fasta file. 
It can be used before or after demultiplexing (`SortReads`). 
However, if using `LFNvariant` and `LFNreadCount` which are partially 
based on the number of reads in negative controls, standardizing the number of 
reads among samples does not make sense. Thus I use `RandomSeq` after `Merge`, 
to get the same number of reads for each replicate series 
(same samples, different replicates in each fasta file).


* [fastainfo](#fastainfo): Either a csv file, or a data frame, with the following columns: 
tag_fw, primer_fw, tag_rv, primer_rv, sample, sample_type, habitat, replicate, fasta.
* `fasta_dir`: Directory containing the input fasta files.
* `n`: Number of sequences to be taken randomly (without replacement).


```{r}
randomseq_dir = file.path(outdir, "random_seq")
fastainfo_df <- RandomSeq(fastainfo_df, 
                          fasta_dir=merged_dir, 
                          outdir=randomseq_dir, 
                          vsearch_path=vsearch_path, 
                          n=10000
                          )
```

##  HistoryBy

This function scans all files in the `dir` that starts by a number. 
(See file names of the output files of the different [filtering steps](#filter)). 
It will select all lines were the `feature` (asv_id/asv/sample/replicate) 
has a `value` we are looking for.

**Examples**

*Get the history of asv_id (`feature`) 27 (`value`).*
```{r}
filter_dir <- file.path(outdir, "filter")
tmp_ASV_27 <- HistoryBy(dir=filter_dir, 
                        feature="asv_id", 
                        value="27"
                        )

knitr::kable(head(tmp_ASV_27), format = "markdown")
```

*Get the history of a sequence of the ASV  (`feature`).*
```{r}
tmp_replicate_1 <- HistoryBy(dir=filter_dir, 
                             feature="asv",
                             value="CCTTTATTTTATTTTCGGTATCTGGTCAGGTCTCGTAGGATCATCACTTAGATTTATTATTCGAATAGAATTAAGAACTCCTGGTAGATTTATTGGCAACGACCAAATTTATAACGTAATTGTTACATCTCATGCATTTATTATAATTTTTTTTATAGTTATACCAATCATAATT"
                             )

knitr::kable(tmp_replicate_1, format = "markdown")
```

*Get the history of the sample (`feature`) tpos1 (`value`).*
```{r}
tmp_sample_tpos1 <- HistoryBy(dir=filter_dir, 
                              feature="sample", 
                              value="tpos1"
                              )

knitr::kable(head(tmp_sample_tpos1), format = "markdown")
```


## SummarizeBy

This function scans all files in the `dir` that starts by a number. 
(See file names of the output files of the different [filtering steps](#filter)). 

It will group each file by a variable (asv/asv_id/sample/replicate) 
and summarize a `feature` (asv/asv_id/sample/replicate/read_count). 
If the `feature` is `read_count`, it will give the sum of the read counts 
for each value of the variable in each file. 
Otherwise, it returns the number of distinct values of the `feature` 
for each value of the variable in each file.

**Examples**

*Get the number of reads of each sample after each filtering steps.*

From this data frame, we can see that the negative control sample become 
"clean" after the LFN filters, and there is a considerable variation 
among the number of reads of different real samples.

```{r}
read_count_by_sample <- SummarizeBy(dir=filter_dir, 
                                    feature="read_count", 
                                    grouped_by="sample"
                                    )

knitr::kable(read_count_by_sample, format = "markdown")
```


*Get the number asv for each sample after each filtering steps.*

From this data frame, we can see that the negative control sample become "clean" 
after the LFN filters, and the mock samples has 10 ASVs at the end.
```{r}
asv_by_sample <- SummarizeBy(dir=filter_dir, 
                             feature="asv", 
                             grouped_by="sample"
                             )

knitr::kable(asv_by_sample, format = "markdown")
```


*Get the number asv_id for each replicate after each filtering steps.*

We can see that number of ASVs are comparable in different replicates.
```{r}
asvid_by_replicate <- SummarizeBy(dir=filter_dir, 
                                  feature="asv_id", 
                                  grouped_by="replicate"
                                  )

knitr::kable(asvid_by_replicate, format = "markdown")
```

## UpdateASVlist

Pools unique `asv` - `asv_id` combinations from the input data frame (`read_count_df`) 
and from the input file (`asv_list`).

The input file is typically a CSV file containing ASVs seen in earlier data sets 
with their `asv_id`.
If there is a conflict within or between the input data the function quits 
with an error message. Otherwise writes the `updated_asv_list` to the outfile.

The safest option of avoiding incoherence between `asv_ids` of earlier 
and present runs is to use the `asv_list` and `updated_asv_list` 
parameters in the `Dereplicate` function as it is done in this [tutorial](#dereplicate). 
This will synchronize the `asv_id` in this run with earlier ones, 
and writes an updated file with all ASV from earlier and the present data set. 
In this case, calling the `UpdateASVlist` function is not necessary, since it is 
automatically called from `Dereplicate`. 

However, if you are analyzing a large number of very large data sets, 
the complete `asv_list` will grow quickly, and might cause memory issues. 
Most of the ASVs in this list are singletons, and filtered out during the analyses. 
Therefore, you can opt for homogenizing the `asv_id`s with earlier runs by the 
`Dereplicate` function, without writing an `updated_asv_list`. 
Then update the `asv_list` after the first steps of filtering 
(e.g. `Swarm`, `LFNglobalReadCount`) to keep only ASVs that are more frequent 
and more likely to appear in future data sets. It is still a quite safe option, 
and reduces greatly the number of ASVs kept in this file.


```{r}
updated_asv_list <- file.path(outdir, 
                              "updated_ASV_list.csv"
                              )
UpdateASVlist(asv_list1=read_count_df,
              asv_list2=asv_list, 
              outfile=updated_asv_list
              )
```

## PoolDatasets

**More than one overlapping marker**

This function pools different data sets and it is particularly useful, 
if the results should be pooled from more than one overlapping markers. 
In that case, ASVs identical on their overlapping regions are pooled into groups, 
and different ASVs of the same group are represented by their centroid 
(longest ASV of the group). Pooling can take the mean read counts of the ASVs 
(`mean_over_markers=T`; default) or their sum (`mean_over_markers=F`).

The function takes several input CSV files, each in [long format](#glossary) 
containing `asv_id`, `sample`, `read_count` and `asv` columns. 
The file names should be organized in data frame, with the marker names for each file.

* `outfile`: Name of the output CSV file with the pooled data set 
(`asv_id`, `sample`, `read_count`, `asv`).
ASVs are grouped to the same line if identical in their overlapping region, 
and only the centroids appear in the `asv` column.
* `asv_with_centroids`: Name of the output CSV file containing each of the the 
original ASVs (with samples, markers, and read_count) as well as their centroids.
* The data frame returned by the function corresponds to the `outfile`.

```{r, eval=FALSE}
files <- data.frame(file=c("vtamR_test/out_mfzr/15_PoolReplicates.csv",
                           "vtamR_test/test/15_PoolReplicates_ZFZR.csv"),
                    marker=c("MFZR", 
                             "ZFZR")
                    )

outfile <- file.path(outdir, "Pooled_datasets.csv") 
asv_with_centroids <- file.path(outdir, 
                                "Pooled_datasets_asv_with_centroids.csv"
                                ) 

read_count_pool <- PoolDatasets(files, 
                                outfile=outfile, 
                                asv_with_centroids=asv_with_centroids, 
                                mean_over_markers=T,
                                vsearch_path=vsearch_path
                                )
```

**Only one marker**

Pooling the results of different data sets of the same marker is very simple. 
Basically, the input files (in [long format](#glossary) with 
`asv`, `asv_id`, `sample`, `read_count` columns) are concatenated. 
The `PoolDatasets` function also  checks if sample names are unique. 
If not, it sums the read count of the same sample and same ASV, but returns a warning.

The output file or data frame can be rearranged to [wide format](#glossary) by the [WriteASVtable](#print-output) function.

## CountReadsDir

Count the number of reads in `fasta` or `fastq` files found in the input directory.
Input files can be gz compressed or uncompressed, but zip files are not supported.

The [fastainfo](#fastainfo) and the [sortedinfo](#sortedinfo) 
files contain the number of reads after `Merge` or `SortReads`, 
so no need to run `CountReadsDir` separately. 
This function can be useful for counting the number of reads in the input `fastq` files.

* `dir`: Input directory containing the fasta of fastq files
* `file_type`: [fasta/fastq]
* `pattern`: Regular expression; Check only files for `pattern` in the file name

```{r, eval=FALSE}
df <- CountReadsDir(dir=fastq_dir, 
                    pattern="_fw.fastq.gz", 
                    file_type="fastq"
                    )
```
 
## Barplot_ReadCountBySample

Make a bar plot of read counts by [sample](#glossary) (`sample_replicate=F`) 
or [sample-replicate](#glossary) (`sample_replicate=T`).
Can use different colors for different sample types ([real/mock/negative](#glossary))

* [read_count_df](#read_count_df): Input data frame
* `sample_types`: data frame or CSV file containing info on sample types for each sample

```{r}
sortedinfo <- file.path(sorted_dir, "sortedinfo.csv")
Barplot_ReadCountBySample(read_count_df=read_count_df, 
                          sample_replicate=F, 
                          sample_types=sortedinfo
                          )
```


## Histogram_ReadCountByVariant

Histogram of read counts by ASV

* [read_count_df](#read_count_df): Input data frame
* `min_read_count`: Ignore variants with read count bellow this value
* `binwidth`: Width of bins

```{r}
Histogram_ReadCountByVariant(read_count_df, 
                             min_read_count=10, 
                             binwidth=1000
                             )
```


## Renkonen distances

Calculate the Renkonen distances among all replicates (compare_all=TRUE) 
or among replicates of the same sample (compare_all=FALSE) and plot them.

* [read_count_df](#read_count_df): Input data frame.
* `sample_types`: Data frame or CSV file containing info on sample types for each sample.

```{r}
sortedinfo <- file.path(sorted_dir, "sortedinfo.csv")
renkonen_within_df <- MakeRenkonenDistances(read_count_df, compare_all=FALSE)
Barplot_RenkonenDistance(renkonen_within_df, sample_types=sortedinfo)
DensityPlot_RenkonenDistance(renkonen_within_df)

renkonen_all_df <- MakeRenkonenDistances(read_count_df, compare_all=TRUE)
DensityPlot_RenkonenDistance(renkonen_all_df)
```

 

# I/O files and data frames

## fastqinfo

CSV file with information on input fastq files, primers, tags, 
samples with the following columns. 
Each line corresponds to a sample-replicate combination.

 * tag_fw: Sequence tag on the 5' of the fw read (NA if file is already demultiplexed)
 * primer_fw: Forward primer (NA if primer has been trimmed)
 * tag_rv: Sequence tag on the 3' of the rv read (NA if file is already demultiplexed)
 * primer_rv: Reverse primer (NA if primer has been trimmed)
 * sample: Name of the sample (alpha-numerical)
 * sample_type: [real/mock/negative](#glossary)
 * habitat: If real or mock samples are from different habitats that cannot 
 contain the same type of organisms (e.g. terrestrial vs. marine), 
 this information is used for detecting false positives.
 Use NA otherwise. Use NA for negative controls.
 * replicate: Numerical id of a replicate within sample 
 (e.g. Sample1 can have replicate 1, 2 or 3)
 * fastq_fw: Forward fastq file
 * fastq_rv: Reverse fastq file


## fastainfo

CSV file with information on input fasta files, primers, tags,
samples with the following columns. 
Each line corresponds to a sample-replicate combination.

 * tag_fw: Sequence tag on the 5' of the fw read (NA if file is already demultiplexed)
 * primer_fw: Forward primer (NA if primer has been trimmed)
 * tag_rv: Sequence tag on the 3' of the rv read (NA if file is already demultiplexed)
 * primer_rv: Reverse primer (NA if primer has been trimmed)
 * sample: Name of the sample (alpha-numerical)
 * sample_type: [real/mock/negative](#glossary)
 * habitat: If real or mock samples are from different habitats that 
 cannot contain the same type of organisms (e.g. terrestrial vs. marine), 
 this information is used for detecting false positives. Use NA otherwise. 
 Use NA for negative controls.
 * replicate: Numerical id of a replicate (e.g. Sample1 can have replicate 1, 2 or 3)
 * fasta: Fasta file
 * Read_count: Number of reads in the fasta file. Optional.

## sortedinfo

CSV file with information on demultiplexed and primer trimmed fasta files 
and samples with the following columns. Each line corresponds to a 
sample-replicate combination.

 * sample: Name of the sample (alpha-numerical)
 * sample_type: [real/mock/negative](#glossary)
 * habitat: If real or mock samples are from different habitats that 
 cannot contain the same type of organisms (e.g. terrestrial vs. marine), 
 this information is used for detecting false positives. Use NA otherwise. 
 Use NA for negative controls.
 * replicate: Numerical id of a replicate (e.g. Sample1 can have replicate 1, 2 or 3)
 * fasta: Fasta file
 * Read_count: Number of reads in the fasta file. Optional.

## mock_composition

CSV file with the following columns.

 * sample: Name of the [mock](#glossary) sample
 * action: 
      * keep: Expected ASV in the mock, that should be kept in the data set
      * tolerate: ASV that can be present in a mock, but it is not essential 
      to keep it in the data set (e.g. badly amplified organism)
 * [asv](#glossary): sequence of the ASV
 * taxon: Optional; Name of the organism
 * [asv_id](#glossary): Optional; If there is a conflict between
 asv and asv_id, the asv_id is ignored

## known_occurrences
CSV file or data frame with the following columns.

 * [sample](#glossary): Name of the sample
 * action:
      * keep: Expected ASVs in a mock sample (corresponds to True Positives)
      * delete: False Positive occurrences: unexpected ASV in a mock sample; 
      all occurrences in negative controls; occurrences in real samples 
      corresponding to an incompatible habitats (e.g. an ASV mostly present 
      in marine samples is unexpected in a freshwater sample)
 * [asv_id](#glossary): optional
 * [asv](#glossary): sequence of the ASV
 
## asv_list

This file lists ASVs and their IDs from earlier data sets. When provided 
(optional), identical ASVs in the present and earlier data sets have the 
same ID. New ASVs (not present in asv_list) will get unique IDs not present in asv_list. 
It is a CSV file with the following columns:

 * [asv_id](#glossary): Unique numerical ID of the ASV
 * [asv](#glossary): ASV sequence
 

## read_count_df

Data frame with the following columns:

 * [asv](#glossary): Sequence of the ASV
 * [asv_id](#glossary): Numerical ID of the ASV
 * [sample](#glossary): Sample name
 * [replicate](#glossary): Replicate within sample (Numerical)
 * read_count: Number of reads of the ASV in the Sample-Replicate

## Reference database for taxonomic assignments

A data base is composed of two elements. A BLAST database (`blast_db`) 
and a `taxonomy` file.

`blast_db` can be produced using the `makeblastdb` command of BLAST:

```{bash, eval=FALSE}
makeblastdb -dbtype nucl -in [FASTA_FILE] -parse_seqids -taxid_map [TAXID_FILE] -out [DB_NAME]
```

 * FASTA_FILE is a fasta file containing reference sequences.
 * TAXID_FILE is a tab separated file with sequence IDs and the 
 corresponding numerical [taxIDs](#glossary).
 * DB_NAME is the name of the newly created BLAST database.

`taxonomy` is a **tab separated** csv file with the following columns:

 * tax_id: Numerical Taxonomic ID. It can be a valid 
 [NCBI taxID](https://www.ncbi.nlm.nih.gov/taxonomy), 
 or arbitrary negative numbers for taxa not in NCBI.
 * parent_tax_id: taxID of the closest parent of tax_id.
 * rank: taxonomic rank (e.g. species, genus, subgenus, no_rank).
 * name_txt: Scientifc name of the taxon.
 * old_tax_id: taxIDs that have been merged to the tax_id by NCBI; 
 if there is more than one for a given tax_id, make one line for each old_tax_id.
 * taxlevel: Integer associated to each major taxonomic rank. 
 (0 => root, 1=> domain, 2=> kingdom, 3=> phylum, 4=> class, 5=> order, 
 6=> family, 7=> genus, 8=> species). 
 Levels in between have 0.5 added to the next highest level 
 (e.g. 5.5 for infraorder and for superfamily).


**A ready to use COI database** in BLAST format and the associated taxonomy 
file can be downloaded from [https://osf.io/vrfwz/](https://osf.io/vrfwz/). 
It was created using [mkCOInr](https://github.com/meglecz/mkCOInr). 
It is also possible to make a [customized database](https://mkcoinr.readthedocs.io/en/latest/content/tutorial.html#customize-database) 
using mkCOInr. It can be dowloaded and extracted manually of using the 
`download_osf` function of vtamR. See [Installation](installation.html)




# Options

* `compress`: If TRUE, output files of the `Merge`, `RandomSeq` and `SortReads` 
functions are compressed. This saves space, but compressing/decompressing
increases run time in some cases. I suggest to avoid compressing intermediate 
files and delete/compress them as soon as the analyses are finished. 
(See more in the [troubleshooting section](#troubleshooting)). 
The input fasta and fastq files of these functions can be compressed 
(`.gz`, `.bz2`, but NOT .zip) or uncompressed files and compression 
is automatically detected based on the file extension.

* `delete_tmp`: Delete automatically intermediate files. TRUE by default.

* `sep`: Separator used in csv files. Use the same separator in all CSV files 
except for the [taxonomy](#reference-database-for-taxonomic-assignments) 
file of the reference database, which is tab separated.

* `quiet`: If TRUE print as little information to the terminal as possible. 
TRUE by default.



# Glossary


* **ASV**: Amplicon Sequence Variant. Unique sequnece, caracterized by 
the number of reads in each sample-replicate.
* **asv_id**: Unique numerical ID of an ASV.
* **demultiplexing**: Sorting reads in a fasta (or fastq) file to different 
sample-replicates according to the tags present at their extremities.
* **dereplication**: The merged reads contain many identical sequenes. 
The dereplication reduces the dataset to unique sequences (ASV), and count 
the number of reads for each ASV in each sample-replicate.
* **data frame (or csv) in long format**: The read_count_df contains one line 
for each occurrence with asv_id, asv, sample, replicate, read_count columns. 
This is the standard format of keeping occurrences throughout the analyses, 
and it is smaller than the wide format, 
if there are many ASVs present in only one or few samples.
* **data frame (or csv) in wide format**: The read_count_df can be rearranged 
in wide format, where lines are ASVs, columns are sample(-replicates) and cells
contain read counts. It is a more human friendly format and it is the base of 
writing an ASV table, where this information can be completed by taxonomic 
assignments and other informations.
* **false negative occurrence**: An expected ASV in a mock sample that is not 
found in the data.
* **false positive occurrence**: Un expected presence an ASV in a Sample.
    * all occurrences in all negative contols, 
    * unexpected occurrences in mock samples
    * presence of an ASV in an incompatible habitat (e.g. ASV with high read 
    count in samples of habitat 1 and low read count in habitat 
    2 is considered as FP in habitat 2).
* **habitat**: Habitat type of the organisms in real or mock samples. 
Use this only if organisms of the different habitats cannot appear in 
another haditat of the dataset. Use NA otherwise.
* **long format**: Each row is a single measurement. Typically, 
there are columns like ASV, sample, and read_count. This format is tidy 
and works well with many tidyverse functions.
* **Low Frequency Noise (LFN)**: ASVs present in low frequencies, 
likely to be due to errors.
* **merge**: Assemble a forward and reverse read pair to one single sequence.
* **mock sample**: An artificial mix of DNA of know organisms.
* **negative sample**: Negative control.
* **real sample**: An environmental sample.
* **replicate**: Technical or biological replicate of a sample. 
Replicates must have numerical identifiers. (e.g. sample tops1 have replicate 1, 2 and 3).
* **sample**: Name of the environmental or control sample. Must be alphanumerical, without space.
* **sample-replicate**: Each sample can have technical of biological replicates. 
sample-replicate refers to one replicate of a given sample.
* **sample_type**: [real/mock/negative](#glossary)
* **singleton**: ASV with a single read in the whole data set.
* **tag**: Short sequence at the extremity of the amplicon. 
It is used at the demultiplexing step to identify the sample-replicate, where the read comes from.
* **taxIDs**: Numerical taxonomic identifier. It can be a valid [NCBI taxID](https://www.ncbi.nlm.nih.gov/taxonomy), 
or arbitrary negative numbers for taxa not in NCBI.
* **trimming**: Cut the extremities of the sequences. It can be based of 
sequences quality, or on the detection of a tag or primer.
* **true positive occurrence**: An expected occurrence in a mock sample.
* **Wide format**: In vtamR wide format data frames, each row is an ASV, 
and each column represents a sample. Read counts are in the cells.




# Troubleshooting

## tmp_FunctionName_######## in vtamR directory

These are temporary directories, that are automatically deleted at the end of 
the function. In some cases, if the function ends prematurely due to an error, 
the directory is not deleted. Just delete them manually.

## LFNvariant eliminate most occurrences of a frequent ASV

`LFNvariant` will filter out all occurrences where

`(number of reads of the ASV in a sample-replicate) / (total number of read of the ASV) < cutoff` 

As a consequence, if an ASV is present in most samples, there are many samples 
in the data set and the cutoff is relatively high, most occurrences can fall 
bellow the cutoff and the total read count of the ASV will decrease strongly at this step.

If the proportion of the read count of an ASV in the output compared to 
the input is less than `min_read_count_prop`, vtamR prints out a Warning. 
Take a close look at the variant. It can be the result of a general contamination, 
and in this case, the whole ASV could/should be eliminated from the data set. 
Otherwise, you might want to reduce the cutoff value for `LFNvariant`.

## Memory issues for running swarm on the whole dataset

Possible workarounds:

* Try to run Swarm sample by sample (`by_sample=T`). 
* Run `global_read_count` first to eliminate singletons then swarm. 

Both solutions make swarm a bit less efficient, but at least your analyses can get through.

## Memory issues in Merge on Windows

If input fastq files are very large and do not go through `Merge`, 
try working with uncompressed input files.


### Files to keep

At the first steps (Merge, RandomSeq, Sortreads), many files are produced,
and some of them can be very large, especially before the dereplication 
(before `Dereplicate`).

Compressing and decompressing between the different steps can take a while 
and can have memory issues in some cases (especially on Windows). Therefore, 
`vtamR` uses by default uncompressed files. This behavior can be changed by the 
[compress](#options) option.

Here is a list of files I would tend to keep, delete or compress.

* `Merge`: Delete the output fasta files, but keep `fastainfo.csv` for 
information on read counts after `Merge`.
* `RandomSeq`: Compress fasta files, but keep them to ensure reproducibility 
in case of re-analyse.
* `SortReads`: Delete the output fasta files, but keep `sortedinfo.csv` for 
information on read counts after `SortReads`. 
* `Dereplicate`: Write the dereplicated info to a file 
(`1_before_filter.csv` in the tutorial) and compress it after the analyses. 
This is an important file, since you can restart the analyses from here,
without re-doing the longest merging, and demultiplexing steps. 
Compress and keep the updated ASV list (`ASV_list_with_IDs.csv` in the tutorial), 
since this can be used when analyzing subsequent data sets to synchronyse the asv_ids.
* Output of different filtering steps: Compress them if they are too large. 
They are useful for tracing back the history of a sample or an ASV 
(see [HistoryBy](#historyby)) or making summary files (see [SummarizeBy](#summarizeby)).

## No or few sequences passing merge

If the amplicon is shorter than the reads, use `fastq_allowmergestagger=T`.
