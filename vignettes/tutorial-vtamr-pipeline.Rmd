---
title: "Tutorial - vtamR pipeline"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Tutorial - vtamR pipeline}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo=TRUE,
  eval=FALSE,
  comment = "#>"
)
```


## Summary

**vtamR** is a revised, completed version of 
[VTAM](https://onlinelibrary.wiley.com/doi/10.1111/1755-0998.13756) 
(Validation and Taxonomic Assignation of Metabarcoding Data) 
rewritten in R. It is a complete metabarcoding pipeline:

* Sequence analyses from raw fastq files of amplicon sequences till 
Amplicon Sequence Variant ([ASV](#glossary)) table of validated ASVs assigned 
to taxonomic groups.
* Handles technical or biological replicates of the same sample
* Uses positive and negative control samples to fine tune the filtering and 
reduce [false positive](#glossary) and [false negative](#glossary) occurrences.
* Can pool multiple data sets (results of earlier analyses)
* Can pool results from overlapping markers

**Novelties compared to VTAM:**

* As it is a series of R functions, vramR is highly adaptable to 
include/exclude and order different steps of the analyses
* Includes swarm for denoising
* Graphic options
* Include functions to get statistics of each filtering steps 
(read and variant count etc.)
* The notion of marker and run has been dropped to simplify the analyses

## Installation

Please, follow the instructions in the 
[installation](installation.html) vignette.

## Tutorial

### Set up

```{r setup}
library(vtamR)
```

**Set general parameters**

```{r set_path_win, eval=FALSE}
# Windows
cutadapt_path <- "C:/Users/Public/cutadapt"
vsearch_path <- "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/vsearch"
blast_path <- "C:/Users/Public/blast-2.16.0+/bin/blastn"
swarm_path <- "C:/Users/Public/swarm-3.1.5-win-x86_64/bin/swarm"
num_threads <- 4
sep <- ","
```

```{r set_path_linux}
# Bombyx
cutadapt_path <- "~/miniconda3/envs/vtam/bin/cutadapt" # v3.4
vsearch_path <- "~/miniconda3/envs/vtam/bin/vsearch" # v2.15.1
blast_path <- "~/miniconda3/envs/vtam/bin/blastn" # v2.10.1+
swarm_path <- "swarm" # v2.1.12
num_threads <- 8
sep <- ","
```

* Adapt the path to third party programs according to your installation
(See [Installation](installation.html)).
* `num_threads` is the number of CPUs for multithreaded programs
* `sep` is the separator used in csv files

Set the path to the database and the accompanying taxonomy file.
See [TaxAssign reference data base](installation.html#taxassign-reference-data-base)

```{r}
taxonomy <- "COInr_db/COInr_for_vtam_2025_05_23_dbV5/COInr_for_vtam_taxonomy.tsv"
blast_db <- "COInr_db/COInr_for_vtam_2025_05_23_dbV5/COInr_for_vtam"
```

* `taxonomy` CSV file with taxonomic information
* `blast_db` BLAST database
* Details are in [Reference database for taxonomic assignments section](#reference-database-for-taxonomic-assignments)

**Set input data**

The demo files bellow are included in the vtamR package, hence the use of 
`system.file()`. Use them to check if everything is all right.
When using your own data just enter the filenames.

```{r}
fastq_dir <- system.file("extdata/demo/fastq", package = "vtamR")
outdir <- "~/vtamR_demo/out_mfzr"
fastqinfo <-  system.file("extdata/demo/fastqinfo_mfzr.csv", package = "vtamR")
mock_composition <-  system.file("extdata/demo/mock_composition_mfzr.csv", package = "vtamR")
asv_list <-  system.file("extdata/demo/asv_list.csv", package = "vtamR")
```

The above file are included in the github repository.

* `fastq_dir` Directory containing the input fastq files.
* `outdir` Name of the output directory.
* [fastqinfo](#fastqinfo) CSV file with information on input files, primers, 
tags, samples.
* [mock_composition](#mock_composition) CSV file with expected ASVs in mock samples. 
See [make-mock-composition-file](make-mock-composition-file.html) vignette to see
how to create this file.
* [asv_list](#asv_list) CSV file with ASVs from earlier data sets. Optional.


**Check the format of input files**

The `CheckFileinfo` function tests if all obligatory columns are present, 
and makes some sanity checks. This can be helpful, since these files are produced 
by the users, and may contain errors difficult to spot by eye.

```{r}
CheckFileinfo(file=fastqinfo, dir=fastq_dir, file_type="fastqinfo")
CheckFileinfo(file=mock_composition, file_type="mock_composition")
CheckFileinfo(file=asv_list, file_type="asv_list")
```

### Merge - Demultiplex

I suppose that you have [installed third party programs](Installation_vtamR.html) 
and followed the [Getting started section](#getting-started).

According to your wetlab and sequencing protocol fastq files can 
contain one or more [sample-replicates](#glossary), and sequences may or may 
not contain [tags](#glossary) (for [demultiplexing](#glossary)) and primer sequences. 
In the following sections I show **3 different scenarios** to obtain the 
[read_count_df](#read_count_df) data frame, which the input to the filtering steps.

#### One sample per fastq - no tag - no primer

Read pairs should be quality filtered merged and written to fasta format.
This can be done by the `Merge` function. 

See the help (`?Merge`) for setting the correct parameters for quality filtering.

* [fastqinfo](#fastqinfo) is either a csv file, or a data frame. The key 
information for `Merge` is the list of the fastq file pairs that should be merged. 
The `tag_fw`, `primer_fw`, `tag_rv`, `primer_rv` are irrelevant in this case, 
just fill them with `NA`.
* `fastq_dir` is the directory containing the input fastq files.
* [sortedinfo_df](#fastainfo) is the output of `Merge`. It is the updated 
version of fastqinfo, where fastq file names have been replaced by fasta file 
names and the read counts are included for each file. 
This data frame and the files listed in it are the input for `Dereplicate`.


```{r, eval=FALSE}
merged_dir <- file.path(outdir, "merged")
sortedinfo_df <- Merge(fastqinfo, 
                       fastq_dir=fastq_dir, 
                       vsearch_path=vsearch_path, 
                       outdir=merged_dir
                       )
```


#### One sample per fastq - primer - no tag

Read pairs should be quality filtered, merged and written to fasta format by 
`Merge` function as in the [previous section](#one-sample-per-fastq---no-tag---no-primer).

Then the `TrimPrimer` function will trim the primers from the reads. 
See the help (`?TrimPrimer`) for setting the correct parameters for primer trimming.

* [fastqinfo](#fastqinfo) Either a csv file, or a data frame.  
The key information for `Merge` is the list of the fastq file pairs that should be merged. 
The `primer_fw`, `primer_rv` columns are irrelevant in this case, just fill them with `NA`.
* `fastq_dir` Directory containing the input fastq files.
* [fastainfo_df](#fastainfo) is the output of `Merge`. 
It is the updated version of fastqinfo, where fastq file names have been 
replaced by fasta file names.
* `fasta_dir` Directory containing he input fasta files for `TrimPrimer`. 
This directory is created by `Merge`.
* If `check_reverse` is TRUE, `TrimPrimer` checks the reverse complementary stand as well.
* `sortedinfo_df` is  updated version of fastainfo. 
This data frame and the files listed in it are the input for `Dereplicate`.

```{r, eval=FALSE}
# merge and quality filter
merged_dir <- file.path(outdir, "merged")
fastainfo_df <- Merge(fastqinfo, 
                      fastq_dir=fastq_dir, 
                      vsearch_path=vsearch_path, 
                      outdir=merged_dir
                      )

# trim primers
sorted_dir <- file.path(outdir, "sorted")
sortedinfo_df <- TrimPrimer(fastainfo_df, 
                            fasta_dir=merged_dir, 
                            outdir=sorted_dir, 
                            cutadapt_path=cutadapt_path, 
                            vsearch_path=vsearch_path, 
                            check_reverse=T
                            )
```

#### Several samples per fastq - tags - primers

Read pairs should be quality filtered, merged and written to fasta format as 
in the [previous sections](#one-sample-per-fastq---no-tag---no-primer).

Then the `SortReads` function will [demultiplex](#demultiplexing) the fasta 
files according to the [tag](#tag) combinations and [trim](#trimming) 
the primers from the reads. See the help (`?SortReads`) for setting the 
correct parameters for demultiplexing and primer trimming:

* [fastqinfo](#fastqinfo) Either a csv file, or a data frame. 
The key information for `Merge` is the list of the fastq file pairs that should be merged.
* `fastq_dir` Directory containing the input fastq files.
* [fastainfo_df](#fastainfo) is the output of `Merge`. 
It is the updated version of fastqinfo, where fastq file names have been 
replaced by fasta file names.
* `fasta_dir` Directory containing the input fasta files for `TrimPrimer`. 
This directory is created by `Merge`.
* If `check_reverse` is TRUE, `SortReads` checks the reverse complementary stand as well.
* `sortedinfo_df` is  updated version of fastainfo. 
This data frame and the files listed in it are the input of the `Dereplicate`.


```{r}
# merge and quality filter
merged_dir <- file.path(outdir, "merged")
fastainfo_df <- Merge(fastqinfo, 
                      fastq_dir=fastq_dir, 
                      vsearch_path=vsearch_path, 
                      outdir=merged_dir
                      )

# demultiplex, trim tags and pimers
sorted_dir <- file.path(outdir, "sorted")
sortedinfo_df <- SortReads(fastainfo_df, 
                           fasta_dir=merged_dir, 
                           outdir=sorted_dir, 
                           check_reverse=TRUE, 
                           cutadapt_path=cutadapt_path, 
                           vsearch_path=vsearch_path
                           )
```


### Dereplicate

At this point, you should have one fasta file per [sample-replicate](#glossary) 
containing merged reads. Tags and primers have been removed from the reads.

The fasta files then should be [dereplicated](#glossary) and a numerical ID 
([asv_id](#glossary)) assigned to each [ASV](#glossary). 
The output is a data frame ([read_count_df](#read_count_df)). 
This can also be written to a csv file if `outfile` is given. 
It is a good idea to write this file, since it is the starting point of data filtering. 
You can experiment with different filtering strategies starting from this file, 
without the need of re-running the long steps of [merging](#glossary) and 
[demultiplexing](#glossary) and [trimming](#glossary) reads.


If you want to keep the IDs of ASVs from earlier analyses, you can give a file 
containing earlier ASVs and their IDs ([asv_list](#asv_list)). 
The completed ASV list is written to `updated_asv_list` file.

When providing only the file name to `updated_asv_list`, but not to `asv_list`, 
the function will write the complete list of ASVs. 
This can be useful later when analyzing subsequent data sets to homogenize 
ASV IDs between data sets.


```{r}
outfile <- file.path(outdir, "1_before_filter.csv")
updated_asv_list <- file.path(outdir, "ASV_list_with_IDs.csv")

read_count_df <- Dereplicate(sortedinfo_df, 
                             dir=sorted_dir, 
                             outfile=outfile, 
                             asv_list=asv_list, 
                             updated_asv_list=updated_asv_list
                             )
```


### Filter

vtamR has a large number of functions to **filter out ASVs or occurrences of ASVs**
in a given sample-replicate, which are the results of 
**technical or biological problems associated with metabarcoding**. 

The different filters 

- can be applied to the original data set and then the results pooled together 
by using the `PoolFilters` function (accepting only occurrences that pass all filters) 
- or can be applied sequentially in the order defined according to your needs.

In this section, I give an example using most filtering options. 
It is up to you to construct your own pipeline.

Each function returns a data frame with the filtered output. 
You can also write the results to a csv file. This can be useful for tracking 
the presence/absence of some of the ASVs/samples throughout the analyses 
using the [HistoryBy](#historyby) and [SummarizeBy](#summarizeby) functions. 
I will start the names of the output files by a number to keep the order 
of the different steps and allow the [SummarizeBy](#summarizeby) function 
to make final statistics at each step of the analyses.

We can also **follow the evolution of the number of reads and ASVs** remaining 
in the data set after each filtering steps. 
Let's define a data frame (`stat_df`), that will contain this information, 
and we will complete it after each step. Then and add a line with 
information on the unfiltered data set.

* `params` String containing the major parameters of each filtering steps.
* `stage` String referring to the filtering step.

```{r}
stat_df <- data.frame(parameters=character(),
                      asv_count=integer(),
                      read_count=integer(),
                      sample_count=integer(),
                      sample_replicate_count=integer())

stat_df <- GetStat(read_count_df, stat_df, stage="Input", params=NA)
```


#### Denoising by swarm

**Swarm** ([Mahé et al., 2015](https://peerj.com/articles/1420/)) 
is a powerful and quick computer program to denoise the data set by clustering ASVs 
that are likely to be the result of sequencing errors to ASVs representing real 
biological sequences. It will considerably reduce the number of ASVs. 

By default, vtamR uses 1 as swam's `d` and the fastidious algorithm. 
See `?Swarm` on how to change this.
It is possible to run `Swarm` separately for each sample (`by_sample=TRUE`; default)
or at once for the whole data set. 
This second option is more efficient in reducing the number of ASVs but can 
cluster very similar real sequences appearing in different samples. 
If you are interested in intra-specific variability, I suggest `by_sample=TRUE` 
or skip this step.


```{r}
by_sample <- TRUE
outfile <- file.path(outdir, "2_Swarm_by_sample.csv")

read_count_df <- Swarm(read_count_df, 
                       outfile=outfile, 
                       swarm_path=swarm_path, 
                       num_threads=num_threads, 
                       by_sample=by_sample
                       )

stat_df <- GetStat(read_count_df, stat_df, stage="Swarm", params=by_sample)
print(stat_df)
```

Let's check the number the reduction of the number of variants after running `swarm`.
```{r}
print(stat_df)
```

We can also plot the number of reads per variant.
```{r}
Histogram_ReadCountByVariant(read_count_df, min_read_count=10, binwidth=100)
```

#### LFNglobalReadCount

Although, swarm has reduced considerably the number of ASVs, there are still 
many ASVs with low read count. 
The `LFNglobalReadCount` function filter out all ASVs with total read count 
bellow a threshold.

Let's eliminate [singletons](#glossary):

```{r}
global_read_count_cutoff = 2
outfile <- file.path(outdir, "3_LFNglobalReadCount.csv")

read_count_df <- LFNglobalReadCount(read_count_df, 
                                    cutoff=global_read_count_cutoff, 
                                    outfile=outfile
                                    )

stat_df <- GetStat(read_count_df, 
                   stat_df, 
                   stage="LFNglobalReadCount", 
                   params=global_read_count_cutoff
                   )
print(stat_df)
```

#### FilterIndel

This filter is **applicable only for coding sequences**. 
The idea is that if the length of the ASV differs from the most frequent ASV 
length by a number that is not a multiple of 3, it is likely to be an erroneous 
sequence or a pseudo-gene.

```{r}
outfile <- file.path(outdir, "4_FilterIndel.csv")

read_count_df <- FilterIndel(read_count_df, outfile=outfile)

stat_df <- GetStat(read_count_df, stat_df, stage="FilterIndel")
print(stat_df)
```

#### FilterCodonStop

This filter is **applicable only for coding sequences**. 
It checks the number of codon STOPs in all 3 reading frames, 
and eliminates ASVs with STOP in all three reading frames.

The numerical id of the genetic code can be chosen from 
[NCBI](https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi?chapter=cgencodes). 
By default, it is the invertebrate mitochondrial genetic code (5), 
since the STOP codons of this genetic codes are also STOP codons in almost all genetic codes.


```{r}
outfile <- file.path(outdir, "5_FilterCodonStop.csv")
genetic_code = 5

read_count_df <- FilterCodonStop(read_count_df, outfile=outfile, genetic_code=genetic_code)

stat_df <- GetStat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
print(stat_df)
```

#### FilterChimera

This function will run the `uchime3_denovo` function implemented in vsearch. 
It can be run sample by sample (`by_sample=T`; default), or on the whole data set at once. 

FilterChimera will eliminate ASVs detected as chimeras when running on the 
whole dataset at once (`by_sample=F`). 
If `by_sample=T`, ASVs are eliminated if they have been identified as chimeras 
in at least `sample_prop` proportion of the samples where they are present.

* `abskew` A chimera must be at least `abskew` times less frequent that the parental ASVs.


```{r}
abskew=2
by_sample = T
sample_prop = 0.8
outfile <- file.path(outdir, "6_FilterChimera.csv")

read_count_df <- FilterChimera(read_count_df, outfile=outfile, vsearch_path=vsearch_path, by_sample=by_sample, sample_prop=sample_prop, abskew=abskew)

params <- paste(abskew, by_sample, sample_prop, sep=";")
stat_df <- GetStat(read_count_df, stat_df, stage="FilterChimera", params=params)
print(stat_df)
```

#### FilterRenkonen

Let's eliminate aberrant replicates that are not similar to other replicates of the same sample. 

We will calculate the renkonen distances among all pairs of replicates within sample and plot them.

```{r}
# calculate renkonen distance among all replicates within sample 
renkonen_within_df <- MakeRenkonenDistances(read_count_df, compare_all=FALSE)
# density plot
renkonen_density_plot <- DensityPlot_RenkonenDistance(renkonen_within_df)
print(renkonen_density_plot)
# barplot
sortedinfo <- file.path(sorted_dir, "sortedinfo.csv")
renkonen_barplot <- Barplot_RenkonenDistance(renkonen_within_df, sample_types=sortedinfo, x_axis_label_size=6)
print(renkonen_barplot)
```

In this example, we have only a few distances, so the density plot is not very relevant. 
The barplot indicates that the replicate 1 of the `tnegtag` sample (negative control) 
is distant from the other two replicates. Lets, establish the Renkonen cutoff 
value to 0.4. We will filter out replicates that have Renkonen distances above 
this cutoff to most other replicates of the sample. In this case, 
replicate 1 of the tnegtag sample will be eliminated.

```{r}
outfile <- file.path(outdir, "7_FilterRenkonen.csv")
cutoff <- 0.4

read_count_df <- FilterRenkonen(read_count_df, outfile=outfile, cutoff=cutoff)

stat_df <- GetStat(read_count_df, stat_df, stage="FilerRenkonen", params=cutoff)
print(stat_df)
```

#### FilterPCRerror

If you have done the denoising step by **swarm**, this function probably redundant.

`FilterPCRerror`filter out an ASV if it is very similar to another, more frequent ASV.
`max_mismatch` argument tells how many mismatches are allowed between a pair of ASV
to be evaluated for PCR error. If the proportion of the read_counts of two similar 
ASVs is less or equal to `pcr_error_var_prop`, the less abundant ASV is flagged 
as a PCR error.

**OptimizePCRerror**

The default value of `pcr_error_var_prop` (0.1) is arbitrary. 
To choose a value adapted to your data set, you can use the
`OptimizePCRerror` function.

`OptimizePCRerror`, will find all highly similar ASV pairs (`max_mismatch=1` by default)
within a mock sample, where one ASV is expected, and the other is not. 
Their `read_count` ratio is printed in the output file. 

The function considers only ASVs with more than `min_read_count` 
reads in the sample to avoid a ratio based on low read counts that are more 
influenced by stochastic events and will be probably filtered out anyway.

See details using the help: `?OptimizePCRerror`. 

**Note:** I use `max_mismatch=2` in this example, 
to have some output in my very small (and artificial) data set,
but I recommend you to use `max_mismatch=1`.

**Note**: If you do not know the exact sequences of the species present in your 
mock samples, and thus, you do not have the [mock_composition](#mock_composition)
file yet, read the vignette [make-mock-composition-file](tutorial-vtamr-pipeline.html) 
that explains how to this file.

```{r}
outfile <- file.path(outdir, "OptimizePCRerror.csv")
OptimizePCRerror_df <- OptimizePCRerror(read_count_df, 
                                        mock_composition=mock_composition, 
                                        vsearch_path=vsearch_path, 
                                        outfile=outfile, 
                                        max_mismatch=2, 
                                        min_read_count=5
                                        )
```

**FilterPCRerror**

It seems that Swarm has done a good job. The highest read count ratio is 0.013 in the output of `OptimizePCRerror`. This should be taken as a lower limit to `pcr_error_var_prop`. Let's use 0.05 for filtering out PCR errors sample by sample.

See the help for more detail `?FilterPCRerror`.

**Note:** Again, I use `max_mismatch<-2` in this example, since I have used this value
for `OptimizePCRerror`, but I recommend you to use `max_mismatch<-1`.

```{r}
pcr_error_var_prop <- 0.05
max_mismatch <- 2
outfile <- file.path(outdir, "8_FilterPCRerror.csv")

read_count_df <- FilterPCRerror(read_count_df, 
                                outfile=outfile, 
                                vsearch_path=vsearch_path, 
                                pcr_error_var_prop=pcr_error_var_prop, 
                                max_mismatch=max_mismatch
                                )

params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sep=";")
stat_df <- GetStat(read_count_df, stat_df, stage="FilterPCRerror", params=params)
print(stat_df)
```

#### LFNsampleReplicate

`LFNsampleReplicate` will eliminate occurrences with very low read counts 
compared to the total number of reads in the sample-replicate. 
The default cutoff proportion is 0.001. 
We can have an idea of the maximum value of this cutoff, 
by examining the proportions of expected ASVs in the mock samples using the 
`OptimizeLFNsampleReplicate` function.

**OptimizeLFNsampleReplicate**

**Note**: If you do not know the exact sequences of the species present in your 
mock samples, and thus, you do not have the [mock_composition](#mock_composition)
file yet, read the vignette [make-mock-composition-file](tutorial-vtamr-pipeline.html) 
that explains how to this file.

```{r}
outfile = file.path(outdir, "OptimizeLFNsampleReplicate.csv")
OptimizeLFNsampleReplicate_df <- OptimizeLFNsampleReplicate(read_count=read_count_df,
                                                            mock_composition=mock_composition,
                                                            outfile=outfile)
head(OptimizeLFNsampleReplicate_df)
```

The lowest proportion of read count of an expected ASV to the total read count
of its sample-relicate is 0.0049 
(Sequence of *Caenis pusilla* in the replicate 3 of tpos1 mock sample). 
A cutoff values for `LFNsampleReplicate` higher than this will eliminate some 
of the expected occurrences and create false negatives. 
Therefore, we choose 0.004 as a cutoff.

**LFNsampleReplicate**

```{r}
lfn_sample_replicate_cutoff <- 0.004
outfile <- file.path(outdir, "9_LFNsampleReplicate.csv")

read_count_df <- LFNsampleReplicate(read_count_df, 
                                    cutoff=lfn_sample_replicate_cutoff, 
                                    outfile=outfile
                                    )

stat_df <- GetStat(read_count_df, 
                   stat_df, 
                   stage="LFNsampleReplicate", 
                   params=lfn_sample_replicate_cutoff
                   )
print(stat_df)
```

#### FilterMinReplicate 1

To ensure repeatability, we can accept occurrences if they are present in at 
least `min_replicate_number` replicates of the sample (2 by default).

```{r}
min_replicate_number <- 2
outfile <- file.path(outdir, "10_FilterMinReplicate.csv")

read_count_df <- FilterMinReplicate(read_count_df, 
                                    min_replicate_number, 
                                    outfile=outfile
                                    )
stat_df <- GetStat(read_count_df, 
                   stat_df, 
                   stage="FilterMinReplicate", 
                   params=min_replicate_number
                   )
print(stat_df)
```

#### LFNvariant and LFNreadCount

The `LFNvariant` filter will eliminate occurrences with very low read counts 
compared to the total number of reads of the ASV. 
The default cutoff proportion is 0.001. 
This filter is designed to filter out occurrences present in the data set due 
to tag-jump or light inter sample contamination.

The `LFNreadCount` filter simply eliminates occurrences with read counts bellow 
a `cutoff` (10 by default). 

To find the best cutoff values for these two filters, 
the `OptimizeLFNreadCountLFNvariant` will count the number of 
[false positive](#glossary), [false negative](#glossary) and
[true positive](#glossary) occurrences using a series of combination of cutoff 
values of these two filters. To run this function, we need a 
[known_occurrences_df](#known_occurrences_df) data frame that lists all 
occurrences that are known to be a FP, TP.

**MakeKnownOccurrences**

The `MakeKnownOccurrences` function will identify [false positive](#glossary), 
[false negative](#glossary) and [true positive](#glossary) 
occurrences in controls samples (mock and negative). 
Some false positives can also be identified in real samples if 
samples of different habitats are included in the data sets. 

The false positive and the true positive occurrences are written to the 
`known_occurrences` data frame (or file), false negatives to the 
`missing_occurrences` data frame (or file), and a `performance_metrics`
data frame is also produced with the count of these occurrences. 
For details see `?MakeKnownOccurrences`.

This function takes a `read_count_samples` data frame as an input,
where the replicates of the same sample have been pooled (see `?PoolReplicates`)

**Note**: If you do not know the exact sequences of the species present in your 
mock samples, and thus, you do not have the [mock_composition](#mock_composition)
file yet, read the vignette [make-mock-composition-file](tutorial-vtamr-pipeline.html) 
that explains how to this file.

 
```{r}
# Pool replicates
read_count_samples_df <- PoolReplicates(read_count_df)

# Detect known occurrences
results <- MakeKnownOccurrences(read_count_samples = read_count_samples_df, 
                                sortedinfo=sortedinfo, 
                                mock_composition=mock_composition
                                )

# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
```


**OptimizeLFNreadCountLFNvariant**

The `LFNreadCount` and `LFNvariant` functions are run for a series of cutoff 
value combinations of the two filters, followed by `FilterMinReplicate`. 
For each parameter combination, the number of FN, TP, and FP is reported. 
Chose the parameter setting that minimizes, FN and FP.

Here we will use the default range of cutoffs to test, 
but you can set the minimum, the maximum and the increment for the cutoff 
values for both filters. (see `?OptimizeLFNreadCountLFNvariant`). 
We set `min_replicate_number` to 2, to eliminate non-repeatable 
occurrences among the three replicates of each sample.


```{r}
outfile = file.path(outdir, "OptimizeLFNreadCountLFNvariant.csv")

OptimizeLFNreadCountLFNvariant_df <- OptimizeLFNreadCountLFNvariant(
  read_count_df,
  known_occurrences=known_occurrences_df,
  outfile= outfile, 
  min_replicate_number=2
  )

head(OptimizeLFNreadCountLFNvariant_df)
```

**LFNvariant, LFNreadCount**

From the output, we choose 0.001 for the cutoff of `LFNvariant` 
and 10 for `LFNreadCount`, since this is the less stringent combination 
the keeps all expected occurrences (6 TP, no FN), and has less FP.

We will run the two filters on the same input data frame 
(for which the parameters has been optimized), 
and pool the results by accepting only occurrences that pass both 
filters by the `PoolFilters` function.

See `?LFNvariant, ?LFNreadCount` and `?PoolFilters` for details.

```{r}
## LFNvariant
lnf_variant_cutoff = 0.001
outfile <- file.path(outdir, "11_LFNvariant.csv")

read_count_df_lnf_variant <- LFNvariant(read_count_df, 
                                        cutoff=lnf_variant_cutoff, 
                                        outfile=outfile
                                        )
stat_df <- GetStat(read_count_df_lnf_variant, 
                   stat_df, 
                   stage="LFNvariant", 
                   params=lnf_variant_cutoff)

## LFNreadCount
lfn_read_count_cutoff <- 10
outfile <- file.path(outdir, "12_LFNreadCount.csv")

read_count_df_lfn_read_count <- LFNreadCount(read_count_df, 
                                             cutoff=lfn_read_count_cutoff, 
                                             outfile=outfile
                                             )
stat_df <- GetStat(read_count_df_lfn_read_count, 
                   stat_df, stage="LFNreadCount", 
                   params=lfn_read_count_cutoff
                   )

## Combine results
outfile <- file.path(outdir, "13_poolLFN.csv")
read_count_df <- PoolFilters(read_count_df_lfn_read_count, 
                             read_count_df_lnf_variant, 
                             outfile=outfile
                             )
stat_df <- GetStat(read_count_df, 
                   stat_df, 
                   stage="FilterLFN"
                   )
# delete temporary data frames
rm(read_count_df_lfn_read_count)
rm(read_count_df_lnf_variant)

print(stat_df)
```

#### FilterMinReplicate 2

Let's run again `FilterMinReplicate` to ensure repeatability among replicates 
of the sample (2 by default).

```{r}
# Set parameter values
min_replicate_number <- 2
outfile <- file.path(outdir, "14_FilterMinReplicate.csv")
# Run filter and get stats
read_count_df <- FilterMinReplicate(read_count_df, 
                                    min_replicate_number, 
                                    outfile=outfile
                                    )
stat_df <- GetStat(read_count_df, 
                   stat_df, 
                   stage="FilterMinReplicate", 
                   params=min_replicate_number
                   )
print(stat_df)
```

### Pool, TaxAssign and document

#### PoolReplicates

Let's pool replicates of the same sample. 
`PoolReplicates` function will take the mean non-zero read counts of each ASV 
over replicates of the same sample.

```{r}
outfile <- file.path(outdir, "15_PoolReplicates.csv")
read_count_samples_df <- PoolReplicates(read_count_df, 
                                        outfile=outfile
                                        )
stat_df <- GetStat(read_count_samples_df, 
                   stat_df, 
                   stage="PoolReplicates"
                   )
print(stat_df)
```

#### Get performance metrics

Run `MakeKnownOccurrences` again to get performance metrics (FP, FN, FP). 
This time we will write the output data frames to files as well. 
The `performance_metrics` file will give you the count of FP, FN, TP, 
accuracy and sensitivity. 
You can find false negatives in `missing_occurrences`, 
and true and false positives in `known_occurrences`.

```{r}
missing_occurrences <- file.path(outdir, "missing_occurrences.csv")
performance_metrics <- file.path(outdir, "performance_metrics.csv")
known_occurrences <- file.path(outdir, "known_occurrences.csv")
sortedinfo <- file.path(sorted_dir, "sortedinfo.csv")

results <- MakeKnownOccurrences(read_count_samples_df, 
                                sortedinfo=sortedinfo, 
                                mock_composition=mock_composition, 
                                known_occurrences=known_occurrences, 
                                missing_occurrences=missing_occurrences,
                                performance_metrics=performance_metrics
                                )
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
```

#### TaxAssign

If you haven't assigned ASV to taxa yet (before the optimization step),
it is time to do it. See [make-mock-composition-file vignette](make-mock-composition-file.html).
If you have already done it, you can use the same output as previously 
(`asv_tax` data frame) and skip this step.

For the format of `taxonomy` and `blast_db` check the 
[Reference database for taxonomic assignments](#reference-database-for-taxonomic-assignments) 
section. See the brief description of the algorithm with `?TaxAssign`.


```{r}
outfile <- file.path(outdir, "TaxAssign.csv")

asv_tax <- TaxAssign(asv=read_count_samples_df, 
                     taxonomy=taxonomy, 
                     blast_db=blast_db, 
                     blast_path=blast_path, 
                     outfile=outfile, 
                     num_threads=num_threads
                     )
```

### Print output

`WriteASVtable` will reorganize the `read_count_samples_df` data frame, 
with samples in columns and ASV in lines 
(from [long format](#glossary) to [wide format](#glossary)).

It is possible to add supplementary information as well:

* Taxonomic assignment (`asv_tax`)
* Total number of reads and samples for each ASV (`add_sums_by_asv`)
* Total number of reads and ASVs in each sample (`add_sums_by_sample`)
* Supplementary column for each mock sample with expected occurrences 
in each of them (`add_expected_asv`)
* Supplementary column for each sample that has been filtered out (`add_empty_samples`)

For more information see `?WriteAsVtable`

```{r}
outfile=file.path(outdir, "Final_asvtable_with_TaxAssign.csv")

asv_table_df <- WriteASVtable(read_count_samples_df, 
                              outfile=outfile, 
                              asv_tax=asv_tax, 
                              sortedinfo=sortedinfo, 
                              add_empty_samples=T, 
                              add_sums_by_sample=T, 
                              add_sums_by_asv=T, 
                              add_expected_asv=T, 
                              mock_composition=mock_composition
                              )
```

Print out the number of reads, ASVs, samples and replicates after each step.

```{r}
write.csv(stat_df, file = file.path(outdir, "stat_steps.csv"))
```

## Suppelmentary functions

### RandomSeq

Random select `n` sequences from each input fasta file. 
It can be used before or after demultiplexing (`SortReads`). 
However, if using `LFNvariant` and `LFNreadCount` which are partially 
based on the number of reads in negative controls, standardizing the number of 
reads among samples does not make sense. Thus I use `RandomSeq` after `Merge`, 
to get the same number of reads for each replicate series 
(same samples, different replicates in each fasta file).


* [fastainfo](#fastainfo) Either a csv file, or a data frame, with the following columns: 
tag_fw, primer_fw, tag_rv, primer_rv, sample, sample_type, habitat, replicate, fasta.
* `fasta_dir` Directory containing the input fasta files.
* `n` Number of sequences to be taken randomly (without replacement).


```{r}
randomseq_dir = file.path(outdir, "random_seq")
fastainfo_df <- RandomSeq(fastainfo_df, 
                          fasta_dir=merged_dir, 
                          outdir=randomseq_dir, 
                          vsearch_path=vsearch_path, 
                          n=10000
                          )
```

###  HistoryBy

This function scans all files in the `dir` that starts by a number. 
(See file names of the output files of the different [filtering steps](#filter)). 
It will select all lines were the `feature` (asv_id/asv/sample/replicate) 
has a `value` we are looking for.

**Examples**

*Get the history of asv_id (`feature`) 27 (`value`).*
```{r}
tmp_ASV_27 <- HistoryBy(dir=outdir, 
                        feature="asv_id", 
                        value="27"
                        )
```

*Same search by using the sequence of the ASV  (`feature`).*
```{r}
tmp_replicate_1 <- HistoryBy(dir=outdir, 
                             feature="asv",
                             value="CCTTTATTTTATTTTCGGTATCTGGTCAGGTCTCGTAGGATCATCACTTAGATTTATTATTCGAATAGAATTAAGAACTCCTGGTAGATTTATTGGCAACGACCAAATTTATAACGTAATTGTTACATCTCATGCATTTATTATAATTTTTTTTATAGTTATACCAATCATAATT"
                             )
```

*Get the history of the sample (`feature`) tpos1 (`value`).*
```{r}
tmp_sample_tpos1 <- HistoryBy(dir=outdir, 
                              feature="sample", 
                              value="tpos1"
                              )
```


### SummarizeBy

This function scans all files in the `dir` that starts by a number. 
See file names of the output files of the different [filtering steps](#filter)). 

It will group each file by a variable (asv/asv_id/sample/replicate) 
and summarize a `feature` (asv/asv_id/sample/replicate/read_count). 
If the `feature` is `read_count`, it will give the sum of the read counts 
for each value of the variable in each file. 
Otherwise, it returns the number of distinct values of the `feature` 
for each value of the variable in each file.

**Examples**

*Get the number of reads of each sample after each filtering steps.*

From this data frame, we can see that the negative control sample become 
"clean" after the LFN filters, and there is a considerable variation 
among the number of reads of different real samples.

```{r}
read_count_by_sample <- SummarizeBy(dir=outdir, 
                                    feature="read_count", 
                                    grouped_by="sample"
                                    )
```


*Get the number asv for each sample after each filtering steps.*

From this data frame, we can see that the negative control sample become "clean" 
after the LFN filters, and the mock samples has 10 ASVs at the end.
```{r}
asv_by_sample <- SummarizeBy(dir=outdir, 
                             feature="asv", 
                             grouped_by="sample"
                             )
```


*Get the number asv_id for each replicate after each filtering steps.*

We can see that number of ASVs are comparable in different replicates.
```{r}
asvid_by_replicate <- SummarizeBy(dir=outdir, 
                                  feature="asv_id", 
                                  grouped_by="replicate"
                                  )
```

### UpdateASVlist

Pools unique `asv` - `asv_id` combinations from the input data frame (`read_count_df`) 
and from the input file (`asv_list`).

The input file is typically a CSV file containing ASVs seen in earlier data sets 
with their `asv_id`.
If there is a conflict within or between the input data the function quits 
with a error message. Otherwise writes the `updated_asv_list` to the outfile.

The safest option of avoiding incoherence between `asv_ids` of earlier 
and present runs is to use the `asv_list` and `updated_asv_list` 
parameters in the `Dereplicate` function as it is done in this [tutorial](#dereplicate). 
This will synchronize the `asv_id` in this run with earlier ones, 
and writes an updated file with all ASV from earlier and the present data set. 
In this case, calling the `UpdateASVlist` function is not necessary, since it is 
automatically called from `Dereplicate`. 

However, if you are analyzing a large number of very large data sets, 
the complete `asv_list` will grow quickly, and might cause memory issues. 
Most of the ASVs in this list are singletons, and filtered out during the analyses. 
Therefore, you can opt for homogenizing the `asv_id`s with earlier runs by the 
`Dereplicate` function, without writing an `updated_asv_list`. 
Then update the `asv_list` after the first steps of filtering 
(e.g. `Swarm`, `LFNglobalReadCount`) to keep only ASVs that are more frequent 
and more likely to appear in future data sets. It is still a quite safe option, 
and reduces greatly the number of ASVs kept in this file.


```{r}
updated_asv_list <- file.path(outdir, 
                              "updated_ASV_list.csv"
                              )
UpdateASVlist(read_count_df,
              asv_list=asv_list, 
              outfile=updated_asv_list
              )
```

### PoolDatasets

**More than one overlapping marker**

This function pools different data sets and it is particularly useful, 
if the results should be pooled from more than one overlapping markers. 
In that case, ASVs identical on their overlapping regions are pooled into groups, 
and different ASVs of the same group are represented by their centroid 
(longest ASV of the group). Pooling can take the mean read counts of the ASVs 
(`mean_over_markers=T`; default) or their sum (`mean_over_markers=F`).

The function takes several input CSV files, each in [long format](#glossary) 
containing `asv_id`, `sample`, `read_count` and `asv` columns. 
The file names should be organized in data frame, with the marker names for each file.

* `outfile` Name of the output CSV file with the pooled data set 
(`asv_id`, `sample`, `read_count`, `asv`).
ASVs are grouped to the same line if identical in their overlapping region, 
and only the centroids appear in the `asv` column.
* `asv_with_centroids` Name of the output CSV file containing each of the the 
original ASVs (with samples, markers, and read_count) as well as their centroids.
* The data frame returned by the function corresponds to the `outfile`.

```{r}
files <- data.frame(file=c("vtamR_test/out_mfzr/15_PoolReplicates.csv",
                           "vtamR_test/test/15_PoolReplicates_ZFZR.csv"),
                    marker=c("MFZR", 
                             "ZFZR")
                    )

outfile <- file.path(outdir, "Pooled_datasets.csv") 
asv_with_centroids <- file.path(outdir, 
                                "Pooled_datasets_asv_with_centroids.csv"
                                ) 

read_count_pool <- PoolDatasets(files, 
                                outfile=outfile, 
                                asv_with_centroids=asv_with_centroids, 
                                mean_over_markers=T,
                                vsearch_path=vsearch_path
                                )
```

**Only one marker**

Pooling the results of different data sets of the same marker is very simple. 
Basically, the input files (in [long format](#glossary) with 
`asv`, `asv_id`, `sample`, `read_count` columns) are concatenated. 
The `PoolDatasets` function also  checks if sample names are unique. 
If not, it sums the read count of the same sample and same ASV, but returns a warning.

The output file or data frame can be rearranged to [wide format](#glossary) by the [WriteASVtable](#print-output) function.

### CountReadsDir

Count the number of reads in `fasta` or `fastq` files found in the input directory.
Input files can be gz compressed or uncompressed, but zip files are not supported.

The [fastainfo](#fastainfo) and the [sortedinfo](#sortedinfo) 
files contain the number of reads after `Merge` or `SortReads`, 
so no need to run `CountReadsDir` separately. 
This function can be useful for counting the number of reads in the input `fastq` files.

* `dir` Input directory containing the fasta of fastq files
* `file_type` [fasta/fastq]
* `pattern` Regular expression; Check only files for `pattern` in the file name

```{r}
dir <- "vtamR_test/data"
df <- CountReadsDir(dir, 
                    pattern="_fw.fastq.gz", 
                    file_type="fastq"
                    )
```
 
### Barplot_ReadCountBySample

Make a bar plot of read counts by [sample](#glossary) (`sample_replicate=F`) 
or [sample-replicate](#glossary) (`sample_replicate=T`).
Can use different colors for different sample types ([real/mock/negative](#glossary))

* [read_count_df](#read_count_df) Input data frame
* `sample_types` data frame or CSV file containing info on sample types for each sample

```{r}
sortedinfo <- file.path(sorted_dir, "sortedinfo.csv")
Barplot_ReadCountBySample(read_count_df=read_count_df, 
                          sample_replicate=F, 
                          sample_types=sortedinfo
                          )
```


### Histogram_ReadCountByVariant

Histogram of read counts by ASV

* [read_count_df](#read_count_df) Input data frame
* `min_read_count` Ignore variants with read count bellow this value
* `binwidth` Width of bins

```{r}
Histogram_ReadCountByVariant(read_count_df, 
                             min_read_count=10, 
                             binwidth=1000
                             )
```


### Renkonen distances

Calculate the Renkonen distances among all replicates (compare_all=TRUE) 
or among replicates of the same sample (compare_all=FALSE) and plot them.

* [read_count_df](#read_count_df) Input data frame.
* `sample_types` Data frame or CSV file containing info on sample types for each sample.

```{r}
renkonen_within_df <- MakeRenkonenDistances(read_count_df, compare_all=FALSE)
Barplot_RenkonenDistance(renkonen_within_df, sample_types=sortedinfo)
DensityPlot_RenkonenDistance(renkonen_within_df)

renkonen_all_df <- MakeRenkonenDistances(read_count_df, compare_all=TRUE)
DensityPlot_RenkonenDistance(renkonen_all_df)
```

 

## I/O files and data frames

### fastqinfo

CSV file with information on input fastq files, primers, tags, 
samples with the following columns. 
Each line corresponds to a sample-replicate combination.

 * tag_fw: Sequence tag on the 5' of the fw read (NA if file is already demultiplexed)
 * primer_fw: Forward primer (NA if primer has been trimmed)
 * tag_rv: Sequence tag on the 3' of the rv read (NA if file is already demultiplexed)
 * primer_rv: Reverse primer (NA if primer has been trimmed)
 * sample: Name of the sample (alpha-numerical)
 * sample_type: [real/mock/negative](#glossary)
 * habitat: If real or mock samples are from different habitats that cannot 
 contain the same type of organisms (e.g. terrestrial vs. marine), 
 this information is used for detecting false positives.
 Use NA otherwise. Use NA for negative controls.
 * replicate: Numerical id of a replicate within sample 
 (e.g. Sample1 can have replicate 1, 2 or 3)
 * fastq_fw: Forward fastq file
 * fastq_rv: Reverse fastq file


### fastainfo

CSV file with information on input fasta files, primers, tags,
samples with the following columns. 
Each line corresponds to a sample-replicate combination.

 * tag_fw: Sequence tag on the 5' of the fw read (NA if file is already demultiplexed)
 * primer_fw: Forward primer (NA if primer has been trimmed)
 * tag_rv: Sequence tag on the 3' of the rv read (NA if file is already demultiplexed)
 * primer_rv: Reverse primer (NA if primer has been trimmed)
 * sample: Name of the sample (alpha-numerical)
 * sample_type: [real/mock/negative](#glossary)
 * habitat: If real or mock samples are from different habitats that 
 cannot contain the same type of organisms (e.g. terrestrial vs. marine), 
 this information is used for detecting false positives. Use NA otherwise. 
 Use NA for negative controls.
 * replicate: Numerical id of a replicate (e.g. Sample1 can have replicate 1, 2 or 3)
 * fasta: Fasta file
 * Read_count: Number of reads in the fasta file. Optional.

### sortedinfo

CSV file with information on demultiplexed and primer trimmed fasta files 
and samples with the following columns. Each line corresponds to a 
sample-replicate combination.

 * sample: Name of the sample (alpha-numerical)
 * sample_type: [real/mock/negative](#glossary)
 * habitat: If real or mock samples are from different habitats that 
 cannot contain the same type of organisms (e.g. terrestrial vs. marine), 
 this information is used for detecting false positives. Use NA otherwise. 
 Use NA for negative controls.
 * replicate: Numerical id of a replicate (e.g. Sample1 can have replicate 1, 2 or 3)
 * fasta: Fasta file
 * Read_count: Number of reads in the fasta file. Optional.

### mock_composition

CSV file with the following columns.

 * sample: Name of the [mock](#glossary) sample
 * action: 
      * keep: Expected ASV in the mock, that should be kept in the data set
      * tolerate: ASV that can be present in a mock, but it is not essential 
      to keep it in the data set (e.g. badly amplified organism)
 * [asv](#glossary): sequence of the ASV
 * taxon: Optional; Name of the organism
 * [asv_id](#glossary): Optional; If there is a conflict between
 asv and asv_id, the asv_id is ignored

### known_occurrences
CSV file or data frame with the following columns.

 * [sample](#glossary): Name of the sample
 * action:
      * keep: Expected ASVs in a mock sample (corresponds to True Positives)
      * delete: False Positive occurrences: unexpected ASV in a mock sample; 
      all occurrences in negative controls; occurrences in real samples 
      corresponding to an incompatible habitats (e.g. an ASV mostly present 
      in marine samples is unexpected in a freshwater sample)
 * [asv_id](#glossary)
 * [asv](#glossary): sequence of the ASV
 
### asv_list

This file lists ASVs and their IDs from earlier data sets. When provided 
(optional), identical ASVs in the present and earlier data sets have the 
same ID. New ASVs (not present in asv_list) will get unique IDs not present in asv_list. 
It is a CSV file with the following columns:

 * [asv_id](#glossary): Unique numerical ID of the ASV
 * [asv](#glossary): ASV sequence
 

### read_count_df

Data frame with the following columns:

 * [asv](#glossary): Sequence of the ASV
 * [asv_id](#glossary): Numerical ID of the ASV
 * [sample](#glossary): Sample name
 * [replicate](#glossary): Replicate within sample (Numerical)
 * read_count: Number of reads of the ASV in the Sample-Replicate

### Reference database for taxonomic assignments

A data base is composed of two elements. A BLAST database (`blast_db`) 
and a `taxonomy` file.

`blast_db` can be produced using the `makeblastdb` command of BLAST:

```{bash, eval=FALSE}
makeblastdb -dbtype nucl -in [FASTA_FILE] -parse_seqids -taxid_map [TAXID_FILE] -out [DB_NAME]
```

 * FASTA_FILE is a fasta file containing reference sequences.
 * TAXID_FILE is a tab separated file with sequence IDs and the 
 corresponding numerical [taxIDs](#glossary).
 * DB_NAME is the name of the newly created BLAST database.

`taxonomy` is a **tab separated** csv file with the following columns:

 * tax_id: Numerical Taxonomic ID. It can be a valid 
 [NCBI taxID](https://www.ncbi.nlm.nih.gov/taxonomy), 
 or arbitrary negative numbers for taxa not in NCBI.
 * parent_tax_id: taxID of the closest parent of tax_id.
 * rank: taxonomic rank (e.g. species, genus, subgenus, no_rank).
 * name_txt: Scientifc name of the taxon.
 * old_tax_id: taxIDs that have been merged to the tax_id by NCBI; 
 if there is more than one for a given tax_id, make one line for each old_tax_id.
 * taxlevel: Integer associated to each major taxonomic rank. 
 (0 => root, 1=> superkingdom, 2=> kingdom, 3=> phylum, 4=> class, 5=> order, 
 6=> family, 7=> genus, 8=> species). 
 Levels in between have 0.5 added to the next highest level 
 (e.g. 5.5 for infraorder and for superfamily).


**A ready to use COI database** in BLAST format and the associated taxonomy 
file can be downloaded from [https://osf.io/vrfwz/](https://osf.io/vrfwz/). 
It was created using [mkCOInr](https://github.com/meglecz/mkCOInr). 
It is also possible to make a [customized database](https://mkcoinr.readthedocs.io/en/latest/content/tutorial.html#customize-database) 
using mkCOInr. It can be dowloaded and extracted manually of using the 
`download_osf` function of vtamR.




## Options

* `compress`: If TRUE, output files of the `Merge`, `RandomSeq` and `SortReads` 
functions are compressed. This saves space, but compressing/decompressing
increases run time in some cases. I suggest to avoid compressing intermediate 
files and delete/compress them as soon as the analyses are finished. 
(See more in the [troubleshooting section](#troubleshooting)). 
The input fasta and fastq files of these functions can be compressed 
(`.gz`, `.bz2`, but NOT .zip) or uncompressed files and compression 
is automatically detected based on the file extension.


* `delete_tmp`: Delete automatically intermediate files. TRUE by default.

* `sep`: Separator used in sv files. Use the same separate in all CSV files 
except for the [taxonomy](#reference-database-for-taxonomic-assignments) 
file of the reference database, which is tag separated.

* `quiet`: If TRUE print as little information to the terminal as possible. 
TRUE by default.



## Glossary


* **ASV**: Amplicon Sequence Variant. Unique sequnece, caracterized by 
the number of reads in each sample-replicate.
* **asv_id**: Unique numerical ID of an ASV.
* **demultiplexing**: Sorting reads in a fasta (or fastq) file to different 
sample-replicates according to the tags present at their extremities.
* **dereplication**: The merged reads contain many identical sequenes. 
The dereplication reduces the dataset to unique sequences (ASV), and count 
the number of reads for each ASV in each sample-replicate.
* **data frame (or csv) in long format**: The read_count_df contains one line 
for each occurrence with asv_id, asv, sample, replicate, read_count columns. 
This is the standard format of keeping occurrences throughout the analyses, 
and it is smaller than the wide format, 
if there are many ASVs present in only one or few samples.
* **data frame (or csv) in wide format**: The read_count_df can be rearranged 
in wide format, where lines are ASVs, columns are sample(-replicates) and cells
contain read counts. It is a more human friendly format and it is the base of 
writing an ASV table, where this information can be completed by taxonomic 
assignments and other informations.
* **false negative occurrence**: An expected ASV in a mock sample that is not 
found in the data.
* **false positive occurrence**: Un expected presence an ASV in a Sample.
    * all occurrences in all negative contols, 
    * unexpected occurrences in mock samples
    * presence of an ASV in an incompatible habitat (e.g. ASV with high read 
    count in samples of habitat 1 and low read count in habitat 
    2 is considered as FP in habitat 2).
* **habitat**: Habitat type of the organisms in real or mock samples. 
Use this only if organisms of the different habitats cannot appear in 
another haditat of the dataset. Use NA otherwise.
* **Long format**: Each row is a single measurement. Typically, 
there are columns like ASV, sample, and read_count. This format is tidy 
and works well with many tidyverse functions.
* **Low Frequency Noise (LFN)**: ASVs present in low frequencies, 
likely to be do to errors.
* **merge**: Assemble a forward and reverse read pair to one single sequence.
* **mock sample**: An artificial mix of DNA of know organisms.
* **negative sample**: Negative control.
* **real sample**: An environmental sample.
* **replicate**: Technical or biological replicate of a sample. 
Replicates must have numerical identifiers. (e.g. sample tops1 have replicate 1, 2 and 3).
* **sample** Name of the environmental or control sample.
* **sample-replicate** Each sample can have technical of biological replicates. 
sample-replicate refers to one replicate of a given sample.
* **sample_type**: [real/mock/negative](#glossary)
* **singleton**: ASV with a single read in the whole data set.
* **tag** Short sequence at the extremity of the amplicon. 
It is used at the demultiplexing step to identify the sample-replicate, where the read comes from.
* **taxIDs**: Numerical taxonomic identifier. It can be a valid [NCBI taxID](https://www.ncbi.nlm.nih.gov/taxonomy), 
or arbitrary negative numbers for taxa not in NCBI.
* **trimming**: Cut the extremities of the sequences. It can be based of 
sequences quality, or on the detection of a tag or primer.
* **true positive occurrence**: An expected occurrence in a mock sample.
* **Wide format**: In vtamR wide format data frames, each row is an ASV), 
and each column represents a sample. Read counts are in the cells.




## Troubleshooting

### tmp_FunctionName_######## in vtamR directory

These are temporary directories, that are automatically deleted at the end of 
the function. In some cases, if the function ends prematurely due to an error, 
the directory is not deleted. Just delete them manually.

### LFNvariant eliminate most occurrences of a frequent ASV

`LFNvariant` will filter out all occurrences where

`(number of reads of the ASV in a sample-replicate) / (total number of read of the ASV) < cutoff` 

As a consequence, if an ASV is present in most samples, there are many samples 
in the data set and the cutoff is relatively high, most occurrences can fall 
bellow the cutoff and the total read count of the ASV will decrease strongly at this step.

If the proportion of the read count of an ASV in the output compared to 
the input is less than `min_read_count_prop`, vtamR prints out a Warning. 
Take a close look at the variant. It can be the result of a general contamination, 
and in this case, the whole ASV could/should be eliminated from the data set. 
Otherwise, you might want to reduce the cutoff value for `LFNvariant`.

### Memory issues for running swarm on the whole dataset

Possible workarounds:

* Try to run Swarm sample by sample (`by_sample=T`). 
* Run `global_read_count` first to eliminate singletons then swarm. 

Both solutions make swarm a bit less efficient, but at least your analyses can get through.

### Memory issues in Merge on Windows

If input fastq files are very large and do not go through `Merge`, 
try working with uncompressed input files.


### Files to keep

At the first steps (Merge, RandomSeq, Sortreads), many files are produced,
and some of them can be very large, especially before the dereplication 
(before `Dereplicate`).

Compressing and decompressing between the different steps can take a while 
and can have memory issues in some cases (especially on Windows). Therefore, 
vtamR uses by default uncompressed files. This behavior can be changed by the 
[compress](#options) option.

Here is a list of files I would tend to keep, delete or compress.

* `Merge`: Delete the output fasta files, but keep `fastainfo.csv` for 
information on read counts after `Merge`.
* `RandomSeq`: Compress fasta files, but keep them to ensure reproducibility 
in case of re-analyse.
* `SortReads`: Delete the output fasta files, but keep `sortedinfo.csv` for 
information on read counts after `SortReads`. 
* `Dereplicate`: Write the dereplicated info to a file 
(`1_before_filter.csv` in the tutorial) and compress it after the analyses. 
This is an important file, since you can restart the analyses from here,
without re-doing the longest merging, and demultiplexing steps. 
Compress and keep the updated ASV list (`ASV_list_with_IDs.csv` in the tutorial), 
since this can be used when analyzing subsequent data sets to synchronyse the asv_ids.
* Output of different filtering steps: Compress them if they are too large. 
They are useful for tracing back the history of a sample or an ASV 
(see [HistoryBy](#historyby)) or making summary files (see [SummarizeBy](#summarizeby)).

### No or few sequneces passing merge

If the amplicon is shorter than the reads, use `fastq_allowmergestagger=T`.
