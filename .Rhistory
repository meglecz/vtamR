#### Read taxonomy info
# read taxonomy file; quote="" is important, since some of the taxon names have quotes and this should be ignored
tax_df <- read.delim(taxonomy, header=T, sep=tax_sep, fill=T, quote="")
tax_sep <- "\t"
#### Read taxonomy info
# read taxonomy file; quote="" is important, since some of the taxon names have quotes and this should be ignored
tax_df <- read.delim(taxonomy, header=T, sep=tax_sep, fill=T, quote="")
# make data frame with old taxids as line numbers and taxids in a columns
old_taxid <- tax_df %>%
filter(!is.na(old_tax_id)) %>%
select(tax_id, old_tax_id)
# delete old_tax_ids from tax_df and make taxids unique
tax_df <- tax_df %>%
select(-old_tax_id)
tax_df <- unique(tax_df)
####
# create a tmp directory for temporary files using time and a random number
outdir_tmp <- paste('tmp_TaxAssign_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
outdir_tmp <- check_dir(outdir_tmp)
####
# create a tmp directory for temporary files using time and a random number
outdir_tmp <- paste('tmp_TaxAssign_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
outdir_tmp <- check_dir(outdir_tmp)
### run blast and clean/complete results
# run blast and read read results to data frame (blast_res columns: "qseqid","pident","qcovhsp","staxids")
blast_res <- run_blast(asv_df, blast_db=blast_db, blast_path=blast_path, outdir=outdir_tmp, qcov_hsp_perc=min(ltg_params_df$pcov), perc_identity=min(ltg_params_df$pid), num_threads=num_threads)
blast_db
# add update old taxids to valid ones
blast_res <- update_taxids(blast_res, old_taxid)
# add taxlevel
blast_res <- left_join(blast_res, tax_df, by=c("staxids" = "tax_id")) %>%
select(-parent_tax_id, -rank, -name_txt)
### make a lineage for each taxid in blastres
lineages <- get_lineage_ids(unique(blast_res$staxids), tax_df)
# input
blast_out <- "/home/meglecz/vtamR/local/blast.out"
taxonomy <- "~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/COInr_for_vtam_taxonomy.tsv"
new_taxonomy_file <- "/home/meglecz/vtamR/vtamR_test/data/db_test/taxonomy_reduced.tsv"
COInr <- "/home/meglecz/mkCOInr/COInr/COInr_2023_05_03/COInr.tsv"
blast_path <- "~/ncbi-blast-2.11.0+/bin/"
# output
out_db_name <- "/home/meglecz/vtamR/vtamR_test/data/db_test/COInr_reduced"
taxids <-"/home/meglecz/vtamR/vtamR_test/data/db_test/COInr_reduced_taxids.tsv"
new_db_fas <-"/home/meglecz/vtamR/vtamR_test/data/db_test/COInr_reduced.fas"
#### Read taxonomy info
# read taxonomy file; quote="" is important, since some of the taxon names have quotes and this should be ignored
tax_df <- read.delim(taxonomy, header=T, sep="\t", fill=T, quote="")
# make data frame with old taxids as line numbers and taxids in a columns
old_taxid <- tax_df %>%
filter(!is.na(old_tax_id)) %>%
select(tax_id, old_tax_id)
# delete old_tax_ids from tax_df and make taxids unique
tax_df <- tax_df %>%
select(-old_tax_id)
tax_df <- unique(tax_df)
### read read results to data frame (blast_res columns: "qseqid","pident","qcovhsp","staxids"); takes into account multiple taxids in one cell
blast_res <- read_blast_res(file=blast_out)
# add update old taxids to valid ones
blast_res <- update_taxids(blast_res, old_taxid)
# get the list of the taxids of the sequences (WO the taxids of the lineages)
unique_lowest_resolution_taxids <- unique(blast_res$staxids)
### make a lineage for each taxid in blastres
lineages <- get_lineage_ids(unique(blast_res$staxids), tax_df)
View(lineages)
View(ltg_params_df)
View(old_taxid)
View(tax_df)
blast_db="vtamR_test/data/db_test/COInr_reduced"
taxonomy="vtamR_test/data/db_test/taxonomy_reduced.tsv"
test_dir="vtamR_test/"
#### Read taxonomy info
# read taxonomy file; quote="" is important, since some of the taxon names have quotes and this should be ignored
tax_df <- read.delim(taxonomy, header=T, sep=tax_sep, fill=T, quote="")
# make data frame with old taxids as line numbers and taxids in a columns
old_taxid <- tax_df %>%
filter(!is.na(old_tax_id)) %>%
select(tax_id, old_tax_id)
# delete old_tax_ids from tax_df and make taxids unique
tax_df <- tax_df %>%
select(-old_tax_id)
tax_df <- unique(tax_df)
####
# create a tmp directory for temporary files using time and a random number
outdir_tmp <- paste('tmp_TaxAssign_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
outdir_tmp <- check_dir(outdir_tmp)
### run blast and clean/complete results
# run blast and read read results to data frame (blast_res columns: "qseqid","pident","qcovhsp","staxids")
blast_res <- run_blast(asv_df, blast_db=blast_db, blast_path=blast_path, outdir=outdir_tmp, qcov_hsp_perc=min(ltg_params_df$pcov), perc_identity=min(ltg_params_df$pid), num_threads=num_threads)
# add update old taxids to valid ones
blast_res <- update_taxids(blast_res, old_taxid)
# add taxlevel
blast_res <- left_join(blast_res, tax_df, by=c("staxids" = "tax_id")) %>%
select(-parent_tax_id, -rank, -name_txt)
### make a lineage for each taxid in blastres
lineages <- get_lineage_ids(unique(blast_res$staxids), tax_df)
# initialize data frame with asv and NA for all other cells
unique_asv_df <- asv_df %>%
ungroup() %>%
select(asv_id, asv) %>%
unique()
taxres_df <- data.frame(asv_id = unique_asv_df$asv_id, ltg_taxid = NA, pid=NA, pcov=NA, phit=NA, taxn=NA, seqn=NA, refres=NA, ltgres=NA, asv = unique_asv_df$asv)
for(i in 1:nrow(taxres_df)){ # go through all sequences
for(p in 1:nrow(ltg_params_df)){ # for each pid
pidl <- ltg_params_df[p,"pid"]
pcovl <- ltg_params_df[p,"pcov"]
phitl <- ltg_params_df[p,"phit"]
taxnl <- ltg_params_df[p,"taxn"]
seqnl <- ltg_params_df[p,"seqn"]
refresl <- ltg_params_df[p,"refres"]
ltgresl <- ltg_params_df[p,"ltgres"]
# filter the blastres according to  pid, pcov, refres
df_intern <- blast_res %>%
filter(qseqid==i & pident>=pidl & qcovhsp>=pcovl & taxlevel>=refresl)
# check if enough taxa and seq among validated hits
tn <- length(unique(df_intern$staxids))
if(tn >= taxnl & nrow(df_intern) >= seqnl ){
# make ltg if all conditions are met
ltg <- make_ltg(df_intern$staxids, lineages, phit = phitl)
# fill out line with the ltg and the parmeters that were used to get it
taxres_df[i,2:(ncol(taxres_df)-1)] <- c(ltg, pidl, pcovl, phitl, taxnl, seqnl, refresl, ltgresl)
break
} # end if
} # end p (pids)
} # end i (asvs)
# get the ranked lineage for each taxid in taxres_df
ranked_lineages <- get_ranked_lineages(unique(taxres_df$ltg_taxid), tax_df)
View(ranked_lineages)
# add lineage to taxres_df
taxres_df <- left_join(taxres_df, ranked_lineages, by="ltg_taxid") %>%
select(asv_id,ltg_taxid,ltg_name,ltg_rank,ltg_rank_index,superkingdom_taxid,superkingdom,kingdom_taxid,kingdom,phylum_taxid,phylum,class_taxid,class,order_taxid,order,family_taxid,family,genus_taxid,genus,species_taxid,species,pid,pcov,phit,taxn,seqn,refres,ltgres,asv)
# adjust resolution if it is higher than ltgres
taxres_df <- adjust_ltgres(taxres_df, tax_df)
# delete temporary  dir
unlink(outdir_tmp, recursive = TRUE)
if(outfile != ""){
write.table(taxres_df, file = outfile,  row.names = F, sep=sep)
}
return(taxres_df)
library("devtools")
library("roxygen2")
library("seqinr") # splitseq for FilterCodonStop
library("dplyr")
library("tidyr") # gather for read_asv_table; pivot_wider in WriteAsVtable and stat_sample !!sym
#library("utils") # to handle zipped files
library("ggplot2")
computer <- "Bombyx" # Bombyx/Endoume/Windows
if(computer == "Bombyx"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
swarm_path <- ""
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/"
#     fastq_dir <- "vtamR_test/data/"
#     fastqinfo <- "vtamR_test/data/fastqinfo_zfzr.csv"
#    outdir <- "vtamR_test/out_zfzr/"
#     mock_composition <- "vtamR_test/data/mock_composition_zfzr.csv"
#      asv_list <- "vtamR_test/data/asv_list_zfzr.csv"
fastq_dir <- "/home/meglecz/vtamR_large_files/fastq/"
fastqinfo <- "/home/meglecz/vtamR_large_files/user_input/fastqinfo_mfzr.csv"
outdir <- "/home/meglecz/vtamR_large_files/out/"
mock_composition <- "/home/meglecz/vtamR_large_files/user_input/mock_composition_mfzr.csv"
asv_list <- "/home/meglecz/vtamR_large_files/user_input/asv_list.csv"
num_threads=8
compress = T
} else if (computer == "Endoume"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/emese/miniconda3/bin/"
vsearch_path = "/home/emese/miniconda3/bin/"
blast_path= "" # deactivate conda
swarm_path <- ""
db_path= "~/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/"
#    fastq_dir <- "vtamR_test/data/"
#     fastqinfo <- "vtamR_test/data/fastqinfo_mfzr.csv"
#     outdir <- "vtamR_test/out_mfzr/"
#     mock_composition <- "vtamR_test/data/mock_composition_mfzr.csv"
#     asv_list <- "vtamR_test/data/asv_list_zfzr.csv"
fastq_dir <- "~/vtamR_large_data"
fastqinfo <- "~/vtamR_large_data/metadata/fastqinfo_Sea18_IIICBR_vtamR.csv"
outdir <- "/home/emese/vtamR_large_data/out/"
mock_composition <- "~/vtamR_large_data/metadata/mock_composition_Sea18_IIICBR_vtamR.csv"
asv_list <- "~/vtamR_large_data/metadata/asv_list.csv"
num_threads=8
compress = T
}else if (computer == "Windows"){
vtam_dir <- "C:/Users/emese/vtamR/"
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
swarm_path <- "C:/swarm-3.1.4-win-x86_64/bin/"
db_path="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/"
#  fastq_dir <- "C:/Users/emese/vtamR_private/fastq/"
fastq_dir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr.csv"
outdir <- "vtamR_test/out_mfzr/"
mock_composition <- "vtamR_test/data/mock_composition_mfzr.csv"
asv_list <- "vtamR_test/data/asv_list.csv"
num_threads=4
compress = F
}
sep=","
setwd(vtam_dir)
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
# load local packages
load_all(".")
roxygenise()
usethis::use_roxygen_md()
print(runtime)
start_time <- Sys.time()
test_TaxAssign(test_dir="vtamR_test/", sep=sep, blast_path=blast_path, blast_db=blast_db, taxonomy=taxonomy, num_threads=num_threads)
end_time <- Sys.time()  # Record the end time
runtime <- end_time - start_time  # Calculate the run time
print(runtime)
start_time <- Sys.time()
test_TaxAssign(test_dir="vtamR_test/", sep=sep, blast_path=blast_path, num_threads=num_threads)
end_time <- Sys.time()  # Record the end time
runtime <- end_time - start_time  # Calculate the run time
print(runtime)
# load local packages
load_all(".")
roxygenise()
usethis::use_roxygen_md()
start_time <- Sys.time()
test_TaxAssign(test_dir="vtamR_test/", sep=sep, blast_path=blast_path, num_threads=num_threads)
end_time <- Sys.time()  # Record the end time
runtime <- end_time - start_time  # Calculate the run time
print(runtime)
library("devtools")
library("roxygen2")
library("seqinr") # splitseq for FilterCodonStop
library("dplyr")
library("tidyr") # gather for read_asv_table; pivot_wider in WriteAsVtable and stat_sample !!sym
#library("utils") # to handle zipped files
library("ggplot2")
computer <- "Bombyx" # Bombyx/Endoume/Windows
if(computer == "Bombyx"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
swarm_path <- ""
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/"
#     fastq_dir <- "vtamR_test/data/"
#     fastqinfo <- "vtamR_test/data/fastqinfo_zfzr.csv"
#    outdir <- "vtamR_test/out_zfzr/"
#     mock_composition <- "vtamR_test/data/mock_composition_zfzr.csv"
#      asv_list <- "vtamR_test/data/asv_list_zfzr.csv"
fastq_dir <- "/home/meglecz/vtamR_large_files/fastq/"
fastqinfo <- "/home/meglecz/vtamR_large_files/user_input/fastqinfo_mfzr.csv"
outdir <- "/home/meglecz/vtamR_large_files/out/"
mock_composition <- "/home/meglecz/vtamR_large_files/user_input/mock_composition_mfzr.csv"
asv_list <- "/home/meglecz/vtamR_large_files/user_input/asv_list.csv"
num_threads=8
compress = T
} else if (computer == "Endoume"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/emese/miniconda3/bin/"
vsearch_path = "/home/emese/miniconda3/bin/"
blast_path= "" # deactivate conda
swarm_path <- ""
db_path= "~/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/"
#    fastq_dir <- "vtamR_test/data/"
#     fastqinfo <- "vtamR_test/data/fastqinfo_mfzr.csv"
#     outdir <- "vtamR_test/out_mfzr/"
#     mock_composition <- "vtamR_test/data/mock_composition_mfzr.csv"
#     asv_list <- "vtamR_test/data/asv_list_zfzr.csv"
fastq_dir <- "~/vtamR_large_data"
fastqinfo <- "~/vtamR_large_data/metadata/fastqinfo_Sea18_IIICBR_vtamR.csv"
outdir <- "/home/emese/vtamR_large_data/out/"
mock_composition <- "~/vtamR_large_data/metadata/mock_composition_Sea18_IIICBR_vtamR.csv"
asv_list <- "~/vtamR_large_data/metadata/asv_list.csv"
num_threads=8
compress = T
}else if (computer == "Windows"){
vtam_dir <- "C:/Users/emese/vtamR/"
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
swarm_path <- "C:/swarm-3.1.4-win-x86_64/bin/"
db_path="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/"
#  fastq_dir <- "C:/Users/emese/vtamR_private/fastq/"
fastq_dir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr.csv"
outdir <- "vtamR_test/out_mfzr/"
mock_composition <- "vtamR_test/data/mock_composition_mfzr.csv"
asv_list <- "vtamR_test/data/asv_list.csv"
num_threads=4
compress = F
}
sep=","
setwd(vtam_dir)
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
# load local packages
load_all(".")
roxygenise()
usethis::use_roxygen_md()
test_TaxAssign(test_dir="vtamR_test/", sep=sep, blast_path=blast_path, num_threads=num_threads)
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/"
swarm_path <- ""
num_threads=8
sep=","
compress = F
library("devtools")
library("roxygen2")
library("seqinr")
library("dplyr")
library("tidyr")
library("ggplot2")
load_all(".")
roxygenise()
usethis::use_roxygen_md()
test_Merge_and_SortReads(vsearch_path=vsearch_path, cutadapt_path=cutadapt_path)
test_Filters(vsearch_path=vsearch_path, swarm_path=swarm_path)
test_MakeKnownOccurrences()
test_Optimize(vsearch_path=vsearch_path)
test_TaxAssign(blast_path=blast_path, num_threads=num_threads)
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/"
swarm_path <- ""
num_threads=8
sep=","
compress = F
taxonomy="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/COInr_for_vtam_taxonomy.tsv"
blast_db="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/COInr_for_vtam"
taxonomy="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/COInr_for_vtam_taxonomy.tsv"
blast_db="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/COInr_for_vtam"
fastq_dir <- "vtamR_test/data/"
outdir <- "vtamR_test/out_mfzr/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr.csv"
mock_composition <- "vtamR_test/data/mock_composition_mfzr.csv"
asv_list <- "vtamR_test/data/asv_list.csv"
check_fileinfo(file=fastqinfo, dir=fastq_dir, file_type="fastqinfo")
check_fileinfo(file=mock_composition, file_type="mock_composition")
check_fileinfo(file=asv_list, file_type="asv_list")
# merge and quality filter
merged_dir <- paste(outdir, "merged/", sep="")
fastainfo_df <- Merge(fastqinfo, fastq_dir=fastq_dir, vsearch_path=vsearch_path, outdir=merged_dir)
# demultiplex, trim tags and pimers
sorted_dir <- paste(outdir, "sorted/", sep="")
sortedinfo_df <- SortReads(fastainfo_df, fasta_dir=merged_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path)
outfile <- paste(outdir, "1_before_filter.csv", sep="")
updated_asv_list <- paste(outdir, "ASV_list_with_IDs.csv", sep="")
read_count_df <- read_fastas_from_sortedinfo(sortedinfo_df, dir=sorted_dir, outfile=outfile, asv_list=asv_list, updated_asv_list=updated_asv_list)
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
by_sample <- TRUE
outfile <- paste(outdir, "2_Swarm_by_sample.csv", sep="")
read_count_df <- Swarm(read_count_df, outfile=outfile, swarm_path=swarm_path, num_threads=num_threads, by_sample=by_sample)
stat_df <- get_stat(read_count_df, stat_df, stage="Swarm", params=by_sample)
global_read_count_cutoff = 2
outfile <- paste(outdir, "3_LFN_global_read_count.csv", sep="")
read_count_df <- LFN_global_read_count(read_count_df, cutoff=global_read_count_cutoff, outfile=outfile)
stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
outfile <- paste(outdir, "4_FilterIndel.csv", sep="")
read_count_df <- FilterIndel(read_count_df, outfile=outfile)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterIndel")
outfile <- paste(outdir, "5_FilterCodonStop.csv", sep="")
genetic_code = 5
read_count_df <- FilterCodonStop(read_count_df, outfile=outfile, genetic_code=genetic_code)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
abskew=2
by_sample = T
sample_prop = 0.8
outfile <- paste(outdir, "6_FilterChimera.csv", sep="")
read_count_df <- FilterChimera(read_count_df, outfile=outfile, vsearch_path=vsearch_path, by_sample=by_sample, sample_prop=sample_prop, abskew=abskew)
params <- paste(abskew, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilterChimera", params=params)
# calculate renkonen distance among all replicates within sample
renkonen_within_df <- make_renkonen_distances(read_count_df, compare="within")
# density plot
renkonen_density_plot <- density_plot_renkonen_distance(renkonen_within_df)
print(renkonen_density_plot)
# barplot
sortedinfo <- paste(sorted_dir, "sortedinfo.csv", sep="")
renkonen_barplot <- barplot_renkonen_distance(renkonen_within_df, sample_types=sortedinfo, x_axis_label_size=6)
print(renkonen_barplot)
outfile <- paste(outdir, "7_FilterRenkonen.csv", sep="")
cutoff <- 0.4
read_count_df <- FilterRenkonen(read_count_df, outfile=outfile, cutoff=cutoff)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerRenkonen", params=cutoff)
outfile <- paste(outdir, "ASV_taxa.csv", sep="")
asv_tax <- TaxAssign(asv=read_count_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, num_threads=num_threads, outfile=outfile)
tmp_read_count_samples_df <- PoolReplicates(read_count_df, digits=0, outfile=outfile, sep=sep)
sortedinfo <- paste(sorted_dir, "sortedinfo.csv", sep ="")
tmp_asv_table <- WriteASVtable(tmp_read_count_samples_df, sortedinfo=sortedinfo, add_sums_by_asv=T, asv_tax=asv_tax)
asv_tpos1 <- tmp_asv_table %>%
select(tpos1, Total_number_of_reads, Number_of_samples, asv_id, phylum, class, order, family, genus, species, asv) %>%
filter(tpos1 > 0) %>%
arrange(desc(tpos1))
outfile <- paste(outdir, "OptimizePCRError.csv", sep="")
OptimizePCRError_df <- OptimizePCRError(read_count_df, mock_composition=mock_composition, outfile=outfile, max_mismatch=2, min_read_count=5)
pcr_error_var_prop <- 0.1
max_mismatch <- 2
outfile <- paste(outdir, "8_FilterPCRerror.csv", sep="")
read_count_df <- FilterPCRerror(read_count_df, outfile=outfile, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch)
params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilerPCRerror", params=params)
outfile = paste(outdir, "OptimizeLFNsampleReplicate.csv", sep="")
OptimizeLFNsampleReplicate_df <- OptimizeLFNsampleReplicate(read_count=read_count_df, mock_composition=mock_composition, outfile=outfile)
outfile = paste(outdir, "OptimizeLFNsampleReplicate.csv", sep="")
OptimizeLFNsampleReplicate_df <- OptimizeLFNsampleReplicate(read_count=read_count_df, mock_composition=mock_composition, outfile=outfile)
lfn_sample_replicate_cutoff <- 0.005
outfile <- paste(outdir, "9_LFN_sample_replicate.csv", sep="")
read_count_df_lnf_sample_replicate <- LFN_sample_replicate(read_count_df, cutoff=lfn_sample_replicate_cutoff, outfile=outfile)
stat_df <- get_stat(read_count_df_lnf_sample_replicate, stat_df, stage="LFN_sample_replicate", params=lfn_sample_replicate_cutoff)
## LFN_variant
# Set parameter values
min_replicate_number <- 2
outfile <- paste(outdir, "10_FilterMinReplicateNumber.csv", sep="")
# Run filter and get stats
read_count_df <- FilterMinReplicateNumber(read_count_df, min_replicate_number, outfile=outfile)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterMinReplicateNumber", params=min_replicate_number)
# Pool replicates
read_count_samples_df <- PoolReplicates(read_count_df)
# Detect known occurrences
results <- MakeKnownOccurrences(read_count_samples = read_count_samples_df, sortedinfo=sortedinfo, mock_composition=mock_composition)
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
outfile = paste(outdir, "OptimizeLFNReadCountAndLFNvariant.csv", sep="")
OptimizeLFNReadCountAndLFNvariant_df <- OptimizeLFNReadCountAndLFNvariant(read_count_df, known_occurrences=known_occurrences_df, outfile= outfile, min_replicate_number=2)
## LFN_variant
# Set parameter values
lnf_variant_cutoff = 0.001
outfile <- paste(outdir, "11_LFN_variant.csv", sep="")
# Run filter and get stats
read_count_df_lnf_variant <- LFN_variant(read_count_df, cutoff=lnf_variant_cutoff, outfile=outfile)
stat_df <- get_stat(read_count_df_lnf_variant, stat_df, stage="LFN_variant", params=lnf_variant_cutoff)
## LFN_read_count
# Set parameter values
lfn_read_count_cutoff <- 85
outfile <- paste(outdir, "12_LFN_read_count.csv", sep="")
# Run filter and get stats
read_count_df_lfn_read_count <- LFN_read_count(read_count_df, cutoff=lfn_read_count_cutoff, outfile=outfile)
stat_df <- get_stat(read_count_df_lfn_read_count, stat_df, stage="LFN_read_count", params=lfn_read_count_cutoff)
## Combine results
# Set parameter values
outfile <- paste(outdir, "13_pool_LFN.csv", sep="")
# Combine results and get stats
read_count_df <- pool_LFN(read_count_df_lfn_read_count, read_count_df_lnf_variant, outfile=outfile)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
rm(read_count_df_lfn_read_count)
rm(read_count_df_lnf_variant)
## LFN_variant
# Set parameter values
min_replicate_number <- 2
outfile <- paste(outdir, "14_FilterMinReplicateNumber.csv", sep="")
# Run filter and get stats
read_count_df <- FilterMinReplicateNumber(read_count_df, min_replicate_number, outfile=outfile)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterMinReplicateNumber", params=min_replicate_number)
# Set parameter values
outfile <- paste(outdir, "15_PoolReplicates.csv", sep="")
read_count_samples_df <- PoolReplicates(read_count_df, outfile=outfile)
# Run function and get stats
stat_df <- get_stat(read_count_samples_df, stat_df, stage="PoolReplicates")
# Set parameter values
missing_occurrences <- paste(outdir, "missing_occurrences.csv", sep= "")
performance_metrics <- paste(outdir, "performance_metrics.csv", sep= "")
known_occurrences <- paste(outdir, "known_occurrences.csv", sep= "")
sortedinfo <- paste(sorted_dir, "sortedinfo.csv", sep ="")
# Run function
results <- MakeKnownOccurrences(read_count_samples_df, sortedinfo=sortedinfo, mock_composition=mock_composition, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, performance_metrics=performance_metrics)
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
# write ASV table completed by taxonomic assignments
outfile=paste(outdir, "Final_asvtable_with_taxassign.csv", sep="")
asv_table_df <- WriteASVtable(read_count_samples_df, outfile=outfile, asv_tax=asv_tax, sortedinfo=sortedinfo, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition)
