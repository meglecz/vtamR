read_count_samples_df <- PoolReplicates(read_count_df, digits=digits, write_csv=T, outdir=outdir, sep=sep)
#### Read taxonomy info
# read taxonomy file; quote="" is important, since some of the taxon names have quotes and this should be ignored
tax_df <- read.delim(taxonomy, header=T, sep="\t", fill=T, quote="")
# make data frame with old taxids as line numbers and taxids in a columns
old_taxid <- tax_df %>%
filter(!is.na(old_tax_id)) %>%
select(tax_id, old_tax_id)
# delete old_tax_ids from tax_df and make taxids unique
tax_df <- tax_df %>%
select(-old_tax_id)
tax_df <- unique(tax_df)
####
# create a tmp directory for temporary files using time and a random number
outdir_tmp <- paste(outdir, 'tmp_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
outdir_tmp <- check_dir(outdir_tmp)
### run blast and clean/complete results
# run blast and read results to data frame (blast_res columns: "qseqid","pident","qcovhsp","staxids")
blast_res <- run_blast(read_count_samples_df, blast_db=blast_db, blast_path=blast_path, outdir=outdir_tmp, qcov_hsp_perc=min(ltg_params_df$pcov), perc_identity=min(ltg_params_df$pid), num_threads=8)
# add update old taxids to valid ones
blast_res <- update_taxids(blast_res, old_taxid)
# add taxlevel
blast_res <- left_join(blast_res, tax_df, by=c("staxids" = "tax_id")) %>%
select(-parent_tax_id, -rank, -name_txt)
### make a lineage for each taxid in blastres
lineages <- get_lineage_ids(unique(blast_res$staxids), tax_df)
# initialize data frame with asv and NA for all other cells
taxres_df <- data.frame(asv = unique(read_count_samples_df$asv), ltg_taxid = NA, pid=NA, pcov=NA, phit=NA, taxn=NA, seqn=NA, refres=NA, ltgres=NA)
for(i in 1:nrow(taxres_df)){ # go through all sequences
for(p in 1:nrow(ltg_params_df)){ # for each pid
pidl <- ltg_params_df[p,"pid"]
pcovl <- ltg_params_df[p,"pcov"]
phitl <- ltg_params_df[p,"phit"]
taxnl <- ltg_params_df[p,"taxn"]
seqnl <- ltg_params_df[p,"seqn"]
refresl <- ltg_params_df[p,"refres"]
ltgresl <- ltg_params_df[p,"ltgres"]
# filter the blastres according to  pid, pcov, refres
df_intern <- blast_res %>%
filter(qseqid==i & pident>=pidl & qcovhsp>=pcovl & taxlevel>=refresl)
# check if enough taxa and seq among validated hits
tn <- length(unique(df_intern$staxids))
if(tn >= taxnl & nrow(df_intern) >= seqnl ){
# make ltg if all conditions are met
ltg <- make_ltg(df_intern$staxids, lineages, phit = phitl)
# fill out line with the ltg and the parmeters that were used to get it
taxres_df[i,2:ncol(taxres_df)] <- c(ltg, pidl, pcovl, phitl, taxnl, seqnl, refresl, ltgresl)
break
} # end if
} # end p (pids)
} # end i (asvs)
ranked_lineages <- get_ranked_lineages(unique(taxres_df$ltg_taxid), tax_df)
taxres_df <- left_join(taxres_df, ranked_lineages, by="ltg_taxid") %>%
select(asv,ltg_taxid,ltg_name,ltg_rank,ltg_rank_index,superkingdom_taxid,superkingdom,kingdom_taxid,kingdom,phylum_taxid,phylum,class_taxid,class,order_taxid,order,family_taxid,family,genus_taxid,genus,species_taxid,species,pid,pcov,phit,taxn,seqn,refres,ltgres)
taxres_df[3,"ltgres"] <- 4
taxres_df[7,"ltgres"] <- 5
taxres_df[8,"ltgres"] <- 3
taxres_df <- adjust_ltgres(taxres_df, taxlevel_index, tax_df)
adjust_ltgres <- function(taxres_df, taxlevel_index, tax_df){
# add the name of the tax rank equivalent to the index in ltgref
taxres_df <- left_join(taxres_df, taxlevel_index, by=c("ltgres" = "taxlevel_index"))
for(i in 1:nrow(taxres_df)){ # all rows
if(!is.na(taxres_df[i,"ltg_taxid"]) & taxres_df[i,"ltg_rank_index"] > taxres_df[i,"ltgres"]){ # if resolution of ltg is higher then ltgres
# get the taxrank that corresponds to ltgres
tl <- taxres_df[i,"taxrank"]
# get the index of the column that corresponds to the ltgres
col_index <- which(colnames(taxres_df) == tl)
# make a data frame with taxid, and get taxinfo from tax_df
new_taxid <- as.data.frame(taxres_df[i, col_index-1])
colnames(new_taxid) <- c("tax_id")
new_taxid <- left_join(new_taxid, tax_df, by="tax_id") %>%
select(tax_id, name_txt, rank, taxlevel)
# replace ltg taxid and associated info
taxres_df[i, 2:5] <- new_taxid[1,]
# replace tax lineage over the ltgref by NA
taxres_df[i, (col_index+1):(ncol(tmp)-9)] <- NA
}# end if
}# end for i
return(taxres_df)
}
taxres_df <- adjust_ltgres(taxres_df, taxlevel_index, tax_df)
adjust_ltgres <- function(taxres_df, taxlevel_index, tax_df){
# add the name of the tax rank equivalent to the index in ltgref
taxres_df <- left_join(taxres_df, taxlevel_index, by=c("ltgres" = "taxlevel_index"))
for(i in 1:nrow(taxres_df)){ # all rows
if(!is.na(taxres_df[i,"ltg_taxid"]) & taxres_df[i,"ltg_rank_index"] > taxres_df[i,"ltgres"]){ # if resolution of ltg is higher then ltgres
# get the taxrank that corresponds to ltgres
tl <- taxres_df[i,"taxrank"]
# get the index of the column that corresponds to the ltgres
col_index <- which(colnames(taxres_df) == tl)
# make a data frame with taxid, and get taxinfo from tax_df
new_taxid <- as.data.frame(taxres_df[i, col_index-1])
colnames(new_taxid) <- c("tax_id")
new_taxid <- left_join(new_taxid, tax_df, by="tax_id") %>%
select(tax_id, name_txt, rank, taxlevel)
# replace ltg taxid and associated info
taxres_df[i, 2:5] <- new_taxid[1,]
# replace tax lineage over the ltgref by NA
taxres_df[i, (col_index+1):(ncol(taxres_df)-9)] <- NA
}# end if
}# end for i
return(taxres_df)
}
taxres_df <- adjust_ltgres(taxres_df, taxlevel_index, tax_df)
View(taxres_df)
adjust_ltgres <- function(taxres_df, taxlevel_index, tax_df){
# add the name of the tax rank equivalent to the index in ltgref
taxres_df <- left_join(taxres_df, taxlevel_index, by=c("ltgres" = "taxlevel_index"))
for(i in 1:nrow(taxres_df)){ # all rows
if(!is.na(taxres_df[i,"ltg_taxid"]) & taxres_df[i,"ltg_rank_index"] > taxres_df[i,"ltgres"]){ # if resolution of ltg is higher then ltgres
# get the taxrank that corresponds to ltgres
tl <- taxres_df[i,"taxrank"]
# get the index of the column that corresponds to the ltgres
col_index <- which(colnames(taxres_df) == tl)
# make a data frame with taxid, and get taxinfo from tax_df
new_taxid <- as.data.frame(taxres_df[i, col_index-1])
colnames(new_taxid) <- c("tax_id")
new_taxid <- left_join(new_taxid, tax_df, by="tax_id") %>%
select(tax_id, name_txt, rank, taxlevel)
# replace ltg taxid and associated info
taxres_df[i, 2:5] <- new_taxid[1,]
# replace tax lineage over the ltgref by NA
taxres_df[i, (col_index+1):(ncol(taxres_df)-9)] <- NA
}# end if
}# end for i
taxres_df <- taxres_df %>%
select(-taxrank)
return(taxres_df)
}
# initialize data frame with asv and NA for all other cells
taxres_df <- data.frame(asv = unique(read_count_samples_df$asv), ltg_taxid = NA, pid=NA, pcov=NA, phit=NA, taxn=NA, seqn=NA, refres=NA, ltgres=NA)
for(i in 1:nrow(taxres_df)){ # go through all sequences
for(p in 1:nrow(ltg_params_df)){ # for each pid
pidl <- ltg_params_df[p,"pid"]
pcovl <- ltg_params_df[p,"pcov"]
phitl <- ltg_params_df[p,"phit"]
taxnl <- ltg_params_df[p,"taxn"]
seqnl <- ltg_params_df[p,"seqn"]
refresl <- ltg_params_df[p,"refres"]
ltgresl <- ltg_params_df[p,"ltgres"]
# filter the blastres according to  pid, pcov, refres
df_intern <- blast_res %>%
filter(qseqid==i & pident>=pidl & qcovhsp>=pcovl & taxlevel>=refresl)
# check if enough taxa and seq among validated hits
tn <- length(unique(df_intern$staxids))
if(tn >= taxnl & nrow(df_intern) >= seqnl ){
# make ltg if all conditions are met
ltg <- make_ltg(df_intern$staxids, lineages, phit = phitl)
# fill out line with the ltg and the parmeters that were used to get it
taxres_df[i,2:ncol(taxres_df)] <- c(ltg, pidl, pcovl, phitl, taxnl, seqnl, refresl, ltgresl)
break
} # end if
} # end p (pids)
} # end i (asvs)
ranked_lineages <- get_ranked_lineages(unique(taxres_df$ltg_taxid), tax_df)
taxres_df <- left_join(taxres_df, ranked_lineages, by="ltg_taxid") %>%
select(asv,ltg_taxid,ltg_name,ltg_rank,ltg_rank_index,superkingdom_taxid,superkingdom,kingdom_taxid,kingdom,phylum_taxid,phylum,class_taxid,class,order_taxid,order,family_taxid,family,genus_taxid,genus,species_taxid,species,pid,pcov,phit,taxn,seqn,refres,ltgres)
taxres_df[3,"ltgres"] <- 4
taxres_df[7,"ltgres"] <- 5
taxres_df[8,"ltgres"] <- 3
taxres_df <- adjust_ltgres(taxres_df, taxlevel_index, tax_df)
View(taxres_df)
colnames(taxres_df)
setwd("~/vtamR")
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
#blast_path="" # endoume deactivate conda
#db_path="~/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/" # Endoume
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/" # Bombyx
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
fastadir <- "local/mfzr/sorted/"
fileinfo <- "local/user_input/fileinfo_mfzr_eu.csv"
mock_composition <- "local/user_input/mock_composition_mfzr_eu.csv"
sep=";"
# create the output directory and check the the slash at the end
outdir <- check_dir(dir="local/out")
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
# read input fasta files in fileinfo, demultiplex and count the number of reads in each plate-sample-replicate
read_count_df <- read_fastas_from_fileinfo(file=fileinfo, dir=fastadir, write_csv=F, outdir=outdir, sep=sep)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
###
### LFN_global_read_count
###
# Eliminate variants with less than global_read_count_cutoff reads in the dataset
global_read_count_cutoff = 60
read_count_df <- LFN_global_read_count(read_count_df, global_read_count_cutoff, write_csv=T, outdir=outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
###
### PoolReplicates
###
digits = 0
read_count_samples_df <- PoolReplicates(read_count_df, digits=digits, write_csv=T, outdir=outdir, sep=sep)
###
### TaxAssign
###
asv_tax <- TaxAssign(read_count_samples_df, ltg_params_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outdir=outdir)
TaxAssign <- function(read_count_samples_df, ltg_params_df, taxonomy="", blast_db="", blast_path="", outdir=""){
#### Read taxonomy info
# read taxonomy file; quote="" is important, since some of the taxon names have quotes and this should be ignored
tax_df <- read.delim(taxonomy, header=T, sep="\t", fill=T, quote="")
# make data frame with old taxids as line numbers and taxids in a columns
old_taxid <- tax_df %>%
filter(!is.na(old_tax_id)) %>%
select(tax_id, old_tax_id)
# delete old_tax_ids from tax_df and make taxids unique
tax_df <- tax_df %>%
select(-old_tax_id)
tax_df <- unique(tax_df)
####
# create a tmp directory for temporary files using time and a random number
outdir_tmp <- paste(outdir, 'tmp_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
outdir_tmp <- check_dir(outdir_tmp)
### run blast and clean/complete results
# run blast and read results to data frame (blast_res columns: "qseqid","pident","qcovhsp","staxids")
blast_res <- run_blast(read_count_samples_df, blast_db=blast_db, blast_path=blast_path, outdir=outdir_tmp, qcov_hsp_perc=min(ltg_params_df$pcov), perc_identity=min(ltg_params_df$pid), num_threads=8)
# add update old taxids to valid ones
blast_res <- update_taxids(blast_res, old_taxid)
# add taxlevel
blast_res <- left_join(blast_res, tax_df, by=c("staxids" = "tax_id")) %>%
select(-parent_tax_id, -rank, -name_txt)
### make a lineage for each taxid in blastres
lineages <- get_lineage_ids(unique(blast_res$staxids), tax_df)
# initialize data frame with asv and NA for all other cells
taxres_df <- data.frame(asv = unique(read_count_samples_df$asv), ltg_taxid = NA, pid=NA, pcov=NA, phit=NA, taxn=NA, seqn=NA, refres=NA, ltgres=NA)
for(i in 1:nrow(taxres_df)){ # go through all sequences
for(p in 1:nrow(ltg_params_df)){ # for each pid
pidl <- ltg_params_df[p,"pid"]
pcovl <- ltg_params_df[p,"pcov"]
phitl <- ltg_params_df[p,"phit"]
taxnl <- ltg_params_df[p,"taxn"]
seqnl <- ltg_params_df[p,"seqn"]
refresl <- ltg_params_df[p,"refres"]
ltgresl <- ltg_params_df[p,"ltgres"]
# filter the blastres according to  pid, pcov, refres
df_intern <- blast_res %>%
filter(qseqid==i & pident>=pidl & qcovhsp>=pcovl & taxlevel>=refresl)
# check if enough taxa and seq among validated hits
tn <- length(unique(df_intern$staxids))
if(tn >= taxnl & nrow(df_intern) >= seqnl ){
# make ltg if all conditions are met
ltg <- make_ltg(df_intern$staxids, lineages, phit = phitl)
# fill out line with the ltg and the parmeters that were used to get it
taxres_df[i,2:ncol(taxres_df)] <- c(ltg, pidl, pcovl, phitl, taxnl, seqnl, refresl, ltgresl)
break
} # end if
} # end p (pids)
} # end i (asvs)
# get the ranked lineage for each taxid in taxres_df
ranked_lineages <- get_ranked_lineages(unique(taxres_df$ltg_taxid), tax_df)
# add lineage to taxres_df
taxres_df <- left_join(taxres_df, ranked_lineages, by="ltg_taxid") %>%
select(asv,ltg_taxid,ltg_name,ltg_rank,ltg_rank_index,superkingdom_taxid,superkingdom,kingdom_taxid,kingdom,phylum_taxid,phylum,class_taxid,class,order_taxid,order,family_taxid,family,genus_taxid,genus,species_taxid,species,pid,pcov,phit,taxn,seqn,refres,ltgres)
# adjust resolution if it is higher than ltgres
taxres_df <- adjust_ltgres(taxres_df, tax_df)
return(taxres_df)
}
###
### TaxAssign
###
asv_tax <- TaxAssign(read_count_samples_df, ltg_params_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outdir=outdir)
View(asv_tax)
View(asv_tax)
colnames(asv_tax)
connames(ltg_params_df)
colnames(ltg_params_df)
setwd("~/vtamR")
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
#blast_path="" # endoume deactivate conda
#db_path="~/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/" # Endoume
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/" # Bombyx
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
fastadir <- "local/mfzr/sorted/"
fileinfo <- "local/user_input/fileinfo_mfzr_eu.csv"
mock_composition <- "local/user_input/mock_composition_mfzr_eu.csv"
sep=";"
# create the output directory and check the the slash at the end
outdir <- check_dir(dir="local/out")
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
# read input fasta files in fileinfo, demultiplex and count the number of reads in each plate-sample-replicate
read_count_df <- read_fastas_from_fileinfo(file=fileinfo, dir=fastadir, write_csv=F, outdir=outdir, sep=sep)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
###
### LFN_global_read_count
###
# Eliminate variants with less than global_read_count_cutoff reads in the dataset
global_read_count_cutoff = 60
read_count_df <- LFN_global_read_count(read_count_df, global_read_count_cutoff, write_csv=T, outdir=outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
###
### PoolReplicates
###
digits = 0
read_count_samples_df <- PoolReplicates(read_count_df, digits=digits, write_csv=T, outdir=outdir, sep=sep)
###
### TaxAssign
###
asv_tax <- TaxAssign(df=read_count_samples_df, ltg_params_df=ltg_params_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outdir=outdir)
setwd("~/vtamR")
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
#blast_path="" # endoume deactivate conda
#db_path="~/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/" # Endoume
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/" # Bombyx
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
####
# define input filenames
#fastadir <- "local/small_test"
#fileinfo <- "local/user_input/fileinfo_small.csv"
fastadir <- "local/mfzr/sorted/"
fileinfo <- "local/user_input/fileinfo_mfzr_eu.csv"
mock_composition <- "local/user_input/mock_composition_mfzr_eu.csv"
sep=";"
#fastadir <- "/home/meglecz/vtam_benchmark_local/vtam_fish/sorted_mfzr/"
#fileinfo <-"/home/meglecz/vtam_benchmark_local/vtam_fish/sorted_mfzr/fileinfo_vtamr.csv"
#mock_composition <- "/home/meglecz/vtamR/local/user_input/mock_composition_mfzr_prerun.csv"
#sep="\t"
#fastadir <- "/home/meglecz/vtam_benchmark_local/vtam_bat/fasta/"
#fileinfo <- "/home/meglecz/vtam_benchmark_local/vtam_bat/fasta/fileinfo_vtamr.csv"
# create the output directory and check the the slash at the end
outdir <- check_dir(dir="local/out")
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
# read input fasta files in fileinfo, demultiplex and count the number of reads in each plate-sample-replicate
read_count_df <- read_fastas_from_fileinfo(file=fileinfo, dir=fastadir, write_csv=F, outdir=outdir, sep=sep)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
#temp_df <- read_count_df
#read_count_df <- temp_df
###
### LFN_global_read_count
###
# Eliminate variants with less than global_read_count_cutoff reads in the dataset
global_read_count_cutoff = 60
read_count_df <- LFN_global_read_count(read_count_df, global_read_count_cutoff, write_csv=T, outdir=outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
###
### PoolReplicates
###
digits = 0
read_count_samples_df <- PoolReplicates(read_count_df, digits=digits, write_csv=T, outdir=outdir, sep=sep)
dim(read_count_samples_df)
start_time <- Sys.time()  # Record the start time
asv_tax <- TaxAssign(df=read_count_samples_df, ltg_params_df=ltg_params_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outdir=outdir)
end_time <- Sys.time()  # Record the end time
runtime <- end_time - start_time  # Calculate the run time
print(runtime)
setwd("~/vtamR")
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
#blast_path="" # endoume deactivate conda
#db_path="~/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/" # Endoume
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/" # Bombyx
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
####
# define input filenames
#fastadir <- "local/small_test"
#fileinfo <- "local/user_input/fileinfo_small.csv"
fastadir <- "local/mfzr/sorted/"
fileinfo <- "local/user_input/fileinfo_mfzr_eu.csv"
mock_composition <- "local/user_input/mock_composition_mfzr_eu.csv"
sep=";"
#fastadir <- "/home/meglecz/vtam_benchmark_local/vtam_fish/sorted_mfzr/"
#fileinfo <-"/home/meglecz/vtam_benchmark_local/vtam_fish/sorted_mfzr/fileinfo_vtamr.csv"
#mock_composition <- "/home/meglecz/vtamR/local/user_input/mock_composition_mfzr_prerun.csv"
#sep="\t"
#fastadir <- "/home/meglecz/vtam_benchmark_local/vtam_bat/fasta/"
#fileinfo <- "/home/meglecz/vtam_benchmark_local/vtam_bat/fasta/fileinfo_vtamr.csv"
# create the output directory and check the the slash at the end
outdir <- check_dir(dir="local/out")
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
# read input fasta files in fileinfo, demultiplex and count the number of reads in each plate-sample-replicate
read_count_df <- read_fastas_from_fileinfo(file=fileinfo, dir=fastadir, write_csv=F, outdir=outdir, sep=sep)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
#temp_df <- read_count_df
#read_count_df <- temp_df
###
### LFN_global_read_count
###
# Eliminate variants with less than global_read_count_cutoff reads in the dataset
global_read_count_cutoff = 2
read_count_df <- LFN_global_read_count(read_count_df, global_read_count_cutoff, write_csv=T, outdir=outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
###
### PoolReplicates
###
digits = 0
read_count_samples_df <- PoolReplicates(read_count_df, digits=digits, write_csv=T, outdir=outdir, sep=sep)
dim(read_count_samples_df)
start_time <- Sys.time()  # Record the start time
asv_tax <- TaxAssign(df=read_count_samples_df, ltg_params_df=ltg_params_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outdir=outdir)
end_time <- Sys.time()  # Record the end time
runtime <- end_time - start_time  # Calculate the run time
print(runtime)
