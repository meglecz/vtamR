setwd(vtam_dir)
getwd()
vtam_dir <- "~/vtamR" # clone from https://github.com/meglecz/vtamR
setwd(vtam_dir)
getwd()
vtam_dir <- "~/vtamR"
setwd(vtam_dir)
getwd()
library("devtools")
library("roxygen2")
library("seqinr")
library("dplyr")
library("tidyr")
library("ggplot2")
load_all(".")
roxygenise()
usethis::use_roxygen_md()
my_marker <- "16S"
my_marker <- "16S"
outdir <- paste("TAS/out_", my_marker, "_pooled", sep="")
outdir <- check_dir(outdir)
files <- data.frame(file=c("TAS/out_16S_TAS1/9_PoolReplicates.csv", "TAS/out_16S_TAS2/9_PoolReplicates.csv"),
marker=c("16S", "16S"))
files <- data.frame(file=c("TAS/out_16S_TAS1/9_PoolReplicates.csv", "TAS/out_16S_TAS2/9_PoolReplicates.csv"),
marker=c("16S", "16S"))
outfile <- paste(outdir, "1_Pooled_datasets_", marker, ".csv", sep="")
outfile <- paste(outdir, "1_Pooled_datasets_", my_marker, ".csv", sep="")
read_count_pool <- PoolDatasets(files, outfile=outfile)
outfile <- paste(outdir, "2_Clustered_ASV_COI.csv", sep="")
clustered_df <- Cluster_size(read_count_pool, id=0.97, vsearch_path=vsearch_path, outfile=outfile)
cutadapt_path <- "~/miniconda3/envs/vtam/bin" # v3.4
vsearch_path <- "~/miniconda3/envs/vtam/bin" # v2.15.1
blast_path <- "~/miniconda3/envs/vtam/bin" # v2.10.1+
swarm_path <- "" # v2.1.12
num_threads <- 8
sep <- ","
clustered_df <- Cluster_size(read_count_pool, id=0.97, vsearch_path=vsearch_path, outfile=outfile)
fasta <- paste(outdir, "3_asv_to_assign.fasta", sep="")
outfile <- paste(outdir, "3_TaxAssign_TAS_16S_rdp.csv", sep="")
write_fasta_df(clustered_df, fasta)
### Use local installation of rdp_classifier_2.14, downloadable from https://sourceforge.net/projects/rdp-classifier/
cmd <- paste('java -Xmx8g -jar ~/rdp_classifier_2.14/dist/classifier.jar classify -t ~/rdp_classifier_2.14/src/data/classifier/16srrna/rRNAClassifier.properties -f fixrank -o ', outfile, ' ', fasta)
print(cmd)
system(cmd)
# select columns
rdp_assignments <- read.table(outfile, sep="\t", header=FALSE, fill = TRUE) %>%
select(1,3,5,6,8,9,11,12,14,15,17,18,20)
colnames(rdp_assignments) <- c("asv_id", "domain", "domain_bootstrap", "phylum", "phylum_bootstrap", "class", "class_bootstrap", "order", "order_bootstrap", "family", "family_bootstrap", "genus", "genus_bootstrap")
# delete Chloroplast
rdp_assignments <- rdp_assignments %>%
filter(class != "Chloroplast")
# delete Chloroplast
rdp_assignments <- rdp_assignments %>%
filter(class != "Chloroplast")
for(i in seq(2, ncol(rdp_assignments), by=2)){
rdp_assignments[[i]][rdp_assignments[[i+1]] < 0.8]  <- NA # delete taxa with < 0.8 bootstrap
rdp_assignments[[i+1]][rdp_assignments[[i+1]] < 0.8]  <- NA
rdp_assignments[[i+1]][grepl("^Gp[0-9]+$", rdp_assignments[[i]])]  <- NA # delete taxa Gpxx
rdp_assignments[[i]][grepl("^Gp[0-9]+$", rdp_assignments[[i]])]  <- NA
}
asv_unique <- clustered_df %>%
select(asv_id, asv) %>%
distinct()
# add ASVs
rdp_assignments <- left_join(rdp_assignments, asv_unique, by=c("asv_id"))
outfile <- paste(outdir, "3_TaxAssign_TAS_16S_rdp_clean.csv", sep="")
write.csv(rdp_assignments, outfile, row.names = FALSE)
sortedinfo1 <- read.csv("TAS/out_16S_TAS1/sorted/sortedinfo.csv")
sortedinfo2 <- read.csv("TAS/out_16S_TAS2/sorted/sortedinfo.csv")
getwd()
sortedinfo1 <- read.csv("TAS/out_16S_TAS1/sorted/sortedinfo.csv")
sortedinfo2 <- read.csv("TAS/out_16S_TAS2/sorted/sortedinfo.csv")
mock_composition <- "TAS/user_input/mock_composition.csv" # comma separated csv with columns: sample	run	marker	action	asv
mock_composition_df <- read.csv(mock_composition) %>%
filter(marker == my_marker) %>%
select(-run, -marker)
View(mock_composition_df)
# Set parameter values
missing_occurrences <- paste(outdir, "missing_occurrences_", marker, "_", run, ".csv", sep= "")
# Set parameter values
missing_occurrences <- paste(outdir, "missing_occurrences_", my_marker, ".csv", sep= "")
performance_metrics <- paste(outdir, "performance_metrics_", my_marker, ".csv", sep= "")
known_occurrences <- paste(outdir, "known_occurrences_", my_marker, ".csv", sep= "")
known_occurrences
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
my_marker <- "COI"
outdir <- paste("TAS/out_", my_marker, "_pooled", sep="")
outdir <- check_dir(outdir)
files <- data.frame(file=c("TAS/out_COI_TAS1/11_PoolReplicates.csv", "TAS/out_COI_TAS2/11_PoolReplicates.csv"),
marker=c("COI", "COI"))
outfile <- paste(outdir, "1_Pooled_datasets_COI.csv", sep="")
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
vtam_dir <- "~/vtamR" # clone from https://github.com/meglecz/vtamR
setwd(vtam_dir)
getwd(vtam_dir)
getwd()
cutadapt_path <- "~/miniconda3/envs/vtam/bin" # v3.4
vsearch_path <- "~/miniconda3/envs/vtam/bin" # v2.15.1
blast_path <- "~/miniconda3/envs/vtam/bin" # v2.10.1+
swarm_path <- "" # v2.1.12
num_threads <- 8
sep <- ","
library("devtools")
library("roxygen2")
library("seqinr")
library("dplyr")
library("tidyr")
library("ggplot2")
load_all(".")
roxygenise()
roxygenise()
usethis::use_roxygen_md()
fastq_dir <- "TAS/fastq" # Directory with fasta files
fastqinfo <- "TAS/user_input/fastqinfo.csv" # comma separated csv with columns: tag_fw	primer_fw	tag_rv	primer_rv	sample	sample_type	habitat	replicate	fastq_fw	fastq_rv	marker	run
mock_composition <- "TAS/user_input/mock_composition.csv" # comma separated csv with columns: sample	run	marker	action	asv
asv_list <- "TAS/user_input/asv_list_16S.csv"  # comma separated csv with columns: asv_id, asv; Used only for standardizing asv_id with earlier datasets; The version in Dryad contains all ASV in the TAS1 and TAS2 datasets
my_run <- "TAS2" # TAS1 or TAS2
my_marker <- "16S"
outdir <- paste("TAS/out", my_marker, my_run, sep="_")
outdir <- check_dir(outdir)
fastqinfo_df <- read.csv(fastqinfo) %>%
filter(run == my_run & marker == my_marker) %>%
select(-run, -marker)
mock_composition_df <- read.csv(mock_composition) %>%
filter(run == my_run & marker == my_marker) %>%
select(-run, -marker)
# merge and quality filter
merged_dir <- paste(outdir, "merged/", sep="")
fastainfo_df <- Merge(fastqinfo_df, fastq_dir=fastq_dir, vsearch_path=vsearch_path, outdir=merged_dir, compress=T)
# Count the number of reads in the input fastq files
my_pattern <- paste(my_run, ".*_L001_R1_001.fastq.gz", sep="")
read_count_input <- CountReadsDir(fastq_dir, pattern=my_pattern, file_type="fastq")
read_count_input <- CountReadsDir(fastq_dir, pattern=my_pattern, file_type="fastq")
total_input <- sum(read_count_input$read_count)
print(paste("Number of reads in input fastq files:", total_input))
# Count the number of reads in the outpu fastq=a files
read_count_merged <- CountReadsDir(merged_dir, pattern="_L001_R1_001.fasta.gz", file_type="fasta")
# Count the number of reads in the outpu fastq=a files
read_count_merged <- CountReadsDir(merged_dir, pattern="_L001_R1_001.fasta.gz", file_type="fasta")
total_merged <- sum(read_count_merged$read_count)
print(paste("Number of reads after Merge:", total_merged))
print(paste("Merged/input read counts:", total_merged/total_input))
# demultiplex, trim tags and pimers
sorted_dir <- paste(outdir, "sorted/", sep="")
sortedinfo_df <- SortReads(fastainfo_df, fasta_dir=merged_dir, outdir=sorted_dir, check_reverse=FALSE, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, cutadapt_minimum_length = 247, cutadapt_maximum_length = 259, compress=T, tag_to_end = F, primer_to_end= F)
# Count the number of reads en each demultiplexed file
read_count_sorted <- CountReadsDir(sorted_dir, pattern=".fasta.gz", file_type="fasta")
total_sorted <- sum(read_count_sorted$read_count)
print(paste("Number of reads after SortReads:", total_sorted))
# In the merged file both 16S and COI sequences are present, therefore a low ratio of sortd/merged is not surprising
print(paste("SortReads/Merged read counts:", total_sorted/total_merged))
outfile <- paste(outdir, "1_before_filter.csv", sep="")
updated_asv_list <- paste(outdir, "asv_list_", my_marker,"_",my_run,".csv", sep="")
read_count_df <- Dereplicate(sortedinfo_df, dir=sorted_dir, outfile=outfile, asv_list=asv_list, updated_asv_list=updated_asv_list)
before_filter <- paste(outdir,"1_before_filter.csv", sep="")
read_count_df <- read.csv(before_filter)
read_count_df <- read.csv(before_filter)
sortedinfo <-paste(outdir, "sorted/sortedinfo.csv", sep="")
sortedinfo_df <-  read.csv(sortedinfo)
# data frame for keeping track of read counts, asv count etc.
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
stat_df <- GetStat(read_count_df, stat_df, stage="Input", params=NA)
by_sample <- TRUE
outfile <- paste(outdir, "2_Swarm_by_sample.csv", sep="")
read_count_df <- Swarm(read_count_df, outfile=outfile, swarm_path=swarm_path, num_threads=num_threads, by_sample=by_sample)
read_count_df <- Swarm(read_count_df, outfile=outfile, swarm_path=swarm_path, num_threads=num_threads, by_sample=by_sample)
stat_df <- GetStat(read_count_df, stat_df, stage="Swarm", params=by_sample)
global_read_count_cutoff = 10
outfile <- paste(outdir, "3_LFNglobalReadCount.csv", sep="")
read_count_df <- LFNglobalReadCount(read_count_df, cutoff=global_read_count_cutoff, outfile=outfile)
read_count_df <- LFNglobalReadCount(read_count_df, cutoff=global_read_count_cutoff, outfile=outfile)
stat_df <- GetStat(read_count_df, stat_df, stage="LFNglobalReadCount", params=global_read_count_cutoff)
View(stat_df)
abskew=16
by_sample = T
sample_prop = 0.8
outfile <- paste(outdir, "4_FilterChimera.csv", sep="")
read_count_df <- FilterChimera(read_count_df, outfile=outfile, vsearch_path=vsearch_path, by_sample=by_sample, sample_prop=sample_prop, abskew=abskew)
params <- paste(abskew, by_sample, sample_prop, sep=";")
stat_df <- GetStat(read_count_df, stat_df, stage="FilterChimera", params=params)
outfile = paste(outdir, "OptimizeLFNsampleReplicate.csv", sep="")
OptimizeLFNsampleReplicate_df <- OptimizeLFNsampleReplicate(read_count=read_count_df, mock_composition=mock_composition_df, outfile=outfile)
View(OptimizeLFNsampleReplicate_df)
### Same for TAS1 and TAS2
lfn_sample_replicate_cutoff <- 0.001
outfile <- paste(outdir, "5_LFNsampleReplicate.csv", sep="")
read_count_df <- LFNsampleReplicate(read_count_df, cutoff=lfn_sample_replicate_cutoff, outfile=outfile)
stat_df <- GetStat(read_count_df, stat_df, stage="LFNsampleReplicate", params=lfn_sample_replicate_cutoff)
read_count_samples_df <- PoolReplicates(read_count_df)
results <- MakeKnownOccurrences(read_count_samples = read_count_samples_df, sortedinfo=sortedinfo_df, mock_composition=mock_composition_df)
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
outfile = paste(outdir, "OptimizeLFNreadCountLFNvariant.csv", sep="")
OptimizeLFNreadCountLFNvariant_df <- OptimizeLFNreadCountLFNvariant(read_count_df, known_occurrences=known_occurrences_df, outfile= outfile, min_replicate_number=1, by_replicate=TRUE)
View(OptimizeLFNsampleReplicate_df)
View(OptimizeLFNsampleReplicate_df)
View(OptimizeLFNreadCountLFNvariant_df)
### Same for TAS1 and TAS2
lnf_variant_cutoff = 0.001
by_replicate = TRUE
outfile <- paste(outdir, "6_LFNvariant.csv", sep="")
read_count_df_lnf_variant <- LFNvariant(read_count_df, cutoff=lnf_variant_cutoff, outfile=outfile, by_replicate=by_replicate)
params <- paste(lnf_variant_cutoff, by_replicate, sep=";")
stat_df <- GetStat(read_count_df_lnf_variant, stat_df, stage="LFNvariant", params=params)
### TAS2
lfn_read_count_cutoff <- 100
outfile <- paste(outdir, "7_LFNreadCount.csv", sep="")
read_count_df_lfn_read_count <- LFNreadCount(read_count_df, cutoff=lfn_read_count_cutoff, outfile=outfile)
stat_df <- GetStat(read_count_df_lfn_read_count, stat_df, stage="LFNreadCount", params=lfn_read_count_cutoff)
outfile <- paste(outdir, "8_poolLFN.csv", sep="")
read_count_df <- PoolFilters(read_count_df_lfn_read_count, read_count_df_lnf_variant, outfile=outfile)
stat_df <- GetStat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
rm(read_count_df_lfn_read_count)
rm(read_count_df_lnf_variant)
# delete temporary data frames
rm(read_count_df_lfn_read_count)
rm(read_count_df_lnf_variant)
outfile <- paste(outdir, "9_PoolReplicates.csv", sep="")
read_count_samples_df <- PoolReplicates(read_count_df, outfile=outfile)
stat_df <- GetStat(read_count_samples_df, stat_df, stage="PoolReplicates")
# Set parameter values
missing_occurrences <- paste(outdir, "missing_occurrences_", my_marker, "_", my_run, ".csv", sep= "")
performance_metrics <- paste(outdir, "performance_metrics_", my_marker, "_", my_run, ".csv", sep= "")
known_occurrences <- paste(outdir, "known_occurrences_", my_marker, "_", my_run, ".csv", sep= "")
# Run function
results <- MakeKnownOccurrences(read_count_samples_df, sortedinfo=sortedinfo_df, mock_composition=mock_composition_df, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, performance_metrics=performance_metrics)
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
outfile <-  paste(outdir, "stat_steps_", my_marker, "_", my_run, ".csv", sep="")
write.csv(stat_df, file =outfile)
outfile=paste(outdir, "Final_asvtable_", my_marker, "_", my_run, ".csv", sep="")
asv_table_df <- WriteASVtable(read_count_samples_df, outfile=outfile, sortedinfo=sortedinfo_df, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition_df)
cutadapt_path <- "~/miniconda3/envs/vtam/bin" # v3.4
vsearch_path <- "~/miniconda3/envs/vtam/bin" # v2.15.1
blast_path <- "~/miniconda3/envs/vtam/bin" # v2.10.1+
swarm_path <- "" # v2.1.12
num_threads <- 8
sep <- ","
getwd()
fastq_dir <- "TAS/fastq" # Directory with fasta files
fastqinfo <- "TAS/user_input/fastqinfo.csv" # comma separated csv with columns: tag_fw	primer_fw	tag_rv	primer_rv	sample	sample_type	habitat	replicate	fastq_fw	fastq_rv	marker	run
mock_composition <- "TAS/user_input/mock_composition.csv" # comma separated csv with columns: sample	run	marker	action	asv
asv_list <- "TAS/user_input/asv_list_COI.csv"  # comma separated csv with columns: asv_id, asv; Used only for standardizing asv_id with earlier datasets; The version in Dryad contains all ASV in the TAS1 and TAS2 datasets
my_run <- "TAS2" # TAS1 or TAS2
my_marker <- "COI"
outdir <- paste("TAS/out", my_marker, my_run, sep="_")
outdir <- check_dir(outdir)
fastqinfo_df <- read.csv(fastqinfo) %>%
filter(run == my_run & marker == my_marker) %>%
select(-run, -marker)
mock_composition_df <- read.csv(mock_composition) %>%
filter(run == my_run & marker == my_marker) %>%
select(-run, -marker)
# merge and quality filter
merged_dir <- paste(outdir, "merged/", sep="")
fastainfo_df <- Merge(fastqinfo_df, fastq_dir=fastq_dir, vsearch_path=vsearch_path, outdir=merged_dir, compress=T)
# Count the number of reads in the input fastq files
my_pattern <- paste(my_run, ".*_L001_R1_001.fastq.gz", sep="")
read_count_input <- CountReadsDir(fastq_dir, pattern=my_pattern, file_type="fastq")
read_count_input <- CountReadsDir(fastq_dir, pattern=my_pattern, file_type="fastq")
total_input <- sum(read_count_input$read_count)
print(paste("Number of reads in input fastq files:", total_input))
# Count the number of reads in the outpu fastq=a files
read_count_merged <- CountReadsDir(merged_dir, pattern="_L001_R1_001.fasta.gz", file_type="fasta")
total_merged <- sum(read_count_merged$read_count)
print(paste("Number of reads after Merge:", total_merged))
print(paste("Merged/input read counts:", total_merged/total_input))
# demultiplex, trim tags and pimers
sorted_dir <- paste(outdir, "sorted/", sep="")
sortedinfo_df <- SortReads(fastainfo_df, fasta_dir=merged_dir, outdir=sorted_dir, check_reverse=FALSE, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, cutadapt_minimum_length = 301, cutadapt_maximum_length = 322, compress=T, tag_to_end = F, primer_to_end= F)
sortedinfo_df <- SortReads(fastainfo_df, fasta_dir=merged_dir, outdir=sorted_dir, check_reverse=FALSE, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, cutadapt_minimum_length = 301, cutadapt_maximum_length = 322, compress=T, tag_to_end = F, primer_to_end= F)
# Count the number of reads en each demultiplexed file
read_count_sorted <- CountReadsDir(sorted_dir, pattern=".fasta.gz", file_type="fasta")
total_sorted <- sum(read_count_sorted$read_count)
print(paste("Number of reads after SortReads:", total_sorted))
# In the merged file both 16S and COI sequences are present, therefore a low ratio of sortd/merged is not surprising
print(paste("SortReads/Merged read counts:", total_sorted/total_merged))
outfile <- paste(outdir, "1_before_filter.csv", sep="")
updated_asv_list <- paste(outdir, "asv_list_", my_marker,"_",my_run,".csv", sep="")
read_count_df <- Dereplicate(sortedinfo_df, dir=sorted_dir, outfile=outfile, asv_list=asv_list, updated_asv_list=updated_asv_list)
# data frame for keeping track of read counts, asv count etc.
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
stat_df <- GetStat(read_count_df, stat_df, stage="Input", params=NA)
by_sample <- TRUE
outfile <- paste(outdir, "2_Swarm_by_sample.csv", sep="")
read_count_df <- Swarm(read_count_df, outfile=outfile, swarm_path=swarm_path, num_threads=num_threads, by_sample=by_sample)
read_count_df <- Swarm(read_count_df, outfile=outfile, swarm_path=swarm_path, num_threads=num_threads, by_sample=by_sample)
stat_df <- GetStat(read_count_df, stat_df, stage="Swarm", params=by_sample)
global_read_count_cutoff = 10
outfile <- paste(outdir, "3_LFNglobalReadCount.csv", sep="")
read_count_df <- LFNglobalReadCount(read_count_df, cutoff=global_read_count_cutoff, outfile=outfile)
read_count_df <- LFNglobalReadCount(read_count_df, cutoff=global_read_count_cutoff, outfile=outfile)
stat_df <- GetStat(read_count_df, stat_df, stage="LFNglobalReadCount", params=global_read_count_cutoff)
outfile <- paste(outdir, "4_FilterIndel.csv", sep="")
read_count_df <- FilterIndel(read_count_df, outfile=outfile)
stat_df <- GetStat(read_count_df, stat_df, stage="FilterIndel")
outfile <- paste(outdir, "5_FilterCodonStop.csv", sep="")
genetic_code = 5
read_count_df <- FilterCodonStop(read_count_df, outfile=outfile, genetic_code=genetic_code)
read_count_df <- FilterCodonStop(read_count_df, outfile=outfile, genetic_code=genetic_code)
stat_df <- GetStat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
outfile = paste(outdir, "OptimizeLFNsampleReplicate.csv", sep="")
OptimizeLFNsampleReplicate_df <- OptimizeLFNsampleReplicate(read_count=read_count_df, mock_composition=mock_composition_df, outfile=outfile)
### Same for TAS1 and TAS2
lfn_sample_replicate_cutoff <- 0.001
outfile <- paste(outdir, "7_LFNsampleReplicate.csv", sep="")
read_count_df <- LFNsampleReplicate(read_count_df, cutoff=lfn_sample_replicate_cutoff, outfile=outfile)
stat_df <- GetStat(read_count_df, stat_df, stage="LFNsampleReplicate", params=lfn_sample_replicate_cutoff)
read_count_samples_df <- PoolReplicates(read_count_df)
results <- MakeKnownOccurrences(read_count_samples = read_count_samples_df, sortedinfo=sortedinfo_df, mock_composition=mock_composition_df)
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
outfile = paste(outdir, "OptimizeLFNreadCountLFNvariant.csv", sep="")
OptimizeLFNreadCountLFNvariant_df <- OptimizeLFNreadCountLFNvariant(read_count_df, known_occurrences=known_occurrences_df, outfile= outfile, min_replicate_number=1, by_replicate=TRUE)
params <- paste(lnf_variant_cutoff, by_replicate, sep=";")
stat_df <- GetStat(read_count_df_lnf_variant, stat_df, stage="LFNvariant", params=params)
### TAS2
lnf_variant_cutoff = 0.001
by_replicate = TRUE
outfile <- paste(outdir, "8_LFNvariant.csv", sep="")
read_count_df_lnf_variant <- LFNvariant(read_count_df, cutoff=lnf_variant_cutoff, outfile=outfile, by_replicate=by_replicate)
params <- paste(lnf_variant_cutoff, by_replicate, sep=";")
stat_df <- GetStat(read_count_df_lnf_variant, stat_df, stage="LFNvariant", params=params)
View(stat_df)
### TAS1and TAS 2
lfn_read_count_cutoff <- 15
outfile <- paste(outdir, "9_LFNreadCount.csv", sep="")
read_count_df_lfn_read_count <- LFNreadCount(read_count_df, cutoff=lfn_read_count_cutoff, outfile=outfile)
stat_df <- GetStat(read_count_df_lfn_read_count, stat_df, stage="LFNreadCount", params=lfn_read_count_cutoff)
outfile <- paste(outdir, "10_poolLFN.csv", sep="")
read_count_df <- PoolFilters(read_count_df_lfn_read_count, read_count_df_lnf_variant, outfile=outfile)
stat_df <- GetStat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
rm(read_count_df_lfn_read_count)
rm(read_count_df_lnf_variant)
outfile <- paste(outdir, "11_PoolReplicates.csv", sep="")
read_count_samples_df <- PoolReplicates(read_count_df, outfile=outfile)
stat_df <- GetStat(read_count_samples_df, stat_df, stage="PoolReplicates")
# Set parameter values
missing_occurrences <- paste(outdir, "missing_occurrences_", my_marker, "_", my_run, ".csv", sep= "")
performance_metrics <- paste(outdir, "performance_metrics_", my_marker, "_", my_run, ".csv", sep= "")
known_occurrences <- paste(outdir, "known_occurrences_", my_marker, "_", my_run, ".csv", sep= "")
# Run function
results <- MakeKnownOccurrences(read_count_samples_df, sortedinfo=sortedinfo_df, mock_composition=mock_composition_df, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, performance_metrics=performance_metrics)
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
outfile <-  paste(outdir, "stat_steps_", my_marker, "_", my_run, ".csv", sep="")
write.csv(stat_df, file =outfile)
outfile=paste(outdir, "Final_asvtable_", my_marker, "_", my_run, ".csv", sep="")
asv_table_df <- WriteASVtable(read_count_samples_df, outfile=outfile, sortedinfo=sortedinfo_df, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition_df)
asv_table_df <- WriteASVtable(read_count_samples_df, outfile=outfile, sortedinfo=sortedinfo_df, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition_df)
cutadapt_path <- "~/miniconda3/envs/vtam/bin" # v3.4
vsearch_path <- "~/miniconda3/envs/vtam/bin" # v2.15.1
blast_path <- "~/miniconda3/envs/vtam/bin" # v2.10.1+
swarm_path <- "" # v2.1.12
num_threads <- 8
sep <- ","
my_marker <- "16S"
outdir <- paste("TAS/out_", my_marker, "_pooled", sep="")
outdir <- check_dir(outdir)
files <- data.frame(file=c("TAS/out_16S_TAS1/9_PoolReplicates.csv", "TAS/out_16S_TAS2/9_PoolReplicates.csv"),
marker=c("16S", "16S"))
outfile <- paste(outdir, "1_Pooled_datasets_", my_marker, ".csv", sep="")
read_count_pool <- PoolDatasets(files, outfile=outfile)
outfile <- paste(outdir, "2_Clustered_ASV_COI.csv", sep="")
clustered_df <- Cluster_size(read_count_pool, id=0.97, vsearch_path=vsearch_path, outfile=outfile)
fasta <- paste(outdir, "3_asv_to_assign.fasta", sep="")
outfile <- paste(outdir, "3_TaxAssign_TAS_16S_rdp.csv", sep="")
write_fasta_df(clustered_df, fasta)
### Use local installation of rdp_classifier_2.14, downloadable from https://sourceforge.net/projects/rdp-classifier/
cmd <- paste('java -Xmx8g -jar ~/rdp_classifier_2.14/dist/classifier.jar classify -t ~/rdp_classifier_2.14/src/data/classifier/16srrna/rRNAClassifier.properties -f fixrank -o ', outfile, ' ', fasta)
print(cmd)
system(cmd)
outfile <- paste(outdir, "2_Clustered_ASV_", my_marker, ".csv", sep="")
outfile
fasta <- paste(outdir, "3_asv_to_assign.fasta", sep="")
outfile <- paste(outdir, "3_TaxAssign_TAS_",my_marker,"_rdp.csv", sep="")
write_fasta_df(clustered_df, fasta)
### Use local installation of rdp_classifier_2.14, downloadable from https://sourceforge.net/projects/rdp-classifier/
cmd <- paste('java -Xmx8g -jar ~/rdp_classifier_2.14/dist/classifier.jar classify -t ~/rdp_classifier_2.14/src/data/classifier/16srrna/rRNAClassifier.properties -f fixrank -o ', outfile, ' ', fasta)
print(cmd)
system(cmd)
asv_unique <- clustered_df %>%
select(asv_id, asv) %>%
distinct()
# add ASVs
rdp_assignments <- left_join(rdp_assignments, asv_unique, by=c("asv_id"))
# select columns
rdp_assignments <- read.table(outfile, sep="\t", header=FALSE, fill = TRUE) %>%
select(1,3,5,6,8,9,11,12,14,15,17,18,20)
colnames(rdp_assignments) <- c("asv_id", "domain", "domain_bootstrap", "phylum", "phylum_bootstrap", "class", "class_bootstrap", "order", "order_bootstrap", "family", "family_bootstrap", "genus", "genus_bootstrap")
# delete Chloroplast
rdp_assignments <- rdp_assignments %>%
filter(class != "Chloroplast")
for(i in seq(2, ncol(rdp_assignments), by=2)){
rdp_assignments[[i]][rdp_assignments[[i+1]] < 0.8]  <- NA # delete taxa with < 0.8 bootstrap
rdp_assignments[[i+1]][rdp_assignments[[i+1]] < 0.8]  <- NA
rdp_assignments[[i+1]][grepl("^Gp[0-9]+$", rdp_assignments[[i]])]  <- NA # delete taxa Gpxx
rdp_assignments[[i]][grepl("^Gp[0-9]+$", rdp_assignments[[i]])]  <- NA
}
asv_unique <- clustered_df %>%
select(asv_id, asv) %>%
distinct()
# add ASVs
rdp_assignments <- left_join(rdp_assignments, asv_unique, by=c("asv_id"))
outfile <- paste(outdir, "3_TaxAssign_TAS_",my_marker,"_rdp_clean.csv", sep="")
write.csv(rdp_assignments, outfile, row.names = FALSE)
# write ASV table completed by taxonomic assignments
outfile=paste(outdir, "4_mOTUtable_with_TaxAssign_",my_marker,".csv", sep="")
sortedinfo1 <- read.csv("TAS/out_16S_TAS1/sorted/sortedinfo.csv")
sortedinfo2 <- read.csv("TAS/out_16S_TAS2/sorted/sortedinfo.csv")
sortedinfo_df <- rbind(sortedinfo1, sortedinfo2)
mock_composition <- "TAS/user_input/mock_composition.csv" # comma separated csv with columns: sample	run	marker	action	asv
mock_composition_df <- read.csv(mock_composition) %>%
filter(marker == my_marker) %>%
select(-run, -marker)
asv_table_df <- WriteASVtable(clustered_df, outfile=outfile, asv_tax=rdp_assignments, sortedinfo=sortedinfo_df, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition_df)
# Set parameter values
missing_occurrences <- paste(outdir, "missing_occurrences_", my_marker, ".csv", sep= "")
performance_metrics <- paste(outdir, "performance_metrics_", my_marker, ".csv", sep= "")
known_occurrences <- paste(outdir, "known_occurrences_", my_marker, ".csv", sep= "")
# Run function
results <- MakeKnownOccurrences(clustered_df, sortedinfo=sortedinfo_df, mock_composition=mock_composition_df, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, performance_metrics=performance_metrics)
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
View(known_occurrences_df)
View(missing_occurrences_df)
View(mock_composition_df)
View(performance_metrics_df)
outfile <-  paste(outdir, "stat_steps_", my_marker, "_", my_run, ".csv", sep="")
dim(asv_table_df)
View(asv_table_df)
cutadapt_path <- "~/miniconda3/envs/vtam/bin" # v3.4
vsearch_path <- "~/miniconda3/envs/vtam/bin" # v2.15.1
blast_path <- "~/miniconda3/envs/vtam/bin" # v2.10.1+
swarm_path <- "" # v2.1.12
num_threads <- 8
sep <- ","
taxonomy <- "~/mkCOInr/COInr/COInr_for_vtam_2025_05_23_dbV5/COInr_for_vtam_taxonomy.tsv"
blast_db <- "~/mkCOInr/COInr/COInr_for_vtam_2025_05_23_dbV5/COInr_for_vtam"
my_marker <- "COI"
outdir <- paste("TAS/out_", my_marker, "_pooled", sep="")
outdir <- check_dir(outdir)
files <- data.frame(file=c("TAS/out_COI_TAS1/11_PoolReplicates.csv", "TAS/out_COI_TAS2/11_PoolReplicates.csv"),
marker=c("COI", "COI"))
outfile <- paste(outdir, "1_Pooled_datasets_COI.csv", sep="")
read_count_pool <- PoolDatasets(files, outfile=outfile)
View(read_count_pool)
files <- data.frame(file=c("TAS/out_COI_TAS1/11_PoolReplicates.csv", "TAS/out_COI_TAS2/11_PoolReplicates.csv"),
marker=c("COI", "COI"))
outfile <- paste(outdir, "1_Pooled_datasets_",my_marker,".csv", sep="")
read_count_pool <- PoolDatasets(files, outfile=outfile)
outfile <- paste(outdir, "2_Clustered_ASV_",my_marker,".csv", sep="")
clustered_df <- Cluster_size(read_count_pool, id=0.97, vsearch_path=vsearch_path, outfile=outfile)
ltg_params_df = data.frame( pid=c(100,97,95,90,85),
pcov=c(80,80,80,80,80),
phit=c(90,90,90,90,90),
taxn=c(1,1,3,3,3),
seqn=c(1,1,3,3,3),
refres=c(1,1,1,1,1),
ltgres=c(8,8,8,8,8)
)
# Set parameter values
outfile <- paste(outdir, "3_TaxAssign_",my_marker,".csv", sep="")
# Run function
asv_tax_VTAM <- TaxAssign(asv=clustered_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outfile=outfile, num_threads=num_threads, ltg_params=ltg_params_df, fill_lineage=TRUE)
# write ASV table completed by taxonomic assignments
outfile=paste(outdir, "4_mOTUtable_with_TaxAssign_",my_marker,".csv", sep="")
sortedinfo1 <- read.csv("TAS/out_COI_TAS1/sorted/sortedinfo.csv")
sortedinfo2 <- read.csv("TAS/out_COI_TAS2/sorted/sortedinfo.csv")
sortedinfo_df <- rbind(sortedinfo1, sortedinfo2)
mock_composition <- "TAS/user_input/mock_composition.csv" # comma separated csv with columns: sample	run	marker	action	asv
mock_composition_df <- read.csv(mock_composition) %>%
filter(marker == my_marker) %>%
select(-run, -marker)
asv_table_df <- WriteASVtable(clustered_df, outfile=outfile, asv_tax=asv_tax_VTAM, sortedinfo=sortedinfo, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition)
asv_table_df <- WriteASVtable(clustered_df, outfile=outfile, asv_tax=asv_tax_VTAM, sortedinfo=sortedinfo_df, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition_df)
# Set parameter values
missing_occurrences <- paste(outdir, "missing_occurrences_", my_marker, ".csv", sep= "")
performance_metrics <- paste(outdir, "performance_metrics_", my_marker, ".csv", sep= "")
known_occurrences <- paste(outdir, "known_occurrences_", my_marker, ".csv", sep= "")
# Run function
results <- MakeKnownOccurrences(clustered_df, sortedinfo=sortedinfo_df, mock_composition=mock_composition_df, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, performance_metrics=performance_metrics)
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
View(missing_occurrences_df)
View(missing_occurrences_df)
View(mock_composition_df)
View(mock_composition_df)
View(performance_metrics_df)
View(asv_table_df)
View(asv_table_df)
dim(asv_table_df)
