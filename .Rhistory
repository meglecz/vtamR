### LFN_filters
###
# LFN_read_count
lfn_read_count_cutoff = 10
read_count_df_lfn_read_count <- LFN_read_count(read_count_df, lfn_read_count_cutoff, write_csv=F, outdir = outdir)
stat_df <- get_stat(read_count_df_lfn_read_count, stat_df, stage="LFN_read_count", params=lfn_read_count_cutoff)
# LFN_sample_replicate (by column)
lfn_sample_replicate_cutoff = 0.001
read_count_df_lnf_sample_replicate <- LFN_sample_replicate(read_count_df, lfn_sample_replicate_cutoff, write_csv=F, outdir = outdir)
stat_df <- get_stat(read_count_df_lnf_sample_replicate, stat_df, stage="LFN_sample_replicate", params=lfn_sample_replicate_cutoff)
# LFN_sample_variant (by line)
lnf_variant_cutoff = 0.001
by_replicate = TRUE
read_count_df_lnf_variant <- LFN_variant(read_count_df, lnf_variant_cutoff, by_replicate, write_csv=F, outdir = outdir)
param_values <- paste(lnf_variant_cutoff, by_replicate, sep=";")
stat_df <- get_stat(read_count_df_lnf_variant, stat_df, stage="LFN_variant", params=param_values)
# pool the results of the different filterLFN to one data frame; keep only occurrences that passed all filters
read_count_df <- pool_LFN(read_count_df_lfn_read_count, read_count_df_lnf_variant, read_count_df_lnf_sample_replicate, write_csv=T, outdir = outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
read_count_df_lfn_read_count <- NULL
read_count_df_lnf_variant <- NULL
read_count_df_lnf_sample_replicate <- NULL
###
### keep repeatable occurrences
###
min_relicate_number = 2
read_count_df <- FilterMinReplicateNumber(read_count_df, min_relicate_number, write_csv=F, outdir = outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterMinReplicateNumber", params=min_relicate_number)
###
### FilerIndel
###
read_count_df <- FilterIndel(read_count_df, write_csv=F, outdir=outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterIndel")
###
### FilerCodonStop
###
genetic_code = 5
read_count_df <- FilterCodonStop(read_count_df, write_csv=F, outdir=outdir, genetic_code=genetic_code)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
###
### FilerPCRerror
###
pcr_error_var_prop = 0.1
max_mismatch = 1
by_sample = F
sample_prop = 0.8
vsearch_path = ""
read_count_df <- FilterPCRerror(read_count_df, write_csv=F, outdir=outdir, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch, by_sample=by_sample, sample_prop=sample_prop)
params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilerPCRerror", params=params)
###
### print output files
###
write.csv(stat_df, file = paste(outdir, "count_stat.csv", sep=""))
write.csv(read_count_df, file = paste(outdir, "read_count_final.csv", sep=""))
end_time <- Sys.time()  # Record the end time
runtime <- end_time - start_time  # Calculate the runtime
print(runtime)
setwd("~/vtamR")
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
####
fastqdir <- "local/mfzr/sorted/"
fileinfo <- "local/user_input/fileinfo_mfzr.csv"
outdir <- check_dir(dir="local/out")
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
# read input fasta files in fileinfo, demultiplex and count the number of reads in each plate-sample-replicate
read_count_df <- read_fastas_from_fileinfo(file=fileinfo, dir=fastqdir, write_csv=F, outdir=outdir)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
temp_df <- read_count_df
###
###
### FilerPCRerror
###
pcr_error_var_prop = 0.1
max_mismatch = 1
by_sample = F
sample_prop = 0.8
vsearch_path = ""
read_count_df <- FilterPCRerror(read_count_df, write_csv=F, outdir=outdir, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch, by_sample=by_sample, sample_prop=sample_prop)
params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilerPCRerror", params=params)
###
### print output files
###
write.csv(stat_df, file = paste(outdir, "count_stat.csv", sep=""))
write.csv(read_count_df, file = paste(outdir, "read_count_final.csv", sep=""))
end_time <- Sys.time()  # Record the end time
runtime <- end_time - start_time  # Calculate the runtime
print(runtime)
View(read_count_df)
setwd("~/vtamR")
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
fastqdir <- "/home/meglecz/vtam_benchmark_local/vtam_fish/sorted_mfzr/"
fileinfo <-"/home/meglecz/vtam_benchmark_local/vtam_fish/sorted_mfzr/fileinfo_vtamr.csv"
outdir <- check_dir(dir="local/out")
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
# read input fasta files in fileinfo, demultiplex and count the number of reads in each plate-sample-replicate
read_count_df <- read_fastas_from_fileinfo(file=fileinfo, dir=fastqdir, write_csv=F, outdir=outdir)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
temp_df <- read_count_df
###
### LFN_global_read_count
###
# Eliminate variants with less than global_read_count_cutoff reads in the dataset
global_read_count_cutoff = 2
read_count_df <- LFN_global_read_count(read_count_df, global_read_count_cutoff, write_csv=F, outdir=outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
###
### LFN_filters
###
# LFN_read_count
lfn_read_count_cutoff = 10
read_count_df_lfn_read_count <- LFN_read_count(read_count_df, lfn_read_count_cutoff, write_csv=F, outdir = outdir)
stat_df <- get_stat(read_count_df_lfn_read_count, stat_df, stage="LFN_read_count", params=lfn_read_count_cutoff)
# LFN_sample_replicate (by column)
lfn_sample_replicate_cutoff = 0.001
read_count_df_lnf_sample_replicate <- LFN_sample_replicate(read_count_df, lfn_sample_replicate_cutoff, write_csv=F, outdir = outdir)
stat_df <- get_stat(read_count_df_lnf_sample_replicate, stat_df, stage="LFN_sample_replicate", params=lfn_sample_replicate_cutoff)
# LFN_sample_variant (by line)
lnf_variant_cutoff = 0.001
by_replicate = TRUE
read_count_df_lnf_variant <- LFN_variant(read_count_df, lnf_variant_cutoff, by_replicate, write_csv=F, outdir = outdir)
param_values <- paste(lnf_variant_cutoff, by_replicate, sep=";")
stat_df <- get_stat(read_count_df_lnf_variant, stat_df, stage="LFN_variant", params=param_values)
# pool the results of the different filterLFN to one data frame; keep only occurrences that passed all filters
read_count_df <- pool_LFN(read_count_df_lfn_read_count, read_count_df_lnf_variant, read_count_df_lnf_sample_replicate, write_csv=T, outdir = outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
read_count_df_lfn_read_count <- NULL
read_count_df_lnf_variant <- NULL
read_count_df_lnf_sample_replicate <- NULL
###
### keep repeatable occurrences
###
min_relicate_number = 2
read_count_df <- FilterMinReplicateNumber(read_count_df, min_relicate_number, write_csv=F, outdir = outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterMinReplicateNumber", params=min_relicate_number)
###
### FilerIndel
###
read_count_df <- FilterIndel(read_count_df, write_csv=F, outdir=outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterIndel")
###
### FilerCodonStop
###
genetic_code = 5
read_count_df <- FilterCodonStop(read_count_df, write_csv=F, outdir=outdir, genetic_code=genetic_code)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
###
### FilerPCRerror
###
pcr_error_var_prop = 0.1
max_mismatch = 1
by_sample = T
sample_prop = 0.8
vsearch_path = ""
read_count_df <- FilterPCRerror(read_count_df, write_csv=F, outdir=outdir, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch, by_sample=by_sample, sample_prop=sample_prop)
params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilerPCRerror", params=params)
###
### print output files
###
write.csv(stat_df, file = paste(outdir, "count_stat.csv", sep=""))
write.csv(read_count_df, file = paste(outdir, "read_count_final.csv", sep=""))
end_time <- Sys.time()  # Record the end time
runtime <- end_time - start_time  # Calculate the runtime
print(runtime)
setwd("~/vtamR")
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
fastqdir <- "/home/meglecz/vtam_benchmark_local/vtam_bat/fasta/"
fileinfo <- "/home/meglecz/vtam_benchmark_local/vtam_bat/fasta/fileinfo_vtamr.csv"
# create the output directory and check the the slash at the end
outdir <- check_dir(dir="local/out")
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
# read input fasta files in fileinfo, demultiplex and count the number of reads in each plate-sample-replicate
read_count_df <- read_fastas_from_fileinfo(file=fileinfo, dir=fastqdir, write_csv=F, outdir=outdir)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
temp_df <- read_count_df
###
### LFN_global_read_count
###
# Eliminate variants with less than global_read_count_cutoff reads in the dataset
global_read_count_cutoff = 2
read_count_df <- LFN_global_read_count(read_count_df, global_read_count_cutoff, write_csv=F, outdir=outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
###
### LFN_filters
###
# LFN_read_count
lfn_read_count_cutoff = 10
read_count_df_lfn_read_count <- LFN_read_count(read_count_df, lfn_read_count_cutoff, write_csv=F, outdir = outdir)
stat_df <- get_stat(read_count_df_lfn_read_count, stat_df, stage="LFN_read_count", params=lfn_read_count_cutoff)
# LFN_sample_replicate (by column)
lfn_sample_replicate_cutoff = 0.001
read_count_df_lnf_sample_replicate <- LFN_sample_replicate(read_count_df, lfn_sample_replicate_cutoff, write_csv=F, outdir = outdir)
stat_df <- get_stat(read_count_df_lnf_sample_replicate, stat_df, stage="LFN_sample_replicate", params=lfn_sample_replicate_cutoff)
# LFN_sample_variant (by line)
lnf_variant_cutoff = 0.001
by_replicate = TRUE
read_count_df_lnf_variant <- LFN_variant(read_count_df, lnf_variant_cutoff, by_replicate, write_csv=F, outdir = outdir)
param_values <- paste(lnf_variant_cutoff, by_replicate, sep=";")
stat_df <- get_stat(read_count_df_lnf_variant, stat_df, stage="LFN_variant", params=param_values)
# pool the results of the different filterLFN to one data frame; keep only occurrences that passed all filters
read_count_df <- pool_LFN(read_count_df_lfn_read_count, read_count_df_lnf_variant, read_count_df_lnf_sample_replicate, write_csv=T, outdir = outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
read_count_df_lfn_read_count <- NULL
read_count_df_lnf_variant <- NULL
read_count_df_lnf_sample_replicate <- NULL
###
### keep repeatable occurrences
###
min_relicate_number = 2
read_count_df <- FilterMinReplicateNumber(read_count_df, min_relicate_number, write_csv=F, outdir = outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterMinReplicateNumber", params=min_relicate_number)
###
### FilerIndel
###
read_count_df <- FilterIndel(read_count_df, write_csv=F, outdir=outdir)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterIndel")
###
### FilerCodonStop
###
genetic_code = 5
read_count_df <- FilterCodonStop(read_count_df, write_csv=F, outdir=outdir, genetic_code=genetic_code)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
###
### FilerPCRerror
###
pcr_error_var_prop = 0.1
max_mismatch = 1
by_sample = T
sample_prop = 0.8
vsearch_path = ""
read_count_df <- FilterPCRerror(read_count_df, write_csv=F, outdir=outdir, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch, by_sample=by_sample, sample_prop=sample_prop)
params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilerPCRerror", params=params)
###
### print output files
###
write.csv(stat_df, file = paste(outdir, "count_stat.csv", sep=""))
write.csv(read_count_df, file = paste(outdir, "read_count_final.csv", sep=""))
end_time <- Sys.time()  # Record the end time
runtime <- end_time - start_time  # Calculate the runtime
print(runtime)
setwd("~/vtamR")
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
####
# define input filenames
fastqdir <- "local/small_test"
fileinfo <- "local/user_input/fileinfo_small.csv"
# create the output directory and check the the slash at the end
outdir <- check_dir(dir="local/out")
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
# read input fasta files in fileinfo, demultiplex and count the number of reads in each plate-sample-replicate
read_count_df <- read_fastas_from_fileinfo(file=fileinfo, dir=fastqdir, write_csv=F, outdir=outdir)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
temp_df <- read_count_df
# get unique list of ASVs with their total read_count in the run
unique_asv_df <- read_count_df %>%
group_by(asv) %>%
summarize(read_count = sum(read_count)) %>%
arrange(desc(read_count))
View(unique_asv_df)
temp_u <- unique_asv_df
# no ASV in the unique_asv_df => return a dataframe with 0 for all ASVs in Chimera column
if(length(unique_asv_df$asv) == 0){
unique_asv_df$chimera <- rep(0, length(unique_asv_df$asv))
return(unique_asv_df)
}
# create a tmp directory for temporary files using time and a random number
outdir_tmp <- paste(outdir, 'tmp_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
outdir_tmp <- check_dir(outdir_tmp)
# make fasta file with unique reads; use sequences as ids
fas <- paste(outdir_tmp, 'unique.fas', sep="")
# Open the file for writing
file <- file(fas, "w")
# Iterate over the sequences and write them to the file
for (i in seq_along(unique_asv_df$asv)) {
header <- paste0(">",unique_asv_df$asv[i], ";size=", unique_asv_df$read_count[i], sep="")
writeLines(c(header, unique_asv_df$asv[i], ""), file)
}
close(file)
# vsearch --usearch_global to find highly similar sequence pairs
vsearch_out <- paste(outdir_tmp, 'unique_vsearch_out.out', sep="")
#  'vsearch --uchime3_denovo '. $sfas.' --uchimeout '.$chim_out.' --quiet';
vsearch <- paste(vsearch_path, "vsearch --uchime3_denovo ", fas, "  --quiet --uchimeout  ", vsearch_out, sep="")
#  'vsearch --uchime3_denovo '. $sfas.' --uchimeout '.$chim_out.' --quiet';
vsearch <- paste(vsearch_path, "vsearch --uchime3_denovo ", fas, " --quiet --uchimeout  ", vsearch_out, sep="")
vsearch_path = ""
#  'vsearch --uchime3_denovo '. $sfas.' --uchimeout '.$chim_out.' --quiet';
vsearch <- paste(vsearch_path, "vsearch --uchime3_denovo ", fas, " --quiet --uchimeout  ", vsearch_out, sep="")
vsearch
abskew=2
#  'vsearch --uchime3_denovo '. $sfas.' --uchimeout '.$chim_out.' --quiet';
vsearch <- paste(vsearch_path, "vsearch --uchime3_denovo ", fas, " --quiet --abskew ", abskew ," --uchimeout  ", vsearch_out, sep="")
vsearch
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
####
# define input filenames
fastqdir <- "local/small_test"
fileinfo <- "local/user_input/fileinfo_small.csv"
# create the output directory and check the the slash at the end
outdir <- check_dir(dir="local/out")
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
# read input fasta files in fileinfo, demultiplex and count the number of reads in each plate-sample-replicate
read_count_df <- read_fastas_from_fileinfo(file=fileinfo, dir=fastqdir, write_csv=F, outdir=outdir)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
temp_df <- read_count_df
temp_u <- unique_asv_df
vsearch_path = ""
abskew=2
# get unique list of ASVs with their total read_count in the run
unique_asv_df <- read_count_df %>%
group_by(asv) %>%
summarize(read_count = sum(read_count)) %>%
arrange(desc(read_count))
vsearch_path = ""
abskew=2
# no ASV in the unique_asv_df => return a dataframe with 0 for all ASVs in Chimera column
if(length(unique_asv_df$asv) == 0){
unique_asv_df$chimera <- rep(0, length(unique_asv_df$asv))
return(unique_asv_df)
}
# create a tmp directory for temporary files using time and a random number
outdir_tmp <- paste(outdir, 'tmp_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
outdir_tmp <- check_dir(outdir_tmp)
# make fasta file with unique reads; use sequences as ids
fas <- paste(outdir_tmp, 'unique.fas', sep="")
# Open the file for writing
file <- file(fas, "w")
# Iterate over the sequences and write them to the file
for (i in seq_along(unique_asv_df$asv)) {
header <- paste0(">",unique_asv_df$asv[i], ";size=", unique_asv_df$read_count[i], sep="")
writeLines(c(header, unique_asv_df$asv[i], ""), file)
}
close(file)
close(file)
# vsearch --usearch_global to find highly similar sequence pairs
vsearch_out <- paste(outdir_tmp, 'unique_vsearch_out.out', sep="")
#  'vsearch --uchime3_denovo '. $sfas.' --uchimeout '.$chim_out.' --quiet';
vsearch <- paste(vsearch_path, "vsearch --uchime3_denovo ", fas, " --quiet --abskew ", abskew ," --uchimeout  ", vsearch_out, sep="")
vsearch
#  'vsearch --uchime3_denovo '. $sfas.' --uchimeout '.$chim_out.' --quiet';
vsearch <- paste(vsearch_path, "vsearch --uchime2_denovo ", fas, " --quiet --abskew ", abskew ," --uchimeout  ", vsearch_out, sep="")
vsearch
setwd("~/vtamR")
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
####
# define input filenames
fastqdir <- "local/small_test"
fileinfo <- "local/user_input/fileinfo_small.csv"
# create the output directory and check the the slash at the end
outdir <- check_dir(dir="local/out")
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
# read input fasta files in fileinfo, demultiplex and count the number of reads in each plate-sample-replicate
read_count_df <- read_fastas_from_fileinfo(file=fileinfo, dir=fastqdir, write_csv=F, outdir=outdir)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
temp_df <- read_count_df
# get unique list of ASVs with their total read_count in the run
unique_asv_df <- read_count_df %>%
group_by(asv) %>%
summarize(read_count = sum(read_count)) %>%
arrange(desc(read_count))
vsearch_path = ""
abskew=2
# no ASV in the unique_asv_df => return a dataframe with 0 for all ASVs in Chimera column
if(length(unique_asv_df$asv) == 0){
unique_asv_df$chimera <- rep(0, length(unique_asv_df$asv))
return(unique_asv_df)
}
# create a tmp directory for temporary files using time and a random number
outdir_tmp <- paste(outdir, 'tmp_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
outdir_tmp <- check_dir(outdir_tmp)
# make fasta file with unique reads; use sequences as ids
fas <- paste(outdir_tmp, 'unique.fas', sep="")
# Open the file for writing
file <- file(fas, "w")
# Iterate over the sequences and write them to the file
for (i in seq_along(unique_asv_df$asv)) {
header <- paste0(">",unique_asv_df$asv[i], ";size=", unique_asv_df$read_count[i], sep="")
writeLines(c(header, unique_asv_df$asv[i], ""), file)
}
close(file)
# vsearch --usearch_global to find highly similar sequence pairs
vsearch_out <- paste(outdir_tmp, 'unique_vsearch_out.out', sep="")
#  'vsearch --uchime3_denovo '. $sfas.' --uchimeout '.$chim_out.' --quiet';
vsearch <- paste(vsearch_path, "vsearch --uchime2_denovo ", fas, " --quiet --abskew ", abskew ," --uchimeout  ", vsearch_out, sep="")
vsearch
#  'vsearch --uchime3_denovo '. $sfas.' --uchimeout '.$chim_out.' --quiet';
vsearch <- paste(vsearch_path, "vsearch --uchime2_denovo ", fas, " --quiet --abskew ", abskew ," --uchimeout  ", vsearch_out, sep="")
#https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/system
system(vsearch)
#https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/system
system(vsearch)
# no vsearch hit => return unique_asv_df completed with a PCRerror, with 0 for all ASVs
if(file.size(vsearch_out) == 0){
unique_asv_df$chimera <- rep(0, length(unique_asv_df$asv))
# Delete the temp directory
unlink(outdir_tmp, recursive = TRUE)
return(unique_asv_df)
}
# read vsearch results
results_vsearch<- read.csv(vsearch_out, header = FALSE, sep="\t")
View(results_vsearch)
# delete unnecessary columns
results_vsearch <- select(results_vsearch, c(2, -1))
# read vsearch results
results_vsearch<- read.csv(vsearch_out, header = FALSE, sep="\t")
# delete unnecessary columns
results_vsearch <- select(results_vsearch, c(2, length(results_vsearch[,1])))
# read vsearch results
results_vsearch<- read.csv(vsearch_out, header = FALSE, sep="\t")
# delete unnecessary columns
results_vsearch <- select(results_vsearch, c(2, ncol(results_vsearch)))
# keep only pairs with max_mismatch differences
colnames(results_vsearch) <- c("asv", "chimera")
results_vsearch <- results_vsearch %>%
filter(chimera == "Y")
results_vsearch$asv <- gsub("size=\d+", "", results_vsearch$asv)
results_vsearch$asv <- gsub("size=", "", results_vsearch$asv)
# read vsearch results
results_vsearch<- read.csv(vsearch_out, header = FALSE, sep="\t")
# delete unnecessary columns
results_vsearch <- select(results_vsearch, c(2, ncol(results_vsearch)))
# keep only pairs with max_mismatch differences
colnames(results_vsearch) <- c("asv", "chimera")
results_vsearch$asv <- gsub(";size=", "", results_vsearch$asv)
# read vsearch results
results_vsearch<- read.csv(vsearch_out, header = FALSE, sep="\t")
# delete unnecessary columns
results_vsearch <- select(results_vsearch, c(2, ncol(results_vsearch)))
# keep only pairs with max_mismatch differences
colnames(results_vsearch) <- c("asv", "chimera")
results_vsearch$asv <- gsub(";size=[0-9]+", "", results_vsearch$asv)
results_vsearch <- results_vsearch %>%
filter(chimera == "Y")
unique_asv_df$chimera <- rep(0, length(unique_asv_df$asv))
unique_asv_df$chimera[unique_asv_df$asv %in% results_vsearch$asv] <- 1
