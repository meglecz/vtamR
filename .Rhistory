setwd(backup_wd)
setwd(outdir)
zip(input_fasta, unzipped_file) # zip outfile
file.remove(unzipped_file) # rm unzipped outfile
setwd(backup_wd)
}else{
input_fasta_p <- paste(fasta_dir, input_fasta, sep="")
output_fasta_p <- paste(outdir, input_fasta, sep="")
seq_n <- count_seq_bis(filename=input_fasta_p)
if(n > seq_n ){
file.copy(input_fasta_p, output_fasta_p)
msg <- paste("WARNING:", input_fasta_p,"has",seq_n,"sequences, which is lower than", n,". The file is copied to the",outdir,"directory without subsampling", sep=" ")
print(msg)
}else{
options(scipen=100)
vsearch_cmd <- paste("vsearch --fastx_subsample ", input_fasta_p, " --fastaout ", output_fasta_p, " --sample_size ", n, " --randseed ", randseed, sep="")
print(vsearch_cmd)
system(vsearch_cmd)
options(scipen=0)
}
}
}
}
RandomSeq_bis(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, n=1000000, randseed=0)
count_seq_bis <- function(filename){
# work for uncompressed and gz files
if(endsWith(filename, ".zip")){
file_connection <- gzfile(filename, "rb")
}else{
file_connection <- file(filename, "r")
}
data <- as.data.frame(readLines(file_connection, n = -1))
close(file_connection)
colnames(data) <- c("read")
data <- data %>%
filter(startsWith(read, ">"))
count = nrow(data)
rm(data)
return(count)
}
n <- count_seq_bis(filename="/home/meglecz/vtamR_large_files/out/random_seq/MFZR1_S4_L001_R1_001.zip")
n <- count_seq_bis(filename="/home/meglecz/vtamR_large_files/out/random_seq/MFZR1_S4_L001_R1_001.zip")
count_seq_bis <- function(filename){
# work for uncompressed and gz files
if(endsWith(filename, ".zip")){
unzip(filename)
filename <- gsub(".zip", ".fasta")
file_connection <- file(filename, "r")
}else if(endsWith(filename, "gz")){
file_connection <- gzfile(filename, "rb")
}else{
file_connection <- file(filename, "r")
}
data <- as.data.frame(readLines(file_connection, n = -1))
close(file_connection)
colnames(data) <- c("read")
data <- data %>%
filter(startsWith(read, ">"))
count = nrow(data)
rm(data)
return(count)
}
n <- count_seq_bis(filename="/home/meglecz/vtamR_large_files/out/random_seq/MFZR1_S4_L001_R1_001.zip")
count_seq_bis <- function(filename){
# work for uncompressed and gz files
if(endsWith(filename, ".zip")){
unzip(filename)
filename <- gsub(".zip", ".fasta", filename)
file_connection <- file(filename, "r")
}else if(endsWith(filename, "gz")){
file_connection <- gzfile(filename, "rb")
}else{
file_connection <- file(filename, "r")
}
data <- as.data.frame(readLines(file_connection, n = -1))
close(file_connection)
colnames(data) <- c("read")
data <- data %>%
filter(startsWith(read, ">"))
count = nrow(data)
rm(data)
return(count)
}
n <- count_seq_bis(filename="/home/meglecz/vtamR_large_files/out/random_seq/MFZR1_S4_L001_R1_001.zip")
count_seq_bis <- function(dir="", filename=filename){
dir <- check_dir(dir)
filename_p <- paste(dir, filename, sep="")
# work for uncompressed and gz files
if(endsWith(filename, ".zip")){
backup_wd <- getwd()
setwd(fasta_dir)
unzip(filename)
setwd(backup_wd) # get back to the original wd, so input and output filepath can be handled more easily
filename_unzipped <- gsub(".zip", ".fasta", filename)
filename_p <- paste(dir, filename_unzipped, sep="")
file_connection <- file(filename_p, "r")
}else if(endsWith(filename_p, "gz")){
file_connection <- gzfile(filename_p, "rb")
}else{
file_connection <- file(filename-p, "r")
}
data <- as.data.frame(readLines(file_connection, n = -1))
close(file_connection)
if(endsWith(filename, ".zip")){
file.remove(filename_p) # rm unzipped
}
colnames(data) <- c("read")
data <- data %>%
filter(startsWith(read, ">"))
count = nrow(data)
rm(data)
return(count)
}
n <- count_seq_bis(dir="/home/meglecz/vtamR_large_files/out/random_seq/", filename="MFZR1_S4_L001_R1_001.zip")
n <- count_seq_bis(fasta_dir="/home/meglecz/vtamR_large_files/out/random_seq/", filename="MFZR1_S4_L001_R1_001.zip")
count_seq_bis <- function(fasta_dir="", filename=filename){
dir <- check_dir(dir)
filename_p <- paste(dir, filename, sep="")
# work for uncompressed and gz files
if(endsWith(filename, ".zip")){
backup_wd <- getwd()
setwd(fasta_dir)
unzip(filename)
setwd(backup_wd) # get back to the original wd, so input and output filepath can be handled more easily
filename_unzipped <- gsub(".zip", ".fasta", filename)
filename_p <- paste(dir, filename_unzipped, sep="")
file_connection <- file(filename_p, "r")
}else if(endsWith(filename_p, "gz")){
file_connection <- gzfile(filename_p, "rb")
}else{
file_connection <- file(filename-p, "r")
}
data <- as.data.frame(readLines(file_connection, n = -1))
close(file_connection)
if(endsWith(filename, ".zip")){
file.remove(filename_p) # rm unzipped
}
colnames(data) <- c("read")
data <- data %>%
filter(startsWith(read, ">"))
count = nrow(data)
rm(data)
return(count)
}
n <- count_seq_bis(fasta_dir="/home/meglecz/vtamR_large_files/out/random_seq/", filename="MFZR1_S4_L001_R1_001.zip")
n <- count_seq_bis(filename="/home/meglecz/vtamR_large_files/out/sorted/run1-MFZR-14Ben01-1.fasta")
count_seq_bis <- function(filename=filename){
# work for uncompressed and gz files
if(endsWith(filename, "gz")){
file_connection <- gzfile(filename, "rb")
}else{
file_connection <- file(filename, "r")
}
data <- as.data.frame(readLines(file_connection, n = -1))
close(file_connection)
if(endsWith(filename, ".zip")){
file.remove(filename_p) # rm unzipped
}
colnames(data) <- c("read")
data <- data %>%
filter(startsWith(read, ">"))
count = nrow(data)
rm(data)
return(count)
}
n <- count_seq_bis(filename="/home/meglecz/vtamR_large_files/out/sorted/run1-MFZR-14Ben01-1.fasta")
n
n <- count_seq_bis(filename="/home/meglecz/vtamR_large_files/out/sorted/run1-MFZR-14Ben01-1.fasta.gz")
n
RandomSeq_bis <- function(fastainfo_df, fasta_dir="", outdir="", n, randseed=0){
# zipping and unzipping is quite long. It is probably better to work on uncompressed files on windows
# quite fast for uncompressed ad gz files
fasta_dir<- check_dir(fasta_dir)
outdir<- check_dir(outdir)
unique_fasta <- unique(fastainfo_df$fasta)
for(i in 1:length(unique_fasta)){ # go through all fasta files
input_fasta <- unique_fasta[i]
# Zip files should be unzipped once for sequence count and select_seq, than results re-zipped, wd is changed several times
if(endsWith(input_fasta, ".zip")){
# change to fasta_dir
backup_wd <- getwd()
setwd(fasta_dir)
unzip(input_fasta)
setwd(backup_wd) # get back to the original wd, so input and output filepath can be handled more easily
unzipped_file <- sub(".zip", ".fasta", input_fasta)
input_fasta_p <- paste(fasta_dir, unzipped_file, sep="")
output_fasta_p <- paste(outdir, unzipped_file, sep="")
# count the number of sequences in the input file
seq_n <- count_seq_bis(filename=input_fasta_p)
#random sample with vsearch
options(scipen=100)
vsearch_cmd <- paste("vsearch --fastx_subsample ", input_fasta_p, " --fastaout ", output_fasta_p, " --sample_size ", n, " --randseed ", randseed, sep="")
print(vsearch_cmd)
system(vsearch_cmd)
options(scipen=0)
# delete unzipped input file
setwd(fasta_dir)
file.remove(unzipped_file)  # remove unzipped input file
# zip output file
setwd(backup_wd)
setwd(outdir)
zip(input_fasta, unzipped_file) # zip outfile
file.remove(unzipped_file) # rm unzipped outfile
setwd(backup_wd)
}else{
input_fasta_p <- paste(fasta_dir, input_fasta, sep="")
output_fasta_p <- paste(outdir, input_fasta, sep="")
seq_n <- count_seq_bis(filename=input_fasta_p)
if(n > seq_n ){
file.copy(input_fasta_p, output_fasta_p)
msg <- paste("WARNING:", input_fasta_p,"has",seq_n,"sequences, which is lower than", n,". The file is copied to the",outdir,"directory without subsampling", sep=" ")
print(msg)
}else{
options(scipen=100)
vsearch_cmd <- paste("vsearch --fastx_subsample ", input_fasta_p, " --fastaout ", output_fasta_p, " --sample_size ", n, " --randseed ", randseed, sep="")
print(vsearch_cmd)
system(vsearch_cmd)
options(scipen=0)
}
}
}
}
RandomSeq_bis(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, n=1000000, randseed=0)
fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
RandomSeq_bis(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, n=1000000, randseed=0)
RandomSeq_bis <- function(fastainfo_df, fasta_dir="", outdir="", n, randseed=0){
# zipping and unzipping is quite long. It is probably better to work on uncompressed files on windows
# quite fast for uncompressed ad gz files
fasta_dir<- check_dir(fasta_dir)
outdir<- check_dir(outdir)
unique_fasta <- unique(fastainfo_df$fasta)
for(i in 1:length(unique_fasta)){ # go through all fasta files
input_fasta <- unique_fasta[i]
# Zip files should be unzipped once for sequence count and select_seq, than results re-zipped, wd is changed several times
if(endsWith(input_fasta, ".zip")){
# change to fasta_dir
backup_wd <- getwd()
setwd(fasta_dir)
unzip(input_fasta)
setwd(backup_wd) # get back to the original wd, so input and output filepath can be handled more easily
unzipped_file <- sub(".zip", ".fasta", input_fasta)
input_fasta_p <- paste(fasta_dir, unzipped_file, sep="")
output_fasta_p <- paste(outdir, unzipped_file, sep="")
# count the number of sequences in the input file
seq_n <- count_seq_bis(filename=input_fasta_p)
#random sample with vsearch
options(scipen=100)
vsearch_cmd <- paste("vsearch --fastx_subsample ", input_fasta_p, " --fastaout ", output_fasta_p, " --sample_size ", n, " --randseed ", randseed, sep="")
print(vsearch_cmd)
system(vsearch_cmd)
options(scipen=0)
# delete unzipped input file
setwd(fasta_dir)
file.remove(unzipped_file)  # remove unzipped input file
# zip output file
setwd(backup_wd)
setwd(outdir)
zip(input_fasta, unzipped_file) # zip outfile
file.remove(unzipped_file) # rm unzipped outfile
setwd(backup_wd)
}else{
input_fasta_p <- paste(fasta_dir, input_fasta, sep="")
output_fasta_p <- paste(outdir, input_fasta, sep="")
seq_n <- count_seq_bis(filename=input_fasta_p)
if(n > seq_n ){
file.copy(input_fasta_p, output_fasta_p)
msg <- paste("WARNING:", input_fasta_p,"has",seq_n,"sequences, which is lower than", n,". The file is copied to the",outdir,"directory without subsampling", sep=" ")
print(msg)
}else{
options(scipen=100)
output_fasta_p <- gsub(".gz", "", output_fasta_p)
vsearch_cmd <- paste("vsearch --fastx_subsample ", input_fasta_p, " --fastaout ", output_fasta_p, " --sample_size ", n, " --randseed ", randseed, sep="")
print(vsearch_cmd)
system(vsearch_cmd)
options(scipen=0)
# compress the output file if the input was gz
if(endsWith(input_fasta_p, ".gz")){
# Specify the path for the gzipped output file
outfile_gz <- paste(output_fasta_p, ".gz", sep="")
# Open the existing uncompressed file for reading
file_content <- readBin(output_fasta_p, "raw", file.info(output_fasta_p)$size)
# Create a gzipped copy of the file
gz <- gzfile(outfile_gz, "wb")
writeBin(file_content, gz)
close(gz)
file.remove(output_fasta_p)
}
}
}
}
}
fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
RandomSeq_bis(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, n=1000000, randseed=0)
computer <- "Bombyx" # Bombyx/Endoume/Windows
if(computer == "Bombyx"){
setwd("~/vtamR")
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/"
fastqdir <- "local/fastq/"
fastqdir <- "/home/meglecz/vtamR_large_files/fastq/"
fastqinfo <- "/home/meglecz/vtamR_large_files/user_input/fastqinfo_mfzr.csv"
outdir <- "/home/meglecz/vtamR_large_files/out/"
mock_composition <- "local/user_input/mock_composition_mfzr_eu.csv"
num_threads=8
} else if (computer == "Endoume"){
setwd("~/vtamR")
cutadapt_path="/home/emese/miniconda3/bin/"
vsearch_path = "/home/emese/miniconda3/bin/"
blast_path= "" # deactivate conda
db_path= "/home/emese/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/"
fastqdir <- "local/fastq/"
num_threads=8
}else if (computer == "Windows"){
setwd("C:/Users/emese/vtamR/")
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
db_path="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/"
fastqdir <- "C:/Users/emese/vtamR_private/fastq/"
num_threads=4
}
sep=";"
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
# create the output directory and check the the slash at the end
outdir <- check_dir(dir=outdir)
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
###
### Merge
###
fastq_ascii <- 33
fastq_maxdiffs <- 10
fastq_maxee <- 1
fastq_minlen <- 50
fastq_maxlen <- 500
fastq_minmergelen <- 50
fastq_maxmergelen <-500
fastq_maxns <- 0
fastq_truncqual <- 10
fastq_minovlen <- 50
fastq_allowmergestagger <- F
compress="gz" # "gz" or "zip" for compressing output files; no comprssion by default
merged_dir <- paste(outdir, "merged/", sep="")
# read fastqinfo
fastqinfo_df <- read.csv(fastqinfo, header=T, sep=sep)
fastainfo_df <- Merge(fastqinfo_df=fastqinfo_df, fastqdir=fastqdir, vsearch_path=vsearch_path, outdir=merged_dir, fastq_ascii=fastq_ascii, fastq_maxdiffs=fastq_maxdiffs, fastq_maxee=fastq_maxee, fastq_minlen=fastq_minlen, fastq_maxlen=fastq_maxlen, fastq_minmergelen=fastq_minmergelen, fastq_maxmergelen=fastq_maxmergelen, fastq_maxns=fastq_maxns, fastq_truncqual=fastq_truncqual, fastq_minovlen=fastq_minovlen, fastq_allowmergestagger=fastq_allowmergestagger, sep=sep, compress=compress)
###
### RandomSeq
###
randomseq_dir = paste(outdir, "random_seq/", sep="")
#fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
#fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, n=1000000, randseed=0)
###
### SortReads
###
sorted_dir <- paste(outdir, "sorted", sep="")
check_reverse <- T
tag_to_end <- F
primer_to_end <-F
cutadapt_error_rate <- 0.1 # -e in cutadapt
cutadapt_minimum_length <- 50 # -m in cutadapt
cutadapt_maximum_length <- 500 # -M in cutadapt
compress <- "gz"
fileinfo_df <- SortReads(fastainfo_df=fastainfo_df, fastadir=randomseq_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, check_reverse=check_reverse, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
###
### Read input fasta files, dereplicate reads to ASV, and count the number of reads of each ASV in each plate-marker-sample-replicate
###
# read fileinfo file to fileinfo_df if starting directly with demultiplexed, trimmed reads
# fileinfo_df <- read.csv(file, header=T, sep=sep)
read_count_df <- read_fastas_from_fileinfo(fileinfo_df, dir=sorted_dir, write_csv=F, outdir=outdir, sep=sep)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
View(stat_df)
###
### LFN_global_read_count
###
# Eliminate variants with less than global_read_count_cutoff reads in the dataset
global_read_count_cutoff = 2
read_count_df <- LFN_global_read_count(read_count_df, global_read_count_cutoff, write_csv=T, outdir=outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
###
### LFN_filters
###
# LFN_read_count
lfn_read_count_cutoff <- 10
read_count_df_lfn_read_count <- LFN_read_count(read_count_df, cutoff=lfn_read_count_cutoff, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df_lfn_read_count, stat_df, stage="LFN_read_count", params=lfn_read_count_cutoff)
# LFN_sample_replicate (by column)
lfn_sample_replicate_cutoff <- 0.001
read_count_df_lnf_sample_replicate <- LFN_sample_replicate(read_count_df, cutoff=lfn_sample_replicate_cutoff, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df_lnf_sample_replicate, stat_df, stage="LFN_sample_replicate", params=lfn_sample_replicate_cutoff)
# LFN_sample_variant (by line)
lnf_variant_cutoff = 0.001
by_replicate = TRUE
read_count_df_lnf_variant <- LFN_variant(read_count_df, cutoff=lnf_variant_cutoff, by_replicate, write_csv=T, outdir = outdir, sep=sep)
param_values <- paste(lnf_variant_cutoff, by_replicate, sep=";")
stat_df <- get_stat(read_count_df_lnf_variant, stat_df, stage="LFN_variant", params=param_values)
# pool the results of the different filterLFN to one data frame; keep only occurrences that passed all filters
read_count_df <- pool_LFN(read_count_df_lfn_read_count, read_count_df_lnf_variant, read_count_df_lnf_sample_replicate, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
read_count_df_lfn_read_count <- NULL
read_count_df_lnf_variant <- NULL
read_count_df_lnf_sample_replicate <- NULL
###
### keep repeatable occurrences
###
min_replicate_number <- 2
read_count_df <- FilterMinReplicateNumber(read_count_df, min_replicate_number, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterMinReplicateNumber", params=min_replicate_number)
###
### FilerPCRerror
###
pcr_error_var_prop <- 0.1
max_mismatch <- 1
by_sample <- T
sample_prop <- 0.8
read_count_df <- FilterPCRerror(read_count_df, write_csv=T, outdir=outdir, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch, by_sample=by_sample, sample_prop=sample_prop, sep=sep)
params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilerPCRerror", params=params)
###
### FilterChimera
###
vsearch_path = ""
abskew=2
by_sample = T
sample_prop = 0.8
read_count_df <- FilterChimera(read_count_df, write_csv=T, outdir=outdir, vsearch_path=vsearch_path, by_sample=by_sample, sample_prop=sample_prop, abskew=abskew, sep=sep)
params <- paste(abskew, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilterChimera", params=params)
###
### FilerRenkonen
###
# Renkonen index:
# PS = summ(min(p1i, p2i))
# p1i = number of reads for variant i in replicate 1 / number of reads in replicate 1
renkonen_distance_quantile = 0.9
read_count_df <- FilerRenkonen(read_count_df, write_csv=T, outdir=outdir, renkonen_distance_quantile=renkonen_distance_quantile, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerRenkonen", params=renkonen_distance_quantile)
###
### FilerIndel
###
read_count_df <- FilterIndel(read_count_df, write_csv=T, outdir=outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterIndel")
###
### FilerCodonStop
###
genetic_code = 5
read_count_df <- FilterCodonStop(read_count_df, write_csv=T, outdir=outdir, genetic_code=genetic_code, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
###
### PoolReplicates
###
digits = 0
read_count_samples_df <- PoolReplicates(read_count_df, digits=digits, write_csv=T, outdir=outdir, sep=sep)
###
### TaxAssign
###
asv_tax <- TaxAssign(df=read_count_samples_df, ltg_params_df=ltg_params_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outdir=outdir, num_threads=num_threads)
# write the list of ASV and their taxonomic assignment
write.csv(asv_tax, file = paste(outdir, "taxa.csv", sep=""), row.names = F)
###
### print output files
###
# write sequence and variant counts after each step
write.csv(stat_df, file = paste(outdir, "count_stat.csv", sep=""))
# long format, each line corresponds to an occurrence ()
write.csv(read_count_samples_df, file = paste(outdir, "Final_asvtable_long.csv", sep=""))
# wide format (ASV table), samples are in columns, ASVs in lines
outfile=paste(outdir, "Final_asvtable.csv", sep="")
write_asvtable(read_count_samples_df, outfile=outfile, fileinfo=fileinfo, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition, sep=sep)
# write ASV table completed by taxonomic assignments
outfile=paste(outdir, "Final_asvtable_with_taxassign.csv", sep="")
write_asvtable(read_count_samples_df, outfile=outfile, asv_tax=asv_tax, fileinfo=fileinfo, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition, sep=sep)
