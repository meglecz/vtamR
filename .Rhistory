# the original file has been decompressed  => remove the decompressed file
if(input_fasta_p != original_input_fasta_p){
file.remove(input_fasta_p)
}
# the outfile is not yet compressed
if(compress && !endsWith(output_fasta_p, ".gz")){
output_fasta_p <- compress_file(filename=output_fasta_p, remove_input=T)
}
fastainfo_df$new_file[which(fastainfo_df$fasta==input_fasta)] <- output_fasta
fastainfo_df$seq_count[which(fastainfo_df$fasta==input_fasta)] <- seq_n
next()
}
# enough seq => resample
options(scipen=100) # do not transform large numbers to scentific forms, since it would lead to an error in vsearch
output_fasta_p <- gsub(".gz", "", output_fasta_p) # vsearch makes decompressed files
vsearch_cmd <- paste(vsearch_path, "vsearch --fastx_subsample ", input_fasta_p, " --fastaout ", output_fasta_p, " --sample_size ", n, " --randseed ", randseed, sep="")
print(vsearch_cmd)
system(vsearch_cmd)
options(scipen=0)
if(compress){ # compress the output file
output_fasta_p <- compress_file(filename=output_fasta_p, remove_input=T)
}
if(!is_linux() && endsWith(original_input_fasta_p, ".gz")){ # delete decompressed input files
file.remove(input_fasta_p)
}
fastainfo_df$new_file[which(fastainfo_df$fasta==input_fasta)] <- output_fasta
fastainfo_df$seq_count[which(fastainfo_df$fasta==input_fasta)] <- n
new_fastainfo <- paste(outdir, "fastainfo.csv", sep="")
write.table(fastainfo_df, file = new_fastainfo,  row.names = F, sep=sep)
}# all files
}
###
### RandomSeq
###
randomseq_dir = paste(outdir, "random_seq/", sep="")
#fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
#fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
compress = T
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=10000, randseed=0, compress=compress)
colnames(fastainfo_df)
#'
#' @param fastainfo_df data frame with a 'fasta' column containing input file names; files can be compressed in gz and zip format
#' @param n integer; the number of randomly selected sequences
#' @param fasta_dir directory that contains the input fasta files
#' @param outdir directory for the output files
#' @param vsearch_path path to vsearch
#' @param randseed positive integer; seed for random sampling; 0 by default means to use a pseudo-random seed, a given non-zero seed produce always the same result
#' @param compress [T/F]; If TRUE, output files are compressed in the same format as the input. Otherwise the output is uncompressed;
#' @export
#'
RandomSeq <- function(fastainfo_df, fasta_dir="", outdir="", vsearch_path="", n, randseed=0, compress=F){
# quite fast for uncompressed and gz files
fasta_dir<- check_dir(fasta_dir)
vsearch_path<- check_dir(vsearch_path)
outdir<- check_dir(outdir)
fastainfo_df$new_file <- NA
fastainfo_df$read_count <- NA
unique_fasta <- unique(fastainfo_df$fasta)
for(i in 1:length(unique_fasta)){ # go through all fasta files
input_fasta <- unique_fasta[i]
# adjust output filename in function of the compression
output_fasta <- input_fasta
if(compress && !endsWith(output_fasta, ".gz")){
output_fasta <- paste(output_fasta, ".gz", sep="")
}
if(!compress && endsWith(output_fasta, ".gz")){
output_fasta <- sub(".gz$", "", output_fasta)
}
if(endsWith(input_fasta, ".zip")){
stop("Zip files are not supported")
}
original_input_fasta_p <- paste(fasta_dir, input_fasta, sep="")
input_fasta_p <- paste(fasta_dir, input_fasta, sep="") # name can change if file de/recompressed
if(!is_linux() && endsWith(original_input_fasta_p, ".gz")){ #Decompress input files, since they are cannot be treated directly by vsearch on the OS
input_fasta_p <- decompress_file(input_fasta_p, remove_input=F)
}
output_fasta_p <- paste(outdir, input_fasta, sep="")
seq_n <- count_seq(file=input_fasta_p)
if(n > seq_n ){ # not enough seq
file.copy(original_input_fasta_p, output_fasta_p)
msg <- paste("WARNING:", input_fasta_p,"has",seq_n,"sequences, which is lower than", n,". The file is copied to the",outdir,"directory without subsampling", sep=" ")
print(msg)
# the original file has been decompressed  => remove the decompressed file
if(input_fasta_p != original_input_fasta_p){
file.remove(input_fasta_p)
}
# the outfile is not yet compressed
if(compress && !endsWith(output_fasta_p, ".gz")){
output_fasta_p <- compress_file(filename=output_fasta_p, remove_input=T)
}
fastainfo_df$new_file[which(fastainfo_df$fasta==input_fasta)] <- output_fasta
fastainfo_df$read_count[which(fastainfo_df$fasta==input_fasta)] <- seq_n
next()
}
# enough seq => resample
options(scipen=100) # do not transform large numbers to scentific forms, since it would lead to an error in vsearch
output_fasta_p <- gsub(".gz", "", output_fasta_p) # vsearch makes decompressed files
vsearch_cmd <- paste(vsearch_path, "vsearch --fastx_subsample ", input_fasta_p, " --fastaout ", output_fasta_p, " --sample_size ", n, " --randseed ", randseed, sep="")
print(vsearch_cmd)
system(vsearch_cmd)
options(scipen=0)
if(compress){ # compress the output file
output_fasta_p <- compress_file(filename=output_fasta_p, remove_input=T)
}
if(!is_linux() && endsWith(original_input_fasta_p, ".gz")){ # delete decompressed input files
file.remove(input_fasta_p)
}
fastainfo_df$new_file[which(fastainfo_df$fasta==input_fasta)] <- output_fasta
fastainfo_df$read_count[which(fastainfo_df$fasta==input_fasta)] <- n
}# all files
fastainfo_df <- fastainfo_df %>%
select(tag_fw,primer_fw,tag_rv,primer_rv,sample,sample_type,habitat,replicate, fasta=new_file, read_count)
new_fastainfo <- paste(outdir, "fastainfo.csv", sep="")
write.table(fastainfo_df, file = new_fastainfo,  row.names = F, sep=sep)
}
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=10000, randseed=0, compress=compress)
#fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
#fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
compress = F
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=10000, randseed=0, compress=compress)
#fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
#fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
compress = F
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=1000000, randseed=0, compress=compress)
#fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
#fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
compress = F
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=40000, randseed=0, compress=compress)
###
### RandomSeq
###
randomseq_dir = paste(outdir, "random_seq/", sep="")
#fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
#fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
compress = T
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=10000, randseed=0, compress=compress)
###
### SortReads
###
sorted_dir <- paste(outdir, "sorted/", sep="")
check_reverse <- T
tag_to_end <- F
primer_to_end <-F
cutadapt_error_rate <- 0.1 # -e in cutadapt
cutadapt_minimum_length <- 50 # -m in cutadapt
cutadapt_maximum_length <- 500 # -M in cutadapt
compress <- F
sortedinfo_df <- SortReads(fastainfo_df=fastainfo_df, fastadir=randomseq_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, check_reverse=check_reverse, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
get_read_counts <- function(df, dir, motif=".fasta"){
df$read_count <- NA
for(file in df$filename){
file_path <- paste(outdir, file, sep="")
read_n <- count_seq(file_path)
df$read_count[which(df$filename==file)] <- read_n
}
return(df)
}
get_read_counts <- function(df, dir){
df$read_count <- NA
for(file in df$filename){
file_path <- paste(outdir, file, sep="")
read_n <- count_seq(file_path)
df$read_count[which(df$filename==file)] <- read_n
}
return(df)
}
sortedinfo_df <- get_read_counts(sortedinfo_df, dir)
get_read_counts <- function(df, dir){
df$read_count <- NA
for(file in df$filename){
file_path <- paste(outdir, file, sep="")
read_n <- count_seq(file_path)
df$read_count[which(df$filename==file)] <- read_n
}
return(df)
}
SortReads <- function(fastainfo_df, fastadir, outdir="", cutadapt_path="" ,vsearch_path="", check_reverse=F, tag_to_end=T, primer_to_end=T, cutadapt_error_rate=0.1,cutadapt_minimum_length=50,cutadapt_maximum_length=500, sep=",",  compress=F){
#########
# SortReads_no_reverse does the whole demultilexing, trimming and compress on the + strand
# If sequences are not oriented, the -strand should be checked =>
# run SortReads_no_reverse of plus strand and on - strand after swapping fw and rev tags and primers,
# take the reverse complement of the -strand results (vsearch)
# pool the results of the 2 strands
# compress if necessary
# run on strand +
if(check_reverse){
#### use +strand, output to sorted_dir, uncompressed
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df, fastadir=fastadir, outdir=outdir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=F)
#### use - strand
# swap fw and rv tags and primers
fastainfo_df_tmp <- fastainfo_df %>%
select(tag_fw_tmp = tag_rv, tag_rv_tmp = tag_fw, primer_fw_tmp = primer_rv, primer_rv_tmp = primer_fw, sample, sample_type,habitat, replicate, fasta) %>%
select(tag_fw = tag_fw_tmp, tag_rv = tag_rv_tmp, primer_fw = primer_fw_tmp, primer_rv = primer_rv_tmp, sample, sample_type,habitat, replicate, fasta)
# make temp dir
outdir <- check_dir(outdir)
rc_dir <-paste(outdir, 'rc_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
rc_dir <- check_dir(rc_dir)
# run sortreads on for reverse strand
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df_tmp, fastadir=fastadir, outdir=rc_dir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=F)
### reverse complement and pool
# get list of files demultiplexed on - strand
files <- list.files(path = rc_dir, pattern=".fasta")
# Filter the files based on the motif using regular expressions
# reverse complement sequences on the minus stand, and append info to the plus strand output
files <- grep(pattern = "\\.fasta", x = files, value = TRUE)
for(i in 1:length(files)){
plus <- paste(outdir, files[i], sep="")
minus <- paste(rc_dir, files[i], sep="")
minus_rc <- paste(rc_dir, "rc_", files[i], sep="")
# reverse complement sequences in minus file
rev_comp_cmd <- paste(vsearch_path, "vsearch --fastx_revcomp ", minus, " --fastaout ", minus_rc, " --quiet", sep="")
print(rev_comp_cmd)
system(rev_comp_cmd)
# append content of minus_rc to plus file
file.append(plus, minus_rc)
}
# delete temporary reverse_comp dir
unlink(rc_dir, recursive = TRUE)
### compress
if(compress){
for(i in 1:nrow(sortedinfo_df)){
file <- sortedinfo_df$filename[i]
sortedinfo_df$filename[i] <- paste(file, ".gz", sep="") # correct output filename
file <- paste(outdir, file, sep="") # add path
file_gz <- compress_file(file, remove_input=T) # compress file
}
write.table(sortedinfo_df, file = paste(outdir, "sortedinfo.csv", sep=""),  row.names = F, sep=sep)
}
}
else{
# check only + strand
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df, fastadir=fastadir, outdir=outdir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
}
sortedinfo_df <- get_read_counts(sortedinfo_df, dir)
return(sortedinfo_df)
}
sortedinfo_df <- SortReads(fastainfo_df=fastainfo_df, fastadir=randomseq_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, check_reverse=check_reverse, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
SortReads <- function(fastainfo_df, fastadir, outdir="", cutadapt_path="" ,vsearch_path="", check_reverse=F, tag_to_end=T, primer_to_end=T, cutadapt_error_rate=0.1,cutadapt_minimum_length=50,cutadapt_maximum_length=500, sep=",",  compress=F){
#########
# SortReads_no_reverse does the whole demultilexing, trimming and compress on the + strand
# If sequences are not oriented, the -strand should be checked =>
# run SortReads_no_reverse of plus strand and on - strand after swapping fw and rev tags and primers,
# take the reverse complement of the -strand results (vsearch)
# pool the results of the 2 strands
# compress if necessary
# run on strand +
if(check_reverse){
#### use +strand, output to sorted_dir, uncompressed
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df, fastadir=fastadir, outdir=outdir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=F)
#### use - strand
# swap fw and rv tags and primers
fastainfo_df_tmp <- fastainfo_df %>%
select(tag_fw_tmp = tag_rv, tag_rv_tmp = tag_fw, primer_fw_tmp = primer_rv, primer_rv_tmp = primer_fw, sample, sample_type,habitat, replicate, fasta) %>%
select(tag_fw = tag_fw_tmp, tag_rv = tag_rv_tmp, primer_fw = primer_fw_tmp, primer_rv = primer_rv_tmp, sample, sample_type,habitat, replicate, fasta)
# make temp dir
outdir <- check_dir(outdir)
rc_dir <-paste(outdir, 'rc_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
rc_dir <- check_dir(rc_dir)
# run sortreads on for reverse strand
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df_tmp, fastadir=fastadir, outdir=rc_dir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=F)
### reverse complement and pool
# get list of files demultiplexed on - strand
files <- list.files(path = rc_dir, pattern=".fasta")
# Filter the files based on the motif using regular expressions
# reverse complement sequences on the minus stand, and append info to the plus strand output
files <- grep(pattern = "\\.fasta", x = files, value = TRUE)
for(i in 1:length(files)){
plus <- paste(outdir, files[i], sep="")
minus <- paste(rc_dir, files[i], sep="")
minus_rc <- paste(rc_dir, "rc_", files[i], sep="")
# reverse complement sequences in minus file
rev_comp_cmd <- paste(vsearch_path, "vsearch --fastx_revcomp ", minus, " --fastaout ", minus_rc, " --quiet", sep="")
print(rev_comp_cmd)
system(rev_comp_cmd)
# append content of minus_rc to plus file
file.append(plus, minus_rc)
}
# delete temporary reverse_comp dir
unlink(rc_dir, recursive = TRUE)
### compress
if(compress){
for(i in 1:nrow(sortedinfo_df)){
file <- sortedinfo_df$filename[i]
sortedinfo_df$filename[i] <- paste(file, ".gz", sep="") # correct output filename
file <- paste(outdir, file, sep="") # add path
file_gz <- compress_file(file, remove_input=T) # compress file
}
write.table(sortedinfo_df, file = paste(outdir, "sortedinfo.csv", sep=""),  row.names = F, sep=sep)
}
}
else{
# check only + strand
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df, fastadir=fastadir, outdir=outdir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
}
sortedinfo_df <- get_read_counts(sortedinfo_df, dir=outdir)
return(sortedinfo_df)
}
sortedinfo_df <- SortReads(fastainfo_df=fastainfo_df, fastadir=randomseq_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, check_reverse=check_reverse, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
get_read_counts <- function(df, dir){
df$read_count <- NA
for(file in df$filename){
file_path <- paste(dir, file, sep="")
read_n <- count_seq(file_path)
df$read_count[which(df$filename==file)] <- read_n
}
return(df)
}
SortReads <- function(fastainfo_df, fastadir, outdir="", cutadapt_path="" ,vsearch_path="", check_reverse=F, tag_to_end=T, primer_to_end=T, cutadapt_error_rate=0.1,cutadapt_minimum_length=50,cutadapt_maximum_length=500, sep=",",  compress=F){
#########
# SortReads_no_reverse does the whole demultilexing, trimming and compress on the + strand
# If sequences are not oriented, the -strand should be checked =>
# run SortReads_no_reverse of plus strand and on - strand after swapping fw and rev tags and primers,
# take the reverse complement of the -strand results (vsearch)
# pool the results of the 2 strands
# compress if necessary
# run on strand +
if(check_reverse){
#### use +strand, output to sorted_dir, uncompressed
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df, fastadir=fastadir, outdir=outdir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=F)
#### use - strand
# swap fw and rv tags and primers
fastainfo_df_tmp <- fastainfo_df %>%
select(tag_fw_tmp = tag_rv, tag_rv_tmp = tag_fw, primer_fw_tmp = primer_rv, primer_rv_tmp = primer_fw, sample, sample_type,habitat, replicate, fasta) %>%
select(tag_fw = tag_fw_tmp, tag_rv = tag_rv_tmp, primer_fw = primer_fw_tmp, primer_rv = primer_rv_tmp, sample, sample_type,habitat, replicate, fasta)
# make temp dir
outdir <- check_dir(outdir)
rc_dir <-paste(outdir, 'rc_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
rc_dir <- check_dir(rc_dir)
# run sortreads on for reverse strand
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df_tmp, fastadir=fastadir, outdir=rc_dir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=F)
### reverse complement and pool
# get list of files demultiplexed on - strand
files <- list.files(path = rc_dir, pattern=".fasta")
# Filter the files based on the motif using regular expressions
# reverse complement sequences on the minus stand, and append info to the plus strand output
files <- grep(pattern = "\\.fasta", x = files, value = TRUE)
for(i in 1:length(files)){
plus <- paste(outdir, files[i], sep="")
minus <- paste(rc_dir, files[i], sep="")
minus_rc <- paste(rc_dir, "rc_", files[i], sep="")
# reverse complement sequences in minus file
rev_comp_cmd <- paste(vsearch_path, "vsearch --fastx_revcomp ", minus, " --fastaout ", minus_rc, " --quiet", sep="")
print(rev_comp_cmd)
system(rev_comp_cmd)
# append content of minus_rc to plus file
file.append(plus, minus_rc)
}
# delete temporary reverse_comp dir
unlink(rc_dir, recursive = TRUE)
### compress
if(compress){
for(i in 1:nrow(sortedinfo_df)){
file <- sortedinfo_df$filename[i]
sortedinfo_df$filename[i] <- paste(file, ".gz", sep="") # correct output filename
file <- paste(outdir, file, sep="") # add path
file_gz <- compress_file(file, remove_input=T) # compress file
}
write.table(sortedinfo_df, file = paste(outdir, "sortedinfo.csv", sep=""),  row.names = F, sep=sep)
}
}
else{
# check only + strand
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df, fastadir=fastadir, outdir=outdir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
}
sortedinfo_df <- get_read_counts(sortedinfo_df, dir=outdir)
return(sortedinfo_df)
}
sortedinfo_df <- SortReads(fastainfo_df=fastainfo_df, fastadir=randomseq_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, check_reverse=check_reverse, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
#' @param cutadapt_path path to cutadapt executables
#' @param outdir output directory
#' @param tag_to_end tags are at the extremity of the reads (starting at the first base); default: T
#' @param primer_to_end primers follow directly the tags (no heterogeneity spacer); default: T
#' @param cutadapt_error_rate maximum proportion of errors between primers and reads (for tags, exact match is required); default: 0.1
#' @param cutadapt_minimum_length minimum length of the trimmed sequence; default: 50
#' @param cutadapt_maximum_length maximum length of the merged sequence; default: 500
#' @param sep separator in input and output csv files; default: ","
#' @param compress [T/F]; compress output to gzip files.
#' @export
SortReads_no_reverse <- function(fastainfo_df, fastadir, outdir="", cutadapt_path="", tag_to_end=T, primer_to_end=T, cutadapt_error_rate=0.1,cutadapt_minimum_length=50,cutadapt_maximum_length=500, sep=",",  compress=F){
# do the complete job of demultiplexing and trimming of input file without checking the reverse sequences
# upper case for all primers and tags
fastainfo_df$tag_fw <- toupper(fastainfo_df$tag_fw)
fastainfo_df$tag_rv <- toupper(fastainfo_df$tag_rv)
fastainfo_df$primer_fw <- toupper(fastainfo_df$primer_fw)
fastainfo_df$primer_rv <- toupper(fastainfo_df$primer_rv)
# make a column for output filenames
fastainfo_df$filename <- NA
# check dirs and make temp dir
outdir <- check_dir(outdir)
fastadir<- check_dir(fastadir)
# get unique list of input fasta files
fastas <- unique(fastainfo_df$fasta)
for(i in 1:length(fastas)){ # for each input fasta
# make temp dir for tag file and tag-trimmed files
tmp_dir <-paste(outdir, 'tmp_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
tmp_dir <- check_dir(tmp_dir)
# select lines in fastainfo_df that corresponds to a given input fasta file
fasta_file <- fastas[i]
df <- fastainfo_df %>%
filter(fasta==fasta_file)
# make a tags.fasta file with all tag combinations of the fasta to be demultiplexed
tag_file <- make_adapter_fasta(fastainfo_df, fasta_file=fasta_file, tag_to_end=tag_to_end, outdir=tmp_dir)
# add path
fasta_file <- paste(fastadir, fasta_file, sep="")
# demultiplex fasta, write output to tmp file
demultiplex_cmd = paste(cutadapt_path, "cutadapt --cores=0 -e 0 --no-indels --trimmed-only -g file:", tag_file," -o ", tmp_dir, "tagtrimmed-{name}.fasta ", fasta_file, sep="")
print(demultiplex_cmd)
system(demultiplex_cmd)
# for a given marker, there is only one primer combination
primer_fwl <- df[1,"primer_fw"]
primer_rvl <- df[1,"primer_rv"]
primer_rvl_rc <- reverse_complement(primer_rvl)
for(f in 1:nrow(df)){# go through each de-multiplexed, tag-trimmed file and trim primers
outfilename <- paste(df[f,"sample"], df[f,"replicate"], sep="-")
outfilename <- paste(outfilename, ".fasta", sep="")
if(compress){
outfilename <- paste(outfilename, ".gz", sep="")
}
# complete fastainfo_df with output fasta name
fastainfo_df$filename[which(fastainfo_df$sample==df[f,"sample"] & fastainfo_df$replicate==df[f,"replicate"])] <- outfilename
# add path to output file
primer_trimmed_file <- paste(outdir, outfilename, sep="")
tag_trimmed_file <- paste(tmp_dir, "tagtrimmed-", df[f,"tag_fw"], "-", df[f,"tag_rv"], ".fasta", sep="")
if(primer_to_end){
primer_trim_cmd <- paste(cutadapt_path, "cutadapt --cores=0 -e ",cutadapt_error_rate ," --no-indels --trimmed-only --minimum-length ", cutadapt_minimum_length ," --maximum-length ", cutadapt_maximum_length, " -g ^", primer_fwl, "...", primer_rvl_rc, "$ --output ", primer_trimmed_file, " ", tag_trimmed_file, sep="")
}
else{
primer_trim_cmd <- paste(cutadapt_path, "cutadapt --cores=0 -e ",cutadapt_error_rate ," --no-indels --trimmed-only --minimum-length ", cutadapt_minimum_length ," --maximum-length ", cutadapt_maximum_length, ' -g "', primer_fwl, ';min_overlap=',nchar(primer_fwl),'...', primer_rvl_rc,  ';min_overlap=',nchar(primer_rvl_rc),'" --output ', primer_trimmed_file, " ", tag_trimmed_file, sep="")
}
print(primer_trim_cmd)
system(primer_trim_cmd)
} # end tag-trimmed
# delete the tmp dir wit the tag-trimmed files
unlink(tmp_dir, recursive = TRUE)
}# end fasta
# make sortedinfo file
fastainfo_df <- fastainfo_df %>%
select(-tag_fw, -primer_fw, -tag_rv, -primer_rv, -fasta)
#  write.table(fastainfo_df, file = paste(outdir, "sortedinfo.csv", sep=""),  row.names = F, sep=sep)
return(fastainfo_df)
}
get_read_counts <- function(df, dir){
df$read_count <- NA
for(file in df$filename){
file_path <- paste(dir, file, sep="")
read_n <- count_seq(file_path)
df$read_count[which(df$filename==file)] <- read_n
}
return(df)
}
a
SortReads <- function(fastainfo_df, fastadir, outdir="", cutadapt_path="" ,vsearch_path="", check_reverse=F, tag_to_end=T, primer_to_end=T, cutadapt_error_rate=0.1,cutadapt_minimum_length=50,cutadapt_maximum_length=500, sep=",",  compress=F){
#########
# SortReads_no_reverse does the whole demultilexing, trimming and compress on the + strand
# If sequences are not oriented, the -strand should be checked =>
# run SortReads_no_reverse of plus strand and on - strand after swapping fw and rev tags and primers,
# take the reverse complement of the -strand results (vsearch)
# pool the results of the 2 strands
# compress if necessary
# run on strand +
if(check_reverse){
#### use +strand, output to sorted_dir, uncompressed
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df, fastadir=fastadir, outdir=outdir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=F)
#### use - strand
# swap fw and rv tags and primers
fastainfo_df_tmp <- fastainfo_df %>%
select(tag_fw_tmp = tag_rv, tag_rv_tmp = tag_fw, primer_fw_tmp = primer_rv, primer_rv_tmp = primer_fw, sample, sample_type,habitat, replicate, fasta) %>%
select(tag_fw = tag_fw_tmp, tag_rv = tag_rv_tmp, primer_fw = primer_fw_tmp, primer_rv = primer_rv_tmp, sample, sample_type,habitat, replicate, fasta)
# make temp dir
outdir <- check_dir(outdir)
rc_dir <-paste(outdir, 'rc_', trunc(as.numeric(Sys.time())), sample(1:100, 1), sep='')
rc_dir <- check_dir(rc_dir)
# run sortreads on for reverse strand
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df_tmp, fastadir=fastadir, outdir=rc_dir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=F)
### reverse complement and pool
# get list of files demultiplexed on - strand
files <- list.files(path = rc_dir, pattern=".fasta")
# Filter the files based on the motif using regular expressions
# reverse complement sequences on the minus stand, and append info to the plus strand output
files <- grep(pattern = "\\.fasta", x = files, value = TRUE)
for(i in 1:length(files)){
plus <- paste(outdir, files[i], sep="")
minus <- paste(rc_dir, files[i], sep="")
minus_rc <- paste(rc_dir, "rc_", files[i], sep="")
# reverse complement sequences in minus file
rev_comp_cmd <- paste(vsearch_path, "vsearch --fastx_revcomp ", minus, " --fastaout ", minus_rc, " --quiet", sep="")
print(rev_comp_cmd)
system(rev_comp_cmd)
# append content of minus_rc to plus file
file.append(plus, minus_rc)
}
# delete temporary reverse_comp dir
unlink(rc_dir, recursive = TRUE)
### compress
if(compress){
for(i in 1:nrow(sortedinfo_df)){
file <- sortedinfo_df$filename[i]
sortedinfo_df$filename[i] <- paste(file, ".gz", sep="") # correct output filename
file <- paste(outdir, file, sep="") # add path
file_gz <- compress_file(file, remove_input=T) # compress file
}
}
}
else{
# check only + strand
sortedinfo_df <- SortReads_no_reverse(fastainfo_df=fastainfo_df, fastadir=fastadir, outdir=outdir, cutadapt_path=cutadapt_path, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
}
sortedinfo_df <- get_read_counts(sortedinfo_df, dir=outdir)
write.table(sortedinfo_df, file = paste(outdir, "sortedinfo.csv", sep=""),  row.names = F, sep=sep)
return(sortedinfo_df)
}
sortedinfo_df <- SortReads(fastainfo_df=fastainfo_df, fastadir=randomseq_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, check_reverse=check_reverse, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
