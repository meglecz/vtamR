phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
# create the output directory and check the the slash at the end
outdir <- check_dir(dir=outdir)
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
###
### Merge
###
fastq_ascii <- 33
fastq_maxdiffs <- 10
fastq_maxee <- 1
fastq_minlen <- 50
fastq_maxlen <- 500
fastq_minmergelen <- 50
fastq_maxmergelen <-500
fastq_maxns <- 0
fastq_truncqual <- 10
fastq_minovlen <- 50
fastq_allowmergestagger <- F
merged_dir <- paste(outdir, "merged/", sep="")
compress = T
# read fastqinfo
fastqinfo_df <- read.csv(fastqinfo, header=T, sep=sep)
fastainfo_df <- Merge(fastqinfo_df=fastqinfo_df, fastqdir=fastqdir, vsearch_path=vsearch_path, outdir=merged_dir, fastq_ascii=fastq_ascii, fastq_maxdiffs=fastq_maxdiffs, fastq_maxee=fastq_maxee, fastq_minlen=fastq_minlen, fastq_maxlen=fastq_maxlen, fastq_minmergelen=fastq_minmergelen, fastq_maxmergelen=fastq_maxmergelen, fastq_maxns=fastq_maxns, fastq_truncqual=fastq_truncqual, fastq_minovlen=fastq_minovlen, fastq_allowmergestagger=fastq_allowmergestagger, sep=sep, compress=compress)
###
### RandomSeq
###
randomseq_dir = paste(outdir, "random_seq/", sep="")
#fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
#fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
compress = T
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=10000, randseed=0, compress=compress)
View(fastainfo_df)
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=10000, randseed=0, compress=compress)
computer <- "Bombyx" # Bombyx/Endoume/Windows
if(computer == "Bombyx"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out/"
#  fastqdir <- "/home/meglecz/vtamR_large_files/fastq/"
#  fastqinfo <- "/home/meglecz/vtamR_large_files/user_input/fastqinfo_mfzr.csv"
#  outdir <- "/home/meglecz/vtamR_large_files/out/"
mock_composition <- "local/user_input/mock_composition_mfzr_eu.csv"
num_threads=8
compress = T
} else if (computer == "Endoume"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/emese/miniconda3/bin/"
vsearch_path = "/home/emese/miniconda3/bin/"
blast_path= "" # deactivate conda
db_path= "/home/emese/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "local/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out/"
num_threads=8
compress = T
}else if (computer == "Windows"){
vtam_dir <- "C:/Users/emese/vtamR/"
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
db_path="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "C:/Users/emese/vtamR_private/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out/"
num_threads=4
compress = F
}
sep=";"
setwd(vtam_dir)
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
# create the output directory and check the the slash at the end
outdir <- check_dir(dir=outdir)
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
###
### Merge
###
fastq_ascii <- 33
fastq_maxdiffs <- 10
fastq_maxee <- 1
fastq_minlen <- 50
fastq_maxlen <- 500
fastq_minmergelen <- 50
fastq_maxmergelen <-500
fastq_maxns <- 0
fastq_truncqual <- 10
fastq_minovlen <- 50
fastq_allowmergestagger <- F
merged_dir <- paste(outdir, "merged/", sep="")
compress = T
# read fastqinfo
fastqinfo_df <- read.csv(fastqinfo, header=T, sep=sep)
fastainfo_df <- Merge(fastqinfo_df=fastqinfo_df, fastqdir=fastqdir, vsearch_path=vsearch_path, outdir=merged_dir, fastq_ascii=fastq_ascii, fastq_maxdiffs=fastq_maxdiffs, fastq_maxee=fastq_maxee, fastq_minlen=fastq_minlen, fastq_maxlen=fastq_maxlen, fastq_minmergelen=fastq_minmergelen, fastq_maxmergelen=fastq_maxmergelen, fastq_maxns=fastq_maxns, fastq_truncqual=fastq_truncqual, fastq_minovlen=fastq_minovlen, fastq_allowmergestagger=fastq_allowmergestagger, sep=sep, compress=compress)
###
### RandomSeq
###
randomseq_dir = paste(outdir, "random_seq/", sep="")
#fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
#fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
compress = T
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=10000, randseed=0, compress=compress)
computer <- "Bombyx" # Bombyx/Endoume/Windows
if(computer == "Bombyx"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out/"
#  fastqdir <- "/home/meglecz/vtamR_large_files/fastq/"
#  fastqinfo <- "/home/meglecz/vtamR_large_files/user_input/fastqinfo_mfzr.csv"
#  outdir <- "/home/meglecz/vtamR_large_files/out/"
mock_composition <- "local/user_input/mock_composition_mfzr_eu.csv"
num_threads=8
compress = T
} else if (computer == "Endoume"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/emese/miniconda3/bin/"
vsearch_path = "/home/emese/miniconda3/bin/"
blast_path= "" # deactivate conda
db_path= "/home/emese/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "local/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out/"
num_threads=8
compress = T
}else if (computer == "Windows"){
vtam_dir <- "C:/Users/emese/vtamR/"
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
db_path="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "C:/Users/emese/vtamR_private/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out/"
num_threads=4
compress = F
}
sep=";"
setwd(vtam_dir)
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
# create the output directory and check the the slash at the end
outdir <- check_dir(dir=outdir)
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
###
### Merge
###
fastq_ascii <- 33
fastq_maxdiffs <- 10
fastq_maxee <- 1
fastq_minlen <- 50
fastq_maxlen <- 500
fastq_minmergelen <- 50
fastq_maxmergelen <-500
fastq_maxns <- 0
fastq_truncqual <- 10
fastq_minovlen <- 50
fastq_allowmergestagger <- F
merged_dir <- paste(outdir, "merged/", sep="")
compress = T
# read fastqinfo
fastqinfo_df <- read.csv(fastqinfo, header=T, sep=sep)
fastainfo_df <- Merge(fastqinfo_df=fastqinfo_df, fastqdir=fastqdir, vsearch_path=vsearch_path, outdir=merged_dir, fastq_ascii=fastq_ascii, fastq_maxdiffs=fastq_maxdiffs, fastq_maxee=fastq_maxee, fastq_minlen=fastq_minlen, fastq_maxlen=fastq_maxlen, fastq_minmergelen=fastq_minmergelen, fastq_maxmergelen=fastq_maxmergelen, fastq_maxns=fastq_maxns, fastq_truncqual=fastq_truncqual, fastq_minovlen=fastq_minovlen, fastq_allowmergestagger=fastq_allowmergestagger, sep=sep, compress=compress)
###
### RandomSeq
###
randomseq_dir = paste(outdir, "random_seq/", sep="")
#fastainfo <- paste(merged_dir, "fastainfo_gz.csv", sep="")
#fastainfo_df <- read.csv(file=fastainfo, header=T, sep=sep)
compress = T
RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=10000, randseed=0, compress=compress)
###
### SortReads
###
sorted_dir <- paste(outdir, "sorted", sep="")
check_reverse <- T
tag_to_end <- F
primer_to_end <-F
cutadapt_error_rate <- 0.1 # -e in cutadapt
cutadapt_minimum_length <- 50 # -m in cutadapt
cutadapt_maximum_length <- 500 # -M in cutadapt
compress <- F
fileinfo_df <- SortReads(fastainfo_df=fastainfo_df, fastadir=merged_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, check_reverse=check_reverse, tag_to_end=tag_to_end, primer_to_end=primer_to_end, cutadapt_error_rate=cutadapt_error_rate, cutadapt_minimum_length=cutadapt_minimum_length, cutadapt_maximum_length=cutadapt_maximum_length, sep=sep, compress=compress)
View(fileinfo_df)
###
### Read input fasta files, dereplicate reads to ASV, and count the number of reads of each ASV in each plate-marker-sample-replicate
###
# read fileinfo file to fileinfo_df if starting directly with demultiplexed, trimmed reads
# fileinfo_df <- read.csv(file, header=T, sep=sep)
fileinfo_df <- read.csv("vtamR_test/out/sorted/fileinfo.csv", header=T, sep=sep)
View(fileinfo_df)
read_count_df <- read_fastas_from_fileinfo(fileinfo_df, dir=sorted_dir, write_csv=F, outdir=outdir, sep=sep)
View(read_count_df)
View(stat_df)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
View(stat_df)
df <- read_count_df %>%
group_by(asv) %>%
summarize(sum_read_count = sum(read_count))
View(df)
View(read_count_df)
df$id <- paste(rownames(df), df$sum_read_count, sep='_')
df <- read_count_df %>%
group_by(asv) %>%
summarize(sum_read_count = sum(read_count))
df$id <- paste(">S", rownames(df), df$sum_read_count, sep='_')
View(df)
df$id <- paste(">S", rownames(df), df$sum_read_count, "\n", df$asv, "\n", sep="" )
View(df)
outfile <- paste(outdir, swarm_input.fasta, sep="")
outdir=swarm_dir
swarm_dir <- paste(outdir, "swarm", sep="")
swarm_dir <- check_dir(swarm_dir)
outdir=swarm_dir
outfile <- paste(outdir, swarm_input.fasta, sep="")
outfile <- paste(outdir, "swarm_input.fasta", sep="")
df <- read_count_df %>%
group_by(asv) %>%
summarize(sum_read_count = sum(read_count))
df$id <- paste(">S", rownames(df), df$sum_read_count, "\n", df$asv, "\n", sep="" )
writeLines(df$id, outfile)
outfile <- paste(outdir, "swarm_input.fasta", sep="")
df <- read_count_df %>%
group_by(asv) %>%
summarize(sum_read_count = sum(read_count))
df$id <- paste(">S", rownames(df), "_", df$sum_read_count, "\n", df$asv, sep="" )
writeLines(df$id, outfile)
swarm <- paste(swarm_path, "swarm -f -t ", num_threads, "-w", representatives, fasta, sep=" ")
representatives <- paste(outdir, "representatives.fasta", sep="")
swarm <- paste(swarm_path, "swarm -f -t ", num_threads, "-w", representatives, fasta, sep=" ")
swarm_path <- ""
swarm_path <- check_dir(swarm_path)
### make a fasta with dereplicated sequences
fasta <- make_fasta_swarm(read_count_df, outdir=outdir)
make_fasta_swarm <- function(read_count_df, outdir=""){
outfile <- paste(outdir, "swarm_input.fasta", sep="")
df <- read_count_df %>%
group_by(asv) %>%
summarize(sum_read_count = sum(read_count))
df$id <- paste(">S", rownames(df), "_", df$sum_read_count, "\n", df$asv, sep="" )
writeLines(df$id, outfile)
return(outfile)
}
### make a fasta with dereplicated sequences
fasta <- make_fasta_swarm(read_count_df, outdir=outdir)
representatives <- paste(outdir, "representatives.fasta", sep="")
swarm <- paste(swarm_path, "swarm -f -t ", num_threads, "-w", representatives, fasta, sep=" ")
print(swarm)
swarm <- paste(swarm_path, "swarm -f -t", num_threads, "-w", representatives, fasta, sep=" ")
print(swarm)
system(swarm)
clusters <- paste(outdir, "clusters.txt", sep="")
swarm <- paste(swarm_path, "swarm -f -t", num_threads, "-w", representatives, "-o", clusters, fasta, sep=" ")
print(swarm)
system(swarm)
View(df)
### make a fasta with dereplicated sequences
df <- read_count_df %>%
group_by(asv) %>%
summarize(sum_read_count = sum(read_count))
#  df$id <- paste(">S", rownames(df), "_", df$sum_read_count, "\n", df$asv, sep="" )
outfile <- paste(outdir, "swarm_input.fasta", sep="")
writeLines(paste(">S", rownames(df), "_", df$sum_read_count, "\n", df$asv, sep="" ), outfile)
### run swarm
representatives <- paste(outdir, "representatives.fasta", sep="")
clusters <- paste(outdir, "clusters.txt", sep="")
#  swarm <- paste(swarm_path, "swarm -f -t", num_threads, "-w", representatives, "-o", clusters, fasta, sep=" ")
swarm <- paste(swarm_path, "swarm -f -t", num_threads, "-o", clusters, fasta, sep=" ")
print(swarm)
system(swarm)
### make df with unique asv and read_count
df <- read_count_df %>%
group_by(asv) %>%
summarize(sum_read_count = sum(read_count))
### make a fasta with dereplicated sequences
input_swarm <- paste(outdir, "swarm_input.fasta", sep="")
writeLines(paste(">", rownames(df), "_", df$sum_read_count, "\n", df$asv, sep="" ), input_swarm)
clusters <- paste(outdir, "clusters.txt", sep="")
#  swarm <- paste(swarm_path, "swarm -f -t", num_threads, "-w", representatives, "-o", clusters, fasta, sep=" ")
swarm <- paste(swarm_path, "swarm -f -t", num_threads, "-o", clusters, fasta, sep=" ")
print(swarm)
system(swarm)
cluster_df <- read.table(clusters, fill =TRUE, strip.white=TRUE)
View(cluster_df)
cluster_df1 <- data.frame(Column1 = rep(cluster_df$V1, each = ncol(cluster_df) -1 ),
Column2 = as.vector(t(cluster_df[,-1]))
)
View(cluster_df1)
cluster_df1 <- na.omit(cluster_df1)
cluster_df1 <- na.omit(cluster_df1)
cluster_df <- read.table(clusters, fill =TRUE, strip.white=TRUE)
cluster_df1 <- data.frame(representative = rep(cluster_df$V1, each = ncol(cluster_df) -1 ),
clustered = as.vector(t(cluster_df[,-1]))
)
cluster_df1 <- cluster_df1 %>%
filter(!is.na(clustered))
cluster_df1 <- cluster_df1 %>%
filter(clustered != "")
cluster_df <- read.table(clusters, fill =TRUE, strip.white=TRUE)
cluster_df1 <- data.frame(representative = rep(cluster_df$V1, each = ncol(cluster_df) -1 ),
clustered = as.vector(t(cluster_df[,]))
)
cluster_df <- read.table(clusters, fill =TRUE, strip.white=TRUE)
cluster_df1 <- data.frame(representative = rep(cluster_df$V1, each = ncol(cluster_df) -1 ),
clustered = as.vector(t(cluster_df[,]))
)
cluster_df <- read.table(clusters, fill =TRUE, strip.white=TRUE)
cluster_df1 <- data.frame(representative = rep(cluster_df$V1, each = ncol(cluster_df)),
clustered = as.vector(t(cluster_df[,]))
)
cluster_df1 <- cluster_df1 %>%
filter(clustered != "")
cluster_df1$representative <- sub("_[0-9]+", "", cluster_df1$representative )
### pool clusters in read_count_df
cluster_df <- read.table(clusters, fill =TRUE, strip.white=TRUE)
cluster_df1 <- data.frame(representative = rep(cluster_df$V1, each = ncol(cluster_df)),
clustered = as.vector(t(cluster_df[,])))
# delete line with no values in clustered
cluster_df1 <- cluster_df1 %>%
filter(clustered != "")
cluster_df1$representative <- sub("_[0-9]+", "", cluster_df1$representative )
cluster_df1 <- sub("_[0-9]+", "", cluster_df1 )
### pool clusters in read_count_df
cluster_df <- read.table(clusters, fill =TRUE, strip.white=TRUE)
cluster_df1 <- data.frame(representative = rep(cluster_df$V1, each = ncol(cluster_df)),
clustered = as.vector(t(cluster_df[,])))
# delete line with no values in clustered
cluster_df1 <- cluster_df1 %>%
filter(clustered != "")
cluster_df1$representative <- sub("_[0-9]+", "", cluster_df1$representative )
cluster_df1$clustered <- sub("_[0-9]+", "", cluster_df1$clustered )
cluster_df <- read.table(clusters, fill =TRUE, strip.white=TRUE)
cluster_df <- data.frame(representative = rep(cluster_df$V1, each = ncol(cluster_df)),
clustered = as.vector(t(cluster_df[,])))
# delete line with no values in clustered
cluster_df <- cluster_df %>%
filter(clustered != "")
cluster_df$representative <- sub("_[0-9]+", "", cluster_df$representative )
cluster_df$clustered <- sub("_[0-9]+", "", cluster_df$clustered )
### make df with unique asv and read_count
df <- read_count_df %>%
group_by(asv) %>%
summarize(sum_read_count = sum(read_count))
df$id <- rownames(df)
### make df with unique asv and read_count
df <- read_count_df %>%
group_by(asv) %>%
summarize(sum_read_count = sum(read_count))
df$id <- rownames(df)
### make a fasta with dereplicated sequences
input_swarm <- paste(outdir, "swarm_input.fasta", sep="")
writeLines(paste(">", df$id, "_", df$sum_read_count, "\n", df$asv, sep="" ), input_swarm)
### run swarm
#  representatives <- paste(outdir, "representatives.fasta", sep="")
clusters <- paste(outdir, "clusters.txt", sep="")
#  swarm <- paste(swarm_path, "swarm -f -t", num_threads, "-w", representatives, "-o", clusters, fasta, sep=" ")
swarm <- paste(swarm_path, "swarm -f -t", num_threads, "-o", clusters, fasta, sep=" ")
print(swarm)
system(swarm)
### pool clusters in read_count_df
cluster_df <- read.table(clusters, fill =TRUE, strip.white=TRUE)
cluster_df <- data.frame(representative = rep(cluster_df$V1, each = ncol(cluster_df)),
clustered = as.vector(t(cluster_df[,])))
# delete line with no values in clustered
cluster_df <- cluster_df %>%
filter(clustered != "")
cluster_df$representative <- sub("_[0-9]+", "", cluster_df$representative )
cluster_df$clustered <- sub("_[0-9]+", "", cluster_df$clustered )
cluster_df <- left_join(cluster_df, df, by= c("representative" = "id"))
cluster_df <- read.table(clusters, fill =TRUE, strip.white=TRUE)
cluster_df <- data.frame(representative = rep(cluster_df$V1, each = ncol(cluster_df)),
clustered = as.vector(t(cluster_df[,])))
# delete line with no values in clustered
cluster_df <- cluster_df %>%
filter(clustered != "")
# delete read counts from id
cluster_df$representative <- sub("_[0-9]+", "", cluster_df$representative )
cluster_df$clustered <- sub("_[0-9]+", "", cluster_df$clustered )
cluster_df <- left_join(cluster_df, df, by= c("representative" = "id")) %>%
select(-represenative, -sum_read_count, asv_represenativ=asv)
cluster_df <- left_join(cluster_df, df, by= c("representative" = "id")) %>%
select(-representative, -sum_read_count, asv_representative=asv)
cluster_df <- cluster_df %>%
filter(clustered != "")
# delete read counts from id
cluster_df$representative <- sub("_[0-9]+", "", cluster_df$representative )
cluster_df$clustered <- sub("_[0-9]+", "", cluster_df$clustered )
cluster_df <- left_join(cluster_df, df, by= c("representative" = "id")) %>%
select(-representative, -sum_read_count, asv_representative=asv) %>%
left_join(cluster_df, df, by= c("clustered" = "id"))
cluster_df <- left_join(cluster_df, df, by= c("clustered" = "id"))%>%
select(-clustered, -sum_read_count, asv_clustered=asv)
View(read_count_df)
read_count_df1 <- left_join(read_count_df, cluster_df,  by= c("asv" = "asv_clustered"))
View(read_count_df1)
colnames(read_count_df1)
read_count_df1 <- read_count_df1 %>%
select(plate, marker,sample,replicate,ead_count, "asv"=asv_representative)
read_count_df1 <- read_count_df1 %>%
select(-asv)
read_count_df1 <- left_join(read_count_df, cluster_df,  by= c("asv" = "asv_clustered"))
read_count_df1 <- read_count_df1 %>%
select(-asv) %>%
select(asv=asv_representative)
read_count_df1 <- left_join(read_count_df, cluster_df,  by= c("asv" = "asv_clustered"))
read_count_df1 <- read_count_df1 %>%
select(-asv) %>%
select(asv=asv_representative,plate,marker,sample,replicate,read_count)
read_count_df1 <- left_join(read_count_df, cluster_df,  by= c("asv" = "asv_clustered"))
read_count_df1 <- read_count_df1 %>%
select(-asv) %>%
select(asv=asv_representative,plate,marker,sample,replicate,read_count) %>%
group_by( c(asv,plate,marker,sample,replicate)) %>%
summarize(read_count_cluster=sum(read_count))
read_count_df1 <- left_join(read_count_df, cluster_df,  by= c("asv" = "asv_clustered"))
read_count_df1 <- read_count_df1 %>%
select(-asv) %>%
select(asv=asv_representative,plate,marker,sample,replicate,read_count)
read_count_df1 <- left_join(read_count_df, cluster_df,  by= c("asv" = "asv_clustered"))
read_count_df1 <- read_count_df1 %>%
select(-asv) %>%
select(asv=asv_representative,plate,marker,sample,replicate,read_count) %>%
group_by(asv,plate,marker,sample,replicate) %>%
summarize(read_count_cluster=sum(read_count))
tmp <- unique(read_count_df1$asv)
length(tmp)
