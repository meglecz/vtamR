lfn_sample_replicate_cutoff <- 0.004
outfile <- file.path(outdir, "filter", "9_LFNsampleReplicate.csv")
read_count_df <- LFNsampleReplicate(read_count_df,
cutoff=lfn_sample_replicate_cutoff,
outfile=outfile
)
stat_df <- GetStat(read_count_df,
stat_df,
stage="LFNsampleReplicate",
params=lfn_sample_replicate_cutoff
)
min_replicate_number <- 2
outfile <- file.path(outdir, "filter", "10_FilterMinReplicate.csv")
read_count_df <- FilterMinReplicate(read_count_df,
cutoff=min_replicate_number,
outfile=outfile
)
stat_df <- GetStat(read_count_df,
stat_df,
stage="FilterMinReplicate",
params=min_replicate_number
)
# Pool replicates
read_count_samples_df <- PoolReplicates(read_count_df)
# Detect known occurrences
results <- MakeKnownOccurrences(read_count_samples = read_count_samples_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition
)
View(read_count_samples_df)
# Detect known occurrences
results <- MakeKnownOccurrences(read_count = read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition
)
library("devtools")
library("roxygen2")
load_all(".")
roxygenise()
usethis::use_roxygen_md()
# Detect known occurrences
results <- MakeKnownOccurrences(read_count = read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition
)
load_all(".")
roxygenise()
usethis::use_roxygen_md()
# Detect known occurrences
results <- MakeKnownOccurrences(read_count = read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition
)
load_all(".")
roxygenise()
usethis::use_roxygen_md()
# Detect known occurrences
results <- MakeKnownOccurrences(read_count = read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition
)
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
outfile = file.path(outdir, "optimize", "OptimizeLFNreadCountLFNvariant.csv")
OptimizeLFNreadCountLFNvariant_df <- OptimizeLFNreadCountLFNvariant(
read_count_df,
known_occurrences=known_occurrences_df,
outfile= outfile,
min_replicate_number=2
)
lnf_variant_cutoff = 0.001
outfile <- file.path(outdir, "filter", "11_LFNvariant.csv")
read_count_df_lnf_variant <- LFNvariant(read_count_df,
cutoff=lnf_variant_cutoff,
outfile=outfile
)
stat_df <- GetStat(read_count_df_lnf_variant,
stat_df,
stage="LFNvariant",
params=lnf_variant_cutoff)
lfn_read_count_cutoff <- 10
outfile <- file.path(outdir, "filter", "12_LFNreadCount.csv")
read_count_df_lfn_read_count <- LFNreadCount(read_count_df,
cutoff=lfn_read_count_cutoff,
outfile=outfile
)
stat_df <- GetStat(read_count_df_lfn_read_count,
stat_df, stage="LFNreadCount",
params=lfn_read_count_cutoff
)
outfile <- file.path(outdir, "filter", "13_poolLFN.csv")
read_count_df <- PoolFilters(read_count_df_lfn_read_count,
read_count_df_lnf_variant,
outfile=outfile
)
stat_df <- GetStat(read_count_df,
stat_df,
stage="FilterLFN"
)
# delete temporary data frames
rm(read_count_df_lfn_read_count)
rm(read_count_df_lnf_variant)
min_replicate_number <- 2
outfile <- file.path(outdir, "filter", "14_FilterMinReplicate.csv")
read_count_df <- FilterMinReplicate(read_count_df,
cutoff=min_replicate_number,
outfile=outfile
)
stat_df <- GetStat(read_count_df,
stat_df,
stage="FilterMinReplicate",
params=min_replicate_number
)
missing_occurrences <- file.path(outdir, "missing_occurrences.csv")
performance_metrics <- file.path(outdir, "performance_metrics.csv")
known_occurrences <- file.path(outdir, "known_occurrences.csv")
results <- MakeKnownOccurrences(read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition,
known_occurrences=known_occurrences,
missing_occurrences=missing_occurrences,
performance_metrics=performance_metrics
)
load_all(".")
roxygenise()
usethis::use_roxygen_md()
results <- MakeKnownOccurrences(read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition,
known_occurrences=known_occurrences,
missing_occurrences=missing_occurrences,
performance_metrics=performance_metrics
)
load_all(".")
roxygenise()
usethis::use_roxygen_md()
results <- MakeKnownOccurrences(read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition,
known_occurrences=known_occurrences,
missing_occurrences=missing_occurrences,
performance_metrics=performance_metrics
)
results <- MakeKnownOccurrences(read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition,
#                                known_occurrences=known_occurrences,
#                                missing_occurrences=missing_occurrences,
#                                performance_metrics=performance_metrics
)
View(read_count_df)
View(read_count_samples_df)
sortedinfo_df
sortedinfo=sortedinfo_df
mock_composition=mock_composition
mock_composition
known_occurrences
missing_occurrences
performance_metrics
results <- MakeKnownOccurrences(read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition,
known_occurrences=known_occurrences,
missing_occurrences=missing_occurrences,
performance_metrics=performance_metrics
)
load_all(".")
roxygenise()
usethis::use_roxygen_md()
results <- MakeKnownOccurrences(read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition,
known_occurrences=known_occurrences,
missing_occurrences=missing_occurrences,
performance_metrics=performance_metrics
)
load_all(".")
roxygenise()
usethis::use_roxygen_md()
results <- MakeKnownOccurrences(read_count_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition,
known_occurrences=known_occurrences,
missing_occurrences=missing_occurrences,
performance_metrics=performance_metrics
)
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
knitr::kable(performance_metrics_df, format = "markdown")
outfile <- file.path(outdir, "filter", "15_PoolReplicates.csv")
read_count_samples_df <- PoolReplicates(read_count_df,
outfile=outfile)
stat_df <- GetStat(read_count_samples_df,
stat_df,
stage="PoolReplicates")
outfile <- file.path(outdir, "filter", "16_mOTU_ClusterSize.csv")
identity <- 0.97
read_count_samples_df_ClusterSize <- ClusterSize(read_count_samples_df,
id=identity,
vsearch_path=vsearch_path,
outfile=outfile)
stat_df <- GetStat(read_count_samples_df_ClusterSize,
stat_df,
stage="ClusterSize_mOTU",
param=identity
)
outfile <- file.path(outdir, "filter", "17_mOTU_Swarm.csv")
by_sample <- FALSE
d = 7
read_count_samples_df_swarm <- Swarm(read_count_samples_df,
swarm_path=swarm_path,
swarm_d=d,
fastidious=FALSE,
num_threads=num_threads,
by_sample=by_sample,
outfile= outfile)
stat_df <- GetStat(read_count_samples_df_swarm, stat_df, stage="Swarm_mOTU", params=d)
outfile <- file.path(outdir, "TaxAssign.csv")
asv_tax <- TaxAssign(asv=read_count_samples_df,
taxonomy=taxonomy,
blast_db=blast_db,
blast_path=blast_path,
outfile=outfile,
num_threads=num_threads
)
outfile=file.path(outdir, "Final_asvtable_with_TaxAssign.csv")
asv_table_df <- WriteASVtable(read_count_samples_df,
outfile=outfile,
asv_tax=asv_tax,
sortedinfo=sortedinfo_df,
add_empty_samples=T,
add_sums_by_sample=T,
add_sums_by_asv=T,
add_expected_asv=T,
mock_composition=mock_composition
)
write.csv(stat_df, file = file.path(outdir, "Summary_steps.csv"))
### If pb with vignette not building correctly
# clean package
unlink("~/R/x86_64-pc-linux-gnu-library/4.0/vtamR", recursive = TRUE, force = TRUE)
file.exists("~/R/x86_64-pc-linux-gnu-library/4.0/vtamR")
# Clean and regenerate documentation:
devtools::clean_dll()
devtools::document()
devtools::install(build_vignettes = FALSE, force = TRUE)
# test library
library(vtamR)
# install with vignettes (local)
devtools::install(build_vignettes = TRUE)
### If pb with vignette not building correctly
# clean package
unlink("~/R/x86_64-pc-linux-gnu-library/4.0/vtamR", recursive = TRUE, force = TRUE)
file.exists("~/R/x86_64-pc-linux-gnu-library/4.0/vtamR")
# Clean and regenerate documentation:
devtools::clean_dll()
devtools::document()
devtools::install(build_vignettes = FALSE, force = TRUE)
# install with vignettes (local)
devtools::install(build_vignettes = TRUE)
# install with vignettes (local)
devtools::install(build_vignettes = TRUE)
## load
library(vtamR)
library(dplyr)
setwd("/home/meglecz/vtamR/")
cutadapt_path <- "~/miniconda3/envs/vtam/bin/cutadapt"
vsearch_path <- "~/miniconda3/envs/vtam/bin/vsearch"
blast_path <- "~/miniconda3/envs/vtam/bin/blastn"
swarm_path <- "swarm"
num_threads <- 8
sep <- ","
outdir <- "~/vtamR_demo_out"
fastq_dir <- system.file("extdata/demo/fastq", package = "vtamR")
fastqinfo <-  system.file("extdata/demo/fastqinfo.csv", package = "vtamR")
mock_composition <-  system.file("extdata/demo/mock_composition.csv", package = "vtamR")
asv_list <-  system.file("extdata/demo/asv_list.csv", package = "vtamR")
taxonomy <- system.file("extdata/db_test/taxonomy_reduced.tsv", package = "vtamR")
blast_db <- system.file("extdata/db_test", package = "vtamR")
blast_db <- file.path(blast_db, "COInr_reduced")
### Merge
merged_dir <- file.path(outdir, "merged")
fastainfo_df <- Merge(fastqinfo,
fastq_dir=fastq_dir,
vsearch_path=vsearch_path,
outdir=merged_dir,
fastq_maxee=1,
fastq_maxns=0,
fastq_allowmergestagger=F
)
### demultiplex
sorted_dir <- file.path(outdir, "sorted")
sortedinfo_df <- SortReads(fastainfo_df,
fasta_dir=merged_dir,
outdir=sorted_dir,
check_reverse=TRUE,
cutadapt_path=cutadapt_path,
vsearch_path=vsearch_path
)
###############
### dereplicate
###############
read_count_df <- Dereplicate(sortedinfo_df,
dir=sorted_dir,
asv_list=asv_list
)
### stat
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
stat_df <- GetStat(read_count_df, stat_df, stage="input_sample_replicate", params=NA)
#### swarm
by_sample <- TRUE
read_count_df <- Swarm(read_count_df,
swarm_path=swarm_path,
num_threads=num_threads,
by_sample=by_sample)
View(read_count_df)
library("devtools")
library("roxygen2")
load_all(".")
roxygenise()
usethis::use_roxygen_md()
#### swarm
by_sample <- TRUE
read_count_df <- Swarm(read_count_df,
swarm_path=swarm_path,
num_threads=num_threads,
by_sample=by_sample)
load_all(".")
roxygenise()
usethis::use_roxygen_md()
#### swarm
by_sample <- TRUE
read_count_df <- Swarm(read_count_df,
swarm_path=swarm_path,
num_threads=num_threads,
by_sample=by_sample)
View(read_count_df)
#### LFNglobalReadCount
global_read_count_cutoff = 2
read_count_df <- LFNglobalReadCount(read_count_df,
cutoff=global_read_count_cutoff)
#### FilterIndel
read_count_df <- FilterIndel(read_count_df)
### FilterCodonStop
genetic_code = 5
read_count_df <- FilterCodonStop(read_count_df,
genetic_code=genetic_code)
### FilterChimera
abskew=2
by_sample = T
sample_prop = 0.8
read_count_df <- FilterChimera(read_count_df,
vsearch_path=vsearch_path,
by_sample=by_sample,
sample_prop=sample_prop,
abskew=abskew)
#### FilterRenkonen
cutoff <- 0.4
read_count_df <- FilterRenkonen(read_count_df,
cutoff=cutoff)
### FilterPCRerror
pcr_error_var_prop <- 0.05
max_mismatch <- 2
read_count_df <- FilterPCRerror(read_count_df,
vsearch_path=vsearch_path,
pcr_error_var_prop=pcr_error_var_prop,
max_mismatch=max_mismatch)
### LFNsampleReplicate
lfn_sample_replicate_cutoff <- 0.004
read_count_df <- LFNsampleReplicate(read_count_df,
cutoff=lfn_sample_replicate_cutoff)
### FilterMinReplicate
min_replicate_number <- 2
read_count_df <- FilterMinReplicate(read_count_df,
cutoff=min_replicate_number)
### LFNvariant
lnf_variant_cutoff = 0.001
read_count_df_lnf_variant <- LFNvariant(read_count_df,
cutoff=lnf_variant_cutoff)
### LFNreadCount
lfn_read_count_cutoff <- 10
read_count_df_lfn_read_count <- LFNreadCount(read_count_df,
cutoff=lfn_read_count_cutoff)
### Combine results
read_count_df <- PoolFilters(read_count_df_lfn_read_count,
read_count_df_lnf_variant)
# delete temporary data frames
rm(read_count_df_lfn_read_count)
rm(read_count_df_lnf_variant)
### FilterMinReplicate
min_replicate_number <- 2
read_count_df <- FilterMinReplicate(read_count_df,
cutoff=min_replicate_number)
#### PoolReplicates
read_count_samples_df <- PoolReplicates(read_count_df)
### MakeKnownOccurrences performance_metrics
results <- MakeKnownOccurrences(read_count_samples_df,
sortedinfo=sortedinfo_df,
mock_composition=mock_composition)
by_sample <- FALSE
d = 7
read_count_df_swarm_motu <- Swarm(read_count_samples_df,
swarm_path=swarm_path,
swarm_d=d,
fastidious=FALSE,
num_threads=num_threads,
by_sample=by_sample)
stat_df <- GetStat(read_count_df_swarm_motu, stat_df, stage="swarm_motu_7", params=d)
View(read_count_df_swarm_motu)
### mOTU with ClusterSize
identity <- 0.97
read_count_df_clustersize_motu <- ClusterSize(read_count_samples_df,
id=identity,
vsearch_path=vsearch_path,
by_sample=FALSE)
View(read_count_df_clustersize_motu)
### set up
cutadapt_path <- "~/miniconda3/envs/vtam/bin/cutadapt"
vsearch_path <- "~/miniconda3/envs/vtam/bin/vsearch"
blast_path <- "~/miniconda3/envs/vtam/bin/blastn"
swarm_path <- "swarm"
num_threads <- 8
sep <- ","
outdir <- "~/vtamR_demo_out"
fastq_dir <- system.file("extdata/demo/fastq", package = "vtamR")
fastqinfo <-  system.file("extdata/demo/fastqinfo.csv", package = "vtamR")
mock_composition <-  system.file("extdata/demo/mock_composition.csv", package = "vtamR")
asv_list <-  system.file("extdata/demo/asv_list.csv", package = "vtamR")
taxonomy <- system.file("extdata/db_test/taxonomy_reduced.tsv", package = "vtamR")
blast_db <- system.file("extdata/db_test", package = "vtamR")
blast_db <- file.path(blast_db, "COInr_reduced")
### Merge
merged_dir <- file.path(outdir, "merged")
fastainfo_df <- Merge(fastqinfo,
fastq_dir=fastq_dir,
vsearch_path=vsearch_path,
outdir=merged_dir,
fastq_maxee=1,
fastq_maxns=0,
fastq_allowmergestagger=F
)
sorted_dir <- file.path(outdir, "sorted")
sortedinfo_df <- SortReads(fastainfo_df,
fasta_dir=merged_dir,
outdir=sorted_dir,
check_reverse=FALSE,
cutadapt_path=cutadapt_path,
vsearch_path=vsearch_path
)
?Merge
load_all(".")
roxygenise()
usethis::use_roxygen_md()
cutadapt_path <- "~/miniconda3/envs/vtam/bin/cutadapt"
vsearch_path <- "~/miniconda3/envs/vtam/bin/vsearch"
blast_path <- "~/miniconda3/envs/vtam/bin/blastn"
swarm_path <- "swarm"
num_threads <- 8
sep <- ","
outdir <- "~/vtamR_demo_out"
fastq_dir <- system.file("extdata/demo/fastq", package = "vtamR")
fastqinfo <-  system.file("extdata/demo/fastqinfo.csv", package = "vtamR")
mock_composition <-  system.file("extdata/demo/mock_composition.csv", package = "vtamR")
asv_list <-  system.file("extdata/demo/asv_list.csv", package = "vtamR")
taxonomy <- system.file("extdata/db_test/taxonomy_reduced.tsv", package = "vtamR")
blast_db <- system.file("extdata/db_test", package = "vtamR")
blast_db <- file.path(blast_db, "COInr_reduced")
### Merge
merged_dir <- file.path(outdir, "merged")
fastainfo_df <- Merge(fastqinfo,
fastq_dir=fastq_dir,
vsearch_path=vsearch_path,
outdir=merged_dir,
fastq_maxee=1,
fastq_maxns=0,
fastq_allowmergestagger=F
)
### demultiplex
sorted_dir <- file.path(outdir, "sorted")
sortedinfo_df <- SortReads(fastainfo_df,
fasta_dir=merged_dir,
outdir=sorted_dir,
check_reverse=TRUE,
cutadapt_path=cutadapt_path,
vsearch_path=vsearch_path
)
###############
### dereplicate
###############
read_count_df <- Dereplicate(sortedinfo_df,
dir=sorted_dir,
asv_list=asv_list
)
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
stat_df <- GetStat(read_count_df, stat_df, stage="input_sample_replicate", params=NA)
#### swarm
by_sample <- TRUE
read_count_df <- Swarm(read_count_df,
swarm_path=swarm_path,
num_threads=num_threads,
by_sample=by_sample)
