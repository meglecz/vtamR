stat_df <- get_stat(read_count_df, stat_df, stage="swarm_by_sample", params=NA)
read_count_df <- read_count_df_backup
read_count_df <- swarm1(read_count_df, outdir=outdir, swarm_path=swarm_path, num_threads=num_threads, swarm_d=swarm_d, fastidious=fastidious, write_csv=T, sep=sep, by_sample=F)
stat_df <- get_stat(read_count_df, stat_df, stage="swarm_all", params=NA)
read_count_df <- read_count_df_backup
computer <- "Bombyx" # Bombyx/Endoume/Windows
if(computer == "Bombyx"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
swarm_path <- ""
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/"
#  fastqdir <- "vtamR_test/data/"
#  fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
#  outdir <- "vtamR_test/out/"
#  mock_composition <- "local/user_input/mock_composition_mfzr_eu.csv"
fastqdir <- "/home/meglecz/vtamR_large_files/fastq/"
fastqinfo <- "/home/meglecz/vtamR_large_files/user_input/fastqinfo_mfzr.csv"
outdir <- "/home/meglecz/vtamR_large_files/out/"
mock_composition <- "local/user_input/mock_composition_mfzr_prerun.csv"
num_threads=8
compress = T
} else if (computer == "Endoume"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/emese/miniconda3/bin/"
vsearch_path = "/home/emese/miniconda3/bin/"
blast_path= "" # deactivate conda
swarm_path <- ""
db_path= "/home/emese/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "local/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out/"
num_threads=8
compress = T
}else if (computer == "Windows"){
vtam_dir <- "C:/Users/emese/vtamR/"
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
swarm_path <- ""
db_path="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "C:/Users/emese/vtamR_private/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out/"
num_threads=4
compress = F
}
sep=";"
setwd(vtam_dir)
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
# create the output directory and check the the slash at the end
outdir <- check_dir(dir=outdir)
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
outdir
###
### SortReads
###
sorted_dir <- paste(outdir, "sorted", sep="")
###
### Read input fasta files, dereplicate reads to ASV, and count the number of reads of each ASV in each plate-marker-sample-replicate
###
# read fileinfo file to fileinfo_df if starting directly with demultiplexed, trimmed reads
# fileinfo_df <- read.csv(file, header=T, sep=sep)
fileinfo <- "/home/meglecz/vtamR_large_files/out/sorted/fileinfo.csv"
fileinfo_df <- read.csv(fileinfo, header=T, sep=sep)
#fileinfo_df <- read.csv("vtamR_test/out/sorted/fileinfo.csv", header=T, sep=sep)
read_count_df <- read_fastas_from_fileinfo(fileinfo_df, dir=sorted_dir, write_csv=T, outdir=outdir, sep=sep)
# make stat counts
stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
read_count_df_backup <- read_count_df
read_count_df <- read_count_df_backup
swarm_d <- 1
fastidious <- TRUE
by_sample <- TRUE
read_count_df_swarm_all <- swarm(read_count_df, outdir=outdir, swarm_path=swarm_path, num_threads=num_threads, swarm_d=swarm_d, fastidious=fastidious, write_csv=T, sep=sep, by_sample=F)
stat_df <- get_stat(read_count_df_swarm_all, stat_df, stage="swarm_all", params=NA)
read_count_df <- read_count_df_backup
read_count_df_swarm_by_sample <- swarm(read_count_df, outdir=outdir, swarm_path=swarm_path, num_threads=num_threads, swarm_d=swarm_d, fastidious=fastidious, write_csv=T, sep=sep, by_sample=T)
stat_df <- get_stat(read_count_df_swarm_by_sample, stat_df, stage="swarm_by_sample", params=NA)
View(read_count_df_swarm_all)
View(read_count_df_swarm_by_sample)
View(read_count_df_swarm_all)
View(read_count_df_swarm_all)
outdir
outdir <- "/home/meglecz/vtamR_large_files/out/swarm_all"
# Eliminate variants with less than global_read_count_cutoff reads in the dataset
global_read_count_cutoff = 2
read_count_df <- read_count_df_swarm_all
read_count_df_wo_swarm <- LFN_global_read_count(read_count_df, global_read_count_cutoff, write_csv=T, outdir=outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
###
### LFN_filters
###
# LFN_read_count
lfn_read_count_cutoff <- 10
read_count_df_lfn_read_count <- LFN_read_count(read_count_df, cutoff=lfn_read_count_cutoff, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df_lfn_read_count, stat_df, stage="LFN_read_count", params=lfn_read_count_cutoff)
# LFN_sample_replicate (by column)
lfn_sample_replicate_cutoff <- 0.001
read_count_df_lnf_sample_replicate <- LFN_sample_replicate(read_count_df, cutoff=lfn_sample_replicate_cutoff, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df_lnf_sample_replicate, stat_df, stage="LFN_sample_replicate", params=lfn_sample_replicate_cutoff)
# LFN_sample_variant (by line)
lnf_variant_cutoff = 0.001
by_replicate = TRUE
read_count_df_lnf_variant <- LFN_variant(read_count_df, cutoff=lnf_variant_cutoff, by_replicate, write_csv=T, outdir = outdir, sep=sep)
param_values <- paste(lnf_variant_cutoff, by_replicate, sep=";")
stat_df <- get_stat(read_count_df_lnf_variant, stat_df, stage="LFN_variant", params=param_values)
# pool the results of the different filterLFN to one data frame; keep only occurrences that passed all filters
read_count_df <- pool_LFN(read_count_df_lfn_read_count, read_count_df_lnf_variant, read_count_df_lnf_sample_replicate, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
read_count_df_lfn_read_count <- NULL
read_count_df_lnf_variant <- NULL
read_count_df_lnf_sample_replicate <- NULL
###
### keep repeatable occurrences
###
min_replicate_number <- 2
read_count_df <- FilterMinReplicateNumber(read_count_df, min_replicate_number, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterMinReplicateNumber", params=min_replicate_number)
###
### FilerPCRerror
###
pcr_error_var_prop <- 0.1
max_mismatch <- 1
by_sample <- T
sample_prop <- 0.8
read_count_df <- FilterPCRerror(read_count_df, write_csv=T, outdir=outdir, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch, by_sample=by_sample, sample_prop=sample_prop, sep=sep)
params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilerPCRerror", params=params)
###
### FilterChimera
###
abskew=2
by_sample = T
sample_prop = 0.8
read_count_df <- FilterChimera(read_count_df, write_csv=T, outdir=outdir, vsearch_path=vsearch_path, by_sample=by_sample, sample_prop=sample_prop, abskew=abskew, sep=sep)
params <- paste(abskew, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilterChimera", params=params)
###
### FilerRenkonen
###
# Renkonen index:
# PS = summ(min(p1i, p2i))
# p1i = number of reads for variant i in replicate 1 / number of reads in replicate 1
renkonen_distance_quantile = 0.9
read_count_df <- FilterRenkonen(read_count_df, write_csv=T, outdir=outdir, renkonen_distance_quantile=renkonen_distance_quantile, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerRenkonen", params=renkonen_distance_quantile)
###
### FilerIndel
###
read_count_df <- FilterIndel(read_count_df, write_csv=T, outdir=outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterIndel")
###
### FilerCodonStop
###
genetic_code = 5
read_count_df <- FilterCodonStop(read_count_df, write_csv=T, outdir=outdir, genetic_code=genetic_code, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
###
### PoolReplicates
###
digits = 0
read_count_samples_df <- PoolReplicates(read_count_df, digits=digits, write_csv=T, outdir=outdir, sep=sep)
###
### TaxAssign
###
asv_tax <- TaxAssign(df=read_count_samples_df, ltg_params_df=ltg_params_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outdir=outdir, num_threads=num_threads)
# write the list of ASV and their taxonomic assignment
write.csv(asv_tax, file = paste(outdir, "taxa.csv", sep=""), row.names = F)
###
### print output files
###
# write sequence and variant counts after each step
write.csv(stat_df, file = paste(outdir, "count_stat.csv", sep=""))
# long format, each line corresponds to an occurrence ()
write.csv(read_count_samples_df, file = paste(outdir, "Final_asvtable_long.csv", sep=""), row.names=F)
# wide format (ASV table), samples are in columns, ASVs in lines
outfile=paste(outdir, "Final_asvtable.csv", sep="")
write_asvtable(read_count_samples_df, outfile=outfile, fileinfo=fileinfo, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition, sep=sep)
# write ASV table completed by taxonomic assignments
outfile=paste(outdir, "Final_asvtable_with_taxassign.csv", sep="")
write_asvtable(read_count_samples_df, outfile=outfile, asv_tax=asv_tax, fileinfo=fileinfo, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition, sep=sep)
outdir <- "/home/meglecz/vtamR_large_files/out/swarm_by_sample/"
read_count_df <- read_count_df_swarm_by_sample
###
### LFN_global_read_count
###
# Eliminate variants with less than global_read_count_cutoff reads in the dataset
global_read_count_cutoff = 2
read_count_df_wo_swarm <- LFN_global_read_count(read_count_df, global_read_count_cutoff, write_csv=T, outdir=outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
###
### LFN_filters
###
# LFN_read_count
lfn_read_count_cutoff <- 10
read_count_df_lfn_read_count <- LFN_read_count(read_count_df, cutoff=lfn_read_count_cutoff, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df_lfn_read_count, stat_df, stage="LFN_read_count", params=lfn_read_count_cutoff)
# LFN_sample_replicate (by column)
lfn_sample_replicate_cutoff <- 0.001
read_count_df_lnf_sample_replicate <- LFN_sample_replicate(read_count_df, cutoff=lfn_sample_replicate_cutoff, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df_lnf_sample_replicate, stat_df, stage="LFN_sample_replicate", params=lfn_sample_replicate_cutoff)
# LFN_sample_variant (by line)
lnf_variant_cutoff = 0.001
by_replicate = TRUE
read_count_df_lnf_variant <- LFN_variant(read_count_df, cutoff=lnf_variant_cutoff, by_replicate, write_csv=T, outdir = outdir, sep=sep)
param_values <- paste(lnf_variant_cutoff, by_replicate, sep=";")
stat_df <- get_stat(read_count_df_lnf_variant, stat_df, stage="LFN_variant", params=param_values)
# pool the results of the different filterLFN to one data frame; keep only occurrences that passed all filters
read_count_df <- pool_LFN(read_count_df_lfn_read_count, read_count_df_lnf_variant, read_count_df_lnf_sample_replicate, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
read_count_df_lfn_read_count <- NULL
read_count_df_lnf_variant <- NULL
read_count_df_lnf_sample_replicate <- NULL
###
### keep repeatable occurrences
###
min_replicate_number <- 2
read_count_df <- FilterMinReplicateNumber(read_count_df, min_replicate_number, write_csv=T, outdir = outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterMinReplicateNumber", params=min_replicate_number)
###
### FilerPCRerror
###
pcr_error_var_prop <- 0.1
max_mismatch <- 1
by_sample <- T
sample_prop <- 0.8
read_count_df <- FilterPCRerror(read_count_df, write_csv=T, outdir=outdir, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch, by_sample=by_sample, sample_prop=sample_prop, sep=sep)
params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilerPCRerror", params=params)
###
### FilterChimera
###
abskew=2
by_sample = T
sample_prop = 0.8
read_count_df <- FilterChimera(read_count_df, write_csv=T, outdir=outdir, vsearch_path=vsearch_path, by_sample=by_sample, sample_prop=sample_prop, abskew=abskew, sep=sep)
params <- paste(abskew, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilterChimera", params=params)
###
### FilerRenkonen
###
# Renkonen index:
# PS = summ(min(p1i, p2i))
# p1i = number of reads for variant i in replicate 1 / number of reads in replicate 1
renkonen_distance_quantile = 0.9
read_count_df <- FilterRenkonen(read_count_df, write_csv=T, outdir=outdir, renkonen_distance_quantile=renkonen_distance_quantile, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerRenkonen", params=renkonen_distance_quantile)
###
### FilerIndel
###
read_count_df <- FilterIndel(read_count_df, write_csv=T, outdir=outdir, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterIndel")
###
### FilerCodonStop
###
genetic_code = 5
read_count_df <- FilterCodonStop(read_count_df, write_csv=T, outdir=outdir, genetic_code=genetic_code, sep=sep)
stat_df <- get_stat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
###
### PoolReplicates
###
digits = 0
read_count_samples_df <- PoolReplicates(read_count_df, digits=digits, write_csv=T, outdir=outdir, sep=sep)
###
### TaxAssign
###
asv_tax <- TaxAssign(df=read_count_samples_df, ltg_params_df=ltg_params_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outdir=outdir, num_threads=num_threads)
# write the list of ASV and their taxonomic assignment
write.csv(asv_tax, file = paste(outdir, "taxa.csv", sep=""), row.names = F)
###
### print output files
###
# write sequence and variant counts after each step
write.csv(stat_df, file = paste(outdir, "count_stat.csv", sep=""))
# long format, each line corresponds to an occurrence ()
write.csv(read_count_samples_df, file = paste(outdir, "Final_asvtable_long.csv", sep=""), row.names=F)
# wide format (ASV table), samples are in columns, ASVs in lines
outfile=paste(outdir, "Final_asvtable.csv", sep="")
write_asvtable(read_count_samples_df, outfile=outfile, fileinfo=fileinfo, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition, sep=sep)
# write ASV table completed by taxonomic assignments
outfile=paste(outdir, "Final_asvtable_with_taxassign.csv", sep="")
write_asvtable(read_count_samples_df, outfile=outfile, asv_tax=asv_tax, fileinfo=fileinfo, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition, sep=sep)
View(stat_df)
# start optimize from almost unfiltered data (after eliminating ASV with low global reads count)
#LFN_global_read_count_out <- paste(outdir, "LFN_global_read_count.csv", sep="")
LFN_global_read_count_out <- read_count_df_swarm_by_sample
optimize_read_count_df <- read.csv(LFN_global_read_count_out, sep=sep)
# start optimize from almost unfiltered data (after eliminating ASV with low global reads count)
#LFN_global_read_count_out <- paste(outdir, "LFN_global_read_count.csv", sep="")
optimize_read_count_df <- read_count_df_swarm_by_sample
#optimize_read_count_df <- read.csv(LFN_global_read_count_out, sep=sep)
dim(optimize_read_count_df)
###
### OptimizePCRError
###
optimize_dir = paste(outdir, "optimize", sep="")
OptimizePCRError(optimize_read_count_df, mock_composition=mock_composition, sep=sep, outdir=optimize_dir, max_mismatch=1, min_read_count=10)
read_count_df <- optimize_read_count_df
max_mismatch=2
###
### OptimizeLFNsampleReplicate
###
OptimizeLFNsampleReplicate(optimize_read_count_df, mock_composition=mock_composition, sep=sep, outdir=optimize_dir)
# start optimize from almost unfiltered data (after eliminating ASV with low global reads count)
#LFN_global_read_count_out <- paste(outdir, "LFN_global_read_count.csv", sep="")
optimize_read_count_df <- read_count_df_swarm_by_sample
###
### OptimizeLFNsampleReplicate
###
OptimizeLFNsampleReplicate(optimize_read_count_df, mock_composition=mock_composition, sep=sep, outdir=optimize_dir)
mock_composition
out = paste(outdir, "OptimizeLFNsampleReplicate.csv", sep="")
out
# read the mock composition file and keep only lines with keep
mock_composition_df <- read.csv(mock_composition, header=T, sep=sep) %>%
filter(action=="keep")
View(mock_composition_df)
# get a complete and unique list of plate, marker, sample, replicate
sample_replicate_list <- read_count_df %>%
select(plate, marker, sample, replicate) %>%
unique
sample_replicate_list
View(sample_replicate_list)
read_count_df <- optimize_read_count_df
# check outdir and make tmp dir
outdir <- check_dir(outdir)
out = paste(outdir, "OptimizeLFNsampleReplicate.csv", sep="")
# read the mock composition file and keep only lines with keep
mock_composition_df <- read.csv(mock_composition, header=T, sep=sep) %>%
filter(action=="keep")
View(mock_composition_df)
# get a complete and unique list of plate, marker, sample, replicate
sample_replicate_list <- read_count_df %>%
select(plate, marker, sample, replicate) %>%
unique
View(sample_replicate_list)
mock_composition_df <- left_join(mock_composition_df, sample_replicate_list, by=c("plate", "marker", "sample"), relationship = "many-to-many")
unique_asv_keep <-  unique(mock_composition_df$asv)
unique_asv_keep
read_count_df <- optimize_read_count_df
# check outdir and make tmp dir
outdir <- check_dir(outdir)
out = paste(outdir, "OptimizeLFNsampleReplicate.csv", sep="")
# read the mock composition file and keep only lines with keep
mock_composition_df <- read.csv(mock_composition, header=T, sep=sep) %>%
filter(action=="keep")
View(mock_composition_df)
# get a complete and unique list of plate, marker, sample, replicate
sample_replicate_list <- read_count_df %>%
select(plate, marker, sample, replicate) %>%
unique
View(sample_replicate_list)
sample_replicate_list <- read_count_df %>%
select(plate, marker, sample, replicate)
View(read_count_df)
sample_replicate_list <- read_count_df %>%
select(plate, marker, sample, replicate)
View(sample_replicate_list)
# get a complete and unique list of plate, marker, sample, replicate
sample_replicate_list <- read_count_df %>%
#    select(plate, marker, sample, replicate) %>%
select(-asv, -read_count) %>%
unique
sample_replicate_list <- read_count_df
View(sample_replicate_list)
sample_replicate_list <- sample_replicate_list %>%
select(plate, marker, sample, replicate)
detach("package:tidyr", unload = TRUE)
sample_replicate_list <- sample_replicate_list %>%
select(plate, marker, sample, replicate)
library(tidyr)
# get a complete and unique list of plate, marker, sample, replicate
sample_replicate_list <- read_count_df %>%
ungroup() %>%
select(plate, marker, sample, replicate) %>%
unique
View(sample_replicate_list)
mock_composition_df <- left_join(mock_composition_df, sample_replicate_list, by=c("plate", "marker", "sample"), relationship = "many-to-many")
unique_asv_keep <-  unique(mock_composition_df$asv)
unique_asv_keep
# get the total number of reads for each sample replicate
sample_replicate_rc <- read_count_df %>%
group_by(plate, marker, sample, replicate) %>%
summarize(read_count_sample_replicate= sum(read_count), .groups="drop_last") %>%
filter(sample %in% mock_composition_df$sample)
View(sample_replicate_rc)
# sum read_counts over replicates
asv_keep_df <- left_join(mock_composition_df, read_count_df, by=c("plate", "marker", "sample", "replicate", "asv"))
asv_keep_df <- left_join(asv_keep_df, sample_replicate_rc, by=c("plate", "marker", "sample", "replicate"))
asv_keep_df$lfn_variant_replicate <- asv_keep_df$read_count/asv_keep_df$read_count_sample_replicate
asv_keep_df$lfn_variant_replicate <- round(asv_keep_df$lfn_variant_replicate-0.00005, digits=4)
asv_keep_df <- asv_keep_df %>%
arrange(lfn_variant_replicate) %>%
select(plate, marker, sample, replicate, everything())
write.table(asv_keep_df, file=out, sep=sep, row.names = F)
out
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
###
### OptimizeLFNsampleReplicate
###
OptimizeLFNsampleReplicate(optimize_read_count_df, mock_composition=mock_composition, sep=sep, outdir=optimize_dir)
###
### Make known occurrences
###
known_occurrences <- paste(outdir, "known_occurrences.csv", sep= "")
missing_occurrences <- paste(outdir, "missing_occurrences.csv", sep= "")
habitat_proportion= 0.5 # for each asv, if the proportion of reads in a habitat is below this cutoff, is is considered as an artifact in all samples of the habitat
make_known_occurrences(read_count_samples_df, fileinfo=fileinfo, mock_composition=mock_composition, sep=sep, out=known_occurrences, missing_occurrences=missing_occurrences, habitat_proportion=habitat_proportion)
lfn_sample_replicate_cutoff=0.004
pcr_error_var_prop=0.1
max_mismatch=1
sample_prop=0.8
by_sample=T
min_replicate_number=2
###
### OptimizeLFNReaCountAndLFNvariant
###
min_replicate_number=2
lfn_sample_replicate_cutoff=0.004
pcr_error_var_prop=0.1
min_lfn_read_count_cutoff=10
max_lfn_read_count_cutoff=100
increment_lfn_read_count_cutoff=5
min_lnf_variant_cutoff=0.001
max_lnf_variant_cutoff=0.05
increment_lnf_variant_cutoff=0.001
by_replicate=FALSE
vsearch_path=""
max_mismatch=1
by_sample=T
sample_prop=0.8
min_replicate_number=2
OptimizeLFNReadCountAndLFNvariant(optimize_read_count_df, known_occurrences=known_occurrences, sep=sep, outdir=optimize_dir, min_lfn_read_count_cutoff=lfn_read_count_cutoff, max_lfn_read_count_cutoff=max_lfn_read_count_cutoff, increment_lfn_read_count_cutoff=increment_lfn_read_count_cutoff, min_lnf_variant_cutoff=min_lnf_variant_cutoff, max_lnf_variant_cutoff=max_lnf_variant_cutoff, increment_lnf_variant_cutoff=increment_lnf_variant_cutoff, by_replicate=by_replicate, lfn_sample_replicate_cutoff=lfn_sample_replicate_cutoff, pcr_error_var_prop=pcr_error_var_prop, vsearch_path=vsearch_path, max_mismatch=max_mismatch, by_sample=by_sample, sample_prop=sample_prop, min_replicate_number=min_replicate_number)
`rlang::last_trace()`
rlang::last_trace()
read_count_df <- optimize_read_count_df
#  read_count_df <- optimize_read_count_df
outdir <- check_dir(outdir)
out = paste(outdir, "OptimizeLFNReadCountAndLFNvariant.csv", sep="")
# read known occurrences
known_occurrences_df <- read.csv(known_occurrences, header=T, sep=sep)
View(known_occurrences_df)
# filter by sample-replicate
df <- LFN_sample_replicate(read_count_df, lfn_sample_replicate_cutoff, write_csv=F, outdir = outdir, sep=sep)
# FilterPCRerror
df <- FilterPCRerror(df, write_csv=F, outdir=outdir, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch, by_sample=by_sample, sample_prop=sample_prop, sep=sep)
# FilterMinReplicateNumber
df <- FilterMinReplicateNumber(df, min_replicate_number, write_csv=F, outdir = outdir, sep=sep)
read_count_df <- df
# read_count_df <- df
# add a temporary column with asv and sample concatenated
read_count_df$tmp <- paste(read_count_df$asv,  read_count_df$sample, sep="-")
View(read_count_df)
# make a df_tmp containing the number of replicates for each asv-sample combination
df_tmp <- read_count_df  %>%
group_by(tmp) %>%
summarize(repl_number=length(tmp))  %>%
filter(repl_number >= cutoff)
cutoff=2
# make a df_tmp containing the number of replicates for each asv-sample combination
df_tmp <- read_count_df  %>%
group_by(tmp) %>%
summarize(repl_number=length(tmp))  %>%
filter(repl_number >= cutoff)
View(df_tmp)
View(df_tmp)
# keep only asv-sample if present at least in min_replicate_number replicates
read_count_df <- filter(read_count_df, (read_count_df$tmp %in% df_tmp$tmp))
# start optimize from almost unfiltered data (after eliminating ASV with low global reads count)
#LFN_global_read_count_out <- paste(outdir, "LFN_global_read_count.csv", sep="")
optimize_read_count_df <- read_count_df_swarm_by_sample %>%
ungroup()
OptimizeLFNReadCountAndLFNvariant(optimize_read_count_df, known_occurrences=known_occurrences, sep=sep, outdir=optimize_dir, min_lfn_read_count_cutoff=lfn_read_count_cutoff, max_lfn_read_count_cutoff=max_lfn_read_count_cutoff, increment_lfn_read_count_cutoff=increment_lfn_read_count_cutoff, min_lnf_variant_cutoff=min_lnf_variant_cutoff, max_lnf_variant_cutoff=max_lnf_variant_cutoff, increment_lnf_variant_cutoff=increment_lnf_variant_cutoff, by_replicate=by_replicate, lfn_sample_replicate_cutoff=lfn_sample_replicate_cutoff, pcr_error_var_prop=pcr_error_var_prop, vsearch_path=vsearch_path, max_mismatch=max_mismatch, by_sample=by_sample, sample_prop=sample_prop, min_replicate_number=min_replicate_number)
