}
###
# Pool asv o the same cluster
###
df <- left_join(df, cent, by=c("asv"))
if(mean_over_markers){
df <- df %>%
group_by(centroid_seq, sample) %>%
summarize(mean_read_count=round(mean(mean_read_count), digits=0), .groups = NULL)
}else{
df <- df %>%
group_by(centroid_seq, sample) %>%
summarize("sum_read_count"=sum(mean_read_count), .groups =  "keep" )
}
df <- df %>%
group_by(centroid_seq, sample) %>%
summarize(mean_read_count=round(mean(mean_read_count), digits=0), .groups = NULL)
###
# pool all data into one data frame (df), check if the all marker.sample combinations are unique among different datasets
###
df <- data.frame("asv" = list(), "sample" = list(), "mean_read_count" = list(), "marker"== list())
samples <- c()
for(i in 1:nrow(files)){
file <- files[i, "file"]
marker <- files[i, "marker"]
#    print(file)
tmp <- read.csv(file, sep=sep) %>%
select(asv, sample, mean_read_count)
tmp$marker <- rep(marker, nrow(tmp)) # add maker
# make a list of marker.sample of the data set that just have been read
local_samples <- unique(paste(tmp$marker, tmp$sample, sep="."))
shared_samples <- intersect(local_samples, samples) # see if they matche earlier read marker.sample combnations
if(length(shared_samples) > 0){
print("The following samples are in at least 2 different datasets of the same marker. Their read_counts will be summed. Use unique names if you want to keep them separate:")
print(shared_samples)
}
samples <- c(samples, local_samples)
# add data set to df
df <- rbind(df, tmp)
}
df <- df %>%
group_by(centroid_seq, sample) %>%
summarize(mean_read_count=round(mean(mean_read_count), digits=0), .groups =  "keep")
###
# pool all data into one data frame (df), check if the all marker.sample combinations are unique among different datasets
###
df <- data.frame("asv" = list(), "sample" = list(), "mean_read_count" = list(), "marker"== list())
samples <- c()
for(i in 1:nrow(files)){
file <- files[i, "file"]
marker <- files[i, "marker"]
#    print(file)
tmp <- read.csv(file, sep=sep) %>%
select(asv, sample, mean_read_count)
tmp$marker <- rep(marker, nrow(tmp)) # add maker
# make a list of marker.sample of the data set that just have been read
local_samples <- unique(paste(tmp$marker, tmp$sample, sep="."))
shared_samples <- intersect(local_samples, samples) # see if they matche earlier read marker.sample combnations
if(length(shared_samples) > 0){
print("The following samples are in at least 2 different datasets of the same marker. Their read_counts will be summed. Use unique names if you want to keep them separate:")
print(shared_samples)
}
samples <- c(samples, local_samples)
# add data set to df
df <- rbind(df, tmp)
}
###
# Pool ASVs identical on their overlapping region, if more than one marker
###
marker_list <- unique(df$marker)
if(length(marker_list) > 1){ # more than one marker => pool sequences identical in their corresponding region
# get full list of ASVs
asvs <- df %>%
group_by(asv) %>%
summarize("rc" = sum(mean_read_count))
# arrange ASVs by decreasing sequence order and add ids
asvs$length <- nchar(asvs$asv)
asvs <- asvs %>%
arrange(desc(length), desc(rc))
asvs$id <- rownames(asvs)
# make a fasta file
fasta <- paste(outdir, "vsearch_input.fasta", sep="")
writeLines(paste(">", asvs$id, "\n", asvs$asv, sep="" ), fasta)
# cluster using cluster_smallmem and 1 as identity limit
centroids_file <- paste(outdir, "consout.txt", sep="")
blastout_file <- paste(outdir, "blastout.tsv", sep="")  #query sequences are shorter than subjects => centroids are in the subjects column
vsearch_cmd <- paste(vsearch_path, "vsearch --cluster_smallmem ", fasta, " --consout ",centroids_file," --blast6out ", blastout_file," --id 1", sep="")
print(vsearch_cmd)
system(vsearch_cmd)
###
# Make  cent data frame with a complete list of ASVs and the centroid for each of them.
###
# read the ids of centoids, and get the list of centroids
cent <- read.table(centroids_file)
colnames(cent) <- c("centroid")
cent <- cent %>%
filter(grepl(">centroid=", centroid))
cent$centroid <- gsub(">centroid=", "", cent$centroid)
cent$nbseq <-   gsub(".+;seqs=", "", cent$centroid)
cent$centroid <- gsub(";.+", "", cent$centroid)
# add to centroid the seqid that are in the same cluster
blastout <- read.table(blastout_file) %>%
select(1,2)
colnames(blastout) <- c("query", "subject")
blastout$query <- as.character(blastout$query)
blastout$subject <- as.character(blastout$subject)
cent <- left_join(cent, blastout, by= c("centroid"="subject"))
# add centroid to query comlum for singletons
cent <- cent %>%
mutate(query = ifelse(is.na(query), centroid, query))
# add a line for each non-singleton centoid, wth cetoide in centroid and in query columns
added_lines <- cent %>%
filter(nbseq>1) %>%
mutate(query=centroid) %>%
unique # add just one lime per centoide, not several if many sequneces in cluster
cent<- rbind(cent, added_lines) %>%
arrange(centroid)
## replace ids by ASVs
cent <- left_join(cent, asvs, by=c("centroid"="id")) %>%
select(query, "centroid_seq"=asv)
cent <- left_join(cent, asvs, by=c("query"="id")) %>%
select(centroid_seq, asv)
###
# Pool asv o the same cluster
###
df <- left_join(df, cent, by=c("asv"))
if(mean_over_markers){
df <- df %>%
group_by(centroid_seq, sample) %>%
summarize(mean_read_count=round(mean(mean_read_count), digits=0), .groups =  "keep")
}else{
df <- df %>%
group_by(centroid_seq, sample) %>%
summarize("sum_read_count"=sum(mean_read_count), .groups =  "keep" )
}
}else{# one marker
df <- df %>%
select(asv, sample, mean_read_count)
}
mean_over_markers=T
pool_datasets <- function(files, outdir="", sep=",", mean_over_markers=T){
###
# pool all data into one data frame (df), check if the all marker.sample combinations are unique among different datasets
###
df <- data.frame("asv" = list(), "sample" = list(), "mean_read_count" = list(), "marker"== list())
samples <- c()
for(i in 1:nrow(files)){
file <- files[i, "file"]
marker <- files[i, "marker"]
#    print(file)
tmp <- read.csv(file, sep=sep) %>%
select(asv, sample, mean_read_count)
tmp$marker <- rep(marker, nrow(tmp)) # add maker
# make a list of marker.sample of the data set that just have been read
local_samples <- unique(paste(tmp$marker, tmp$sample, sep="."))
shared_samples <- intersect(local_samples, samples) # see if they matche earlier read marker.sample combnations
if(length(shared_samples) > 0){
print("The following samples are in at least 2 different datasets of the same marker. Their read_counts will be summed. Use unique names if you want to keep them separate:")
print(shared_samples)
}
samples <- c(samples, local_samples)
# add data set to df
df <- rbind(df, tmp)
}
###
# Pool ASVs identical on their overlapping region, if more than one marker
###
marker_list <- unique(df$marker)
if(length(marker_list) > 1){ # more than one marker => pool sequences identical in their corresponding region
# get full list of ASVs
asvs <- df %>%
group_by(asv) %>%
summarize("rc" = sum(mean_read_count))
# arrange ASVs by decreasing sequence order and add ids
asvs$length <- nchar(asvs$asv)
asvs <- asvs %>%
arrange(desc(length), desc(rc))
asvs$id <- rownames(asvs)
# make a fasta file
fasta <- paste(outdir, "vsearch_input.fasta", sep="")
writeLines(paste(">", asvs$id, "\n", asvs$asv, sep="" ), fasta)
# cluster using cluster_smallmem and 1 as identity limit
centroids_file <- paste(outdir, "consout.txt", sep="")
blastout_file <- paste(outdir, "blastout.tsv", sep="")  #query sequences are shorter than subjects => centroids are in the subjects column
vsearch_cmd <- paste(vsearch_path, "vsearch --cluster_smallmem ", fasta, " --consout ",centroids_file," --blast6out ", blastout_file," --id 1", sep="")
print(vsearch_cmd)
system(vsearch_cmd)
###
# Make  cent data frame with a complete list of ASVs and the centroid for each of them.
###
# read the ids of centoids, and get the list of centroids
cent <- read.table(centroids_file)
colnames(cent) <- c("centroid")
cent <- cent %>%
filter(grepl(">centroid=", centroid))
cent$centroid <- gsub(">centroid=", "", cent$centroid)
cent$nbseq <-   gsub(".+;seqs=", "", cent$centroid)
cent$centroid <- gsub(";.+", "", cent$centroid)
# add to centroid the seqid that are in the same cluster
blastout <- read.table(blastout_file) %>%
select(1,2)
colnames(blastout) <- c("query", "subject")
blastout$query <- as.character(blastout$query)
blastout$subject <- as.character(blastout$subject)
cent <- left_join(cent, blastout, by= c("centroid"="subject"))
# add centroid to query comlum for singletons
cent <- cent %>%
mutate(query = ifelse(is.na(query), centroid, query))
# add a line for each non-singleton centoid, wth cetoide in centroid and in query columns
added_lines <- cent %>%
filter(nbseq>1) %>%
mutate(query=centroid) %>%
unique # add just one lime per centoide, not several if many sequneces in cluster
cent<- rbind(cent, added_lines) %>%
arrange(centroid)
## replace ids by ASVs
cent <- left_join(cent, asvs, by=c("centroid"="id")) %>%
select(query, "centroid_seq"=asv)
cent <- left_join(cent, asvs, by=c("query"="id")) %>%
select(centroid_seq, asv)
###
# Pool asv o the same cluster
###
df <- left_join(df, cent, by=c("asv"))
if(mean_over_markers){
df <- df %>%
group_by(centroid_seq, sample) %>%
summarize(mean_read_count=round(mean(mean_read_count), digits=0), .groups =  "keep")
}else{
df <- df %>%
group_by(centroid_seq, sample) %>%
summarize("sum_read_count"=sum(mean_read_count), .groups =  "keep" )
}
}else{# one marker
df <- df %>%
select(asv, sample, mean_read_count)
}
return(df)
}
read_count_pool <- pool_datasets(files, outdir=outdir, sep=sep, mean_over_markers=T)
View(read_count_pool)
computer <- "Windows" # Bombyx/Endoume/Windows
computer <- "Windows" # Bombyx/Endoume/Windows
if(computer == "Bombyx"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
swarm_path <- ""
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_zfzr_gz.csv"
outdir <- "vtamR_test/out_zfzr/"
mock_composition <- "vtamR_test/data/mock_composition_zfzr_eu.csv"
#fastqdir <- "/home/meglecz/vtamR_large_files/fastq/"
#fastqinfo <- "/home/meglecz/vtamR_large_files/user_input/fastqinfo_mfzr.csv"
#outdir <- "/home/meglecz/vtamR_large_files/out/"
#mock_composition <- "local/user_input/mock_composition_mfzr_prerun.csv"
num_threads=8
compress = T
} else if (computer == "Endoume"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/emese/miniconda3/bin/"
vsearch_path = "/home/emese/miniconda3/bin/"
blast_path= "" # deactivate conda
swarm_path <- ""
db_path= "/home/emese/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "local/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out/"
num_threads=8
compress = T
}else if (computer == "Windows"){
vtam_dir <- "C:/Users/emese/vtamR/"
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
swarm_path <- "C:/swarm-3.1.4-win-x86_64/bin/"
db_path="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "C:/Users/emese/vtamR_private/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_zfzr_gz.csv"
outdir <- "vtamR_test/out_zfzr/"
mock_composition <- "vtamR_test/data/mock_composition_zfzr_eu.csv"
num_threads=4
compress = F
}
sep=";"
setwd(vtam_dir)
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
files <- data.frame(file=c("vtamR_test/out_mfzr/PoolReplicates.csv", "vtamR_test/out_zfzr/PoolReplicates.csv"),
marker=c("MFZR", "ZFZR"))
pool_datasets <- function(files, outdir="", sep=",", mean_over_markers=T){
###
# pool all data into one data frame (df), check if the all marker.sample combinations are unique among different datasets
###
df <- data.frame("asv" = list(), "sample" = list(), "mean_read_count" = list(), "marker"== list())
samples <- c()
for(i in 1:nrow(files)){
file <- files[i, "file"]
marker <- files[i, "marker"]
#    print(file)
tmp <- read.csv(file, sep=sep) %>%
select(asv, sample, mean_read_count)
tmp$marker <- rep(marker, nrow(tmp)) # add maker
# make a list of marker.sample of the data set that just have been read
local_samples <- unique(paste(tmp$marker, tmp$sample, sep="."))
shared_samples <- intersect(local_samples, samples) # see if they matche earlier read marker.sample combnations
if(length(shared_samples) > 0){
print("The following samples are in at least 2 different datasets of the same marker. Their read_counts will be summed. Use unique names if you want to keep them separate:")
print(shared_samples)
}
samples <- c(samples, local_samples)
# add data set to df
df <- rbind(df, tmp)
}
###
# Pool ASVs identical on their overlapping region, if more than one marker
###
marker_list <- unique(df$marker)
if(length(marker_list) > 1){ # more than one marker => pool sequences identical in their corresponding region
# get full list of ASVs
asvs <- df %>%
group_by(asv) %>%
summarize("rc" = sum(mean_read_count))
# arrange ASVs by decreasing sequence order and add ids
asvs$length <- nchar(asvs$asv)
asvs <- asvs %>%
arrange(desc(length), desc(rc))
asvs$id <- rownames(asvs)
# make a fasta file
fasta <- paste(outdir, "vsearch_input.fasta", sep="")
writeLines(paste(">", asvs$id, "\n", asvs$asv, sep="" ), fasta)
# cluster using cluster_smallmem and 1 as identity limit
centroids_file <- paste(outdir, "consout.txt", sep="")
blastout_file <- paste(outdir, "blastout.tsv", sep="")  #query sequences are shorter than subjects => centroids are in the subjects column
vsearch_cmd <- paste(vsearch_path, "vsearch --cluster_smallmem ", fasta, " --consout ",centroids_file," --blast6out ", blastout_file," --id 1", sep="")
print(vsearch_cmd)
system(vsearch_cmd)
###
# Make  cent data frame with a complete list of ASVs and the centroid for each of them.
###
# read the ids of centoids, and get the list of centroids
cent <- read.table(centroids_file)
colnames(cent) <- c("centroid")
cent <- cent %>%
filter(grepl(">centroid=", centroid))
cent$centroid <- gsub(">centroid=", "", cent$centroid)
cent$nbseq <-   gsub(".+;seqs=", "", cent$centroid)
cent$centroid <- gsub(";.+", "", cent$centroid)
# add to centroid the seqid that are in the same cluster
blastout <- read.table(blastout_file) %>%
select(1,2)
colnames(blastout) <- c("query", "subject")
blastout$query <- as.character(blastout$query)
blastout$subject <- as.character(blastout$subject)
cent <- left_join(cent, blastout, by= c("centroid"="subject"))
# add centroid to query comlum for singletons
cent <- cent %>%
mutate(query = ifelse(is.na(query), centroid, query))
# add a line for each non-singleton centoid, wth cetoide in centroid and in query columns
added_lines <- cent %>%
filter(nbseq>1) %>%
mutate(query=centroid) %>%
unique # add just one lime per centoide, not several if many sequneces in cluster
cent<- rbind(cent, added_lines) %>%
arrange(centroid)
## replace ids by ASVs
cent <- left_join(cent, asvs, by=c("centroid"="id")) %>%
select(query, "centroid_seq"=asv)
cent <- left_join(cent, asvs, by=c("query"="id")) %>%
select(centroid_seq, asv)
###
# Pool asv o the same cluster
###
df <- left_join(df, cent, by=c("asv"))
if(mean_over_markers){
df <- df %>%
group_by(centroid_seq, sample) %>%
summarize(mean_read_count=round(mean(mean_read_count), digits=0), .groups =  "keep")
}else{
df <- df %>%
group_by(centroid_seq, sample) %>%
summarize("sum_read_count"=sum(mean_read_count), .groups =  "keep" )
}
}else{# one marker
df <- df %>%
select(asv, sample, mean_read_count)
}
return(df)
}
read_count_pool <- pool_datasets(files, outdir=outdir, sep=sep, mean_over_markers=T)
files <- data.frame(file=c("vtamR_test/out_mfzr/PoolReplicates.csv"),
marker=c("MFZR"))
read_count_pool <- pool_datasets(files, outdir=outdir, sep=sep, mean_over_markers=T)
View(read_count_pool)
files <- data.frame(file=c("vtamR_test/out_mfzr/PoolReplicates.csv", "vtamR_test/out_zfzr/PoolReplicates.csv"),
marker=c("MFZR", "ZFZR"))
read_count_pool <- pool_datasets(files, outdir=outdir, sep=sep, mean_over_markers=T)
View(read_count_pool)
files <- data.frame(file=c("vtamR_test/out_mfzr/PoolReplicates.csv", "vtamR_test/out_zfzr/PoolReplicates.csv"),
marker=c("MFZR", "ZFZR"))
read_count_pool <- pool_datasets(files, outdir=outdir, sep=sep, mean_over_markers=T, write_csv=T)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
usethis::use_roxygen_md() # rebuild the help files ?
files <- data.frame(file=c("vtamR_test/out_mfzr/PoolReplicates.csv", "vtamR_test/out_zfzr/PoolReplicates.csv"),
marker=c("MFZR", "ZFZR"))
read_count_pool <- pool_datasets(files, outdir=outdir, sep=sep, mean_over_markers=T, write_csv=T)
computer <- "Windows" # Bombyx/Endoume/Windows
if(computer == "Bombyx"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
swarm_path <- ""
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_zfzr_gz.csv"
outdir <- "vtamR_test/out_zfzr/"
mock_composition <- "vtamR_test/data/mock_composition_zfzr_eu.csv"
#fastqdir <- "/home/meglecz/vtamR_large_files/fastq/"
#fastqinfo <- "/home/meglecz/vtamR_large_files/user_input/fastqinfo_mfzr.csv"
#outdir <- "/home/meglecz/vtamR_large_files/out/"
#mock_composition <- "local/user_input/mock_composition_mfzr_prerun.csv"
num_threads=8
compress = T
} else if (computer == "Endoume"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/emese/miniconda3/bin/"
vsearch_path = "/home/emese/miniconda3/bin/"
blast_path= "" # deactivate conda
swarm_path <- ""
db_path= "/home/emese/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "local/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out/"
num_threads=8
compress = T
}else if (computer == "Windows"){
vtam_dir <- "C:/Users/emese/vtamR/"
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
swarm_path <- "C:/swarm-3.1.4-win-x86_64/bin/"
db_path="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "C:/Users/emese/vtamR_private/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_zfzr_gz.csv"
outdir <- "vtamR_test/out_zfzr/"
mock_composition <- "vtamR_test/data/mock_composition_zfzr_eu.csv"
num_threads=4
compress = F
}
sep=";"
setwd(vtam_dir)
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files ?
files <- data.frame(file=c("vtamR_test/out_mfzr/PoolReplicates.csv", "vtamR_test/out_zfzr/PoolReplicates.csv"),
marker=c("MFZR", "ZFZR"))
read_count_pool <- pool_datasets(files, outdir=outdir, sep=sep, mean_over_markers=T, write_csv=T)
files <- data.frame(file=c("vtamR_test/out_mfzr/PoolReplicates.csv"),
marker=c("MFZR"))
read_count_pool <- pool_datasets(files, outdir=outdir, sep=sep, mean_over_markers=T, write_csv=T)
