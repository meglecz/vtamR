View(tmp2)
# group by asv and habitat and count the total number of reads for each habitat-asv combination
tmp <- occurrence_df %>%
group_by(habitat, asv) %>%
summarize(habitat_read_count=sum(mean_read_count), .groups="drop_last") %>%
filter(!is.na(habitat))
tmp2 <- tmp %>%
group_by(asv) %>%
summarize(nb_habitat=length(asv))
tmp2 <- tmp %>%
group_by(asv) %>%
summarize(nb_habitat=length(asv)) %>%
filter(nb_habitat>1)
# keep only selected asvs in tmp
tmp <- tmp[tmp$asv %in% tmp2$asv, ]
# group by asv and habitat and count the total number of reads for each habitat-asv combination
tmp <- occurrence_df %>%
group_by(habitat, asv) %>%
summarize(habitat_read_count=sum(mean_read_count), .groups="drop_last") %>%
filter(!is.na(habitat))
# get the total readcount for each asv in tmp
tmp3 <- tmp %>%
group_by(asv) %>%
summarize(sum_read_count = sum(habitat_read_count))
View(tmp3)
# group by asv and habitat and count the total number of reads for each habitat-asv combination
tmp <- occurrence_df %>%
group_by(habitat, asv) %>%
summarize(habitat_read_count=sum(mean_read_count), .groups="drop_last") %>%
filter(!is.na(habitat))
tmp2 <- tmp %>%
group_by(asv) %>%
summarize(nb_habitat=length(asv))
# keep only selected asvs in tmp
tmp <- tmp[tmp$asv %in% tmp2$asv, ]
# get the total readcount for each asv in tmp
tmp3 <- tmp %>%
group_by(asv) %>%
summarize(sum_read_count = sum(habitat_read_count))
# add total readcount of asv to tmp and keep only lines where the asv in a given habitat is less frequent than in the other habitats
tmp <- left_join(tmp, tmp3, by="asv")
tmp <- tmp[tmp$habitat_read_count/tmp$sum_read_count < 0.5, ]
# keep only pertinent columns in tmp and add hab_action column with "delete"
tmp <- tmp %>%
select(habitat, asv)
tmp$hab_action <- rep("delete", nrow(tmp))
occurrence_df <- left_join(occurrence_df, tmp, by=c("habitat", "asv"))
occurrence_df$action[which(occurrence_df$hab_action=="delete")] <- "delete"
occurrence_df <- occurrence_df %>%
select(-hab_action)
#' Flag FP occurrences in samples based on the habitat the samples are from.
#' All ASVs present in more than one habitat are checked.
#' For each of these ASVs, if the proportion of reads in a habitat is below a cutoff (habitat_proportion),
#' it is considered as an artifact in all samples of the habitat.
#'
#' @param occurrence_df data frame with the following variables: asv, plate, marker, sample, mean_read_count, sample_type, habitat, action
#' @param fileinfo_df csv file with columns: plate, marker, sample, sample_type(mock/negative/real), habitat
#' @param habitat_proportion For each of ASVs, if the proportion of reads in a habitat is below this cutoff it is considered as an artifact in all samples of the habitat.
#' @export
#'
flag_from_habitat <- function(occurrence_df, fileinfo_df, habitat_proportion=0.5){
# group by asv and habitat and count the total number of reads for each habitat-asv combination
tmp <- occurrence_df %>%
group_by(habitat, asv) %>%
summarize(habitat_read_count=sum(mean_read_count), .groups="drop_last") %>%
filter(!is.na(habitat))
# count the number of habitats for each asv and keep only the ones present in at least two different habitats
tmp2 <- tmp %>%
group_by(asv) %>%
summarize(nb_habitat=length(asv)) %>%
filter(nb_habitat>1)
# keep only selected asvs in tmp
tmp <- tmp[tmp$asv %in% tmp2$asv, ]
# get the total readcount for each asv in tmp
tmp3 <- tmp %>%
group_by(asv) %>%
summarize(sum_read_count = sum(habitat_read_count))
# add total readcount of asv to tmp and keep only lines where the asv in a given habitat is less frequent than in the other habitats
tmp <- left_join(tmp, tmp3, by="asv")
tmp <- tmp[tmp$habitat_read_count/tmp$sum_read_count < 0.5, ]
# keep only pertinent columns in tmp and add hab_action column with "delete"
tmp <- tmp %>%
select(habitat, asv)
tmp$hab_action <- rep("delete", nrow(tmp))
occurrence_df <- left_join(occurrence_df, tmp, by=c("habitat", "asv"))
occurrence_df$action[which(occurrence_df$hab_action=="delete")] <- "delete"
occurrence_df <- occurrence_df %>%
select(-hab_action)
return(occurrence_df)
}
# read info on samples types and keep only relevant info
fileinfo_df <- read.csv(fileinfo, header=T, sep=sep) %>%
select(sample, sample_type, habitat)
# get unique lines to avoid replicates
fileinfo_df <- unique(fileinfo_df)
# define data frame for known occurrences
occurrence_df <- read_count_samples_df
# add habitat and sample_type to occurrence_df
occurrence_df <- left_join(occurrence_df, fileinfo_df, by="sample")
# add action column
occurrence_df$action <- rep(NA, nrow(occurrence_df))
# flag occurrences in negative control samples as delete
occurrence_df$action[which(occurrence_df$sample_type=="negative")] <- "delete"
# flag all expected occurrences in mock samples as "keep", NA for tolerate, and delete for all others
occurrence_df <- flag_from_mock(occurrence_df, mock_composition, fileinfo_df, sep=sep)
# flag occurrences as delete with low read count in habitat, compared to the other habitats
occurrence_df <- flag_from_habitat(occurrence_df, fileinfo_df, habitat_proportion=habitat_proportion)
# keep only relevant columns and lines, sort data
occurrence_df <- occurrence_df %>%
select(sample,action,asv) %>%
filter(!is.na(action)) %>%
arrange(sample, action)
# define data frame for known occurrences
occurrence_df <- read_count_samples_df
# add habitat and sample_type to occurrence_df
occurrence_df <- left_join(occurrence_df, fileinfo_df, by="sample")
# add action column
occurrence_df$action <- rep(NA, nrow(occurrence_df))
# flag occurrences in negative control samples as delete
occurrence_df$action[which(occurrence_df$sample_type=="negative")] <- "delete"
# flag all expected occurrences in mock samples as "keep", NA for tolerate, and delete for all others
occurrence_df <- flag_from_mock(occurrence_df, mock_composition, fileinfo_df, sep=sep)
# flag occurrences as delete with low read count in habitat, compared to the other habitats
occurrence_df <- flag_from_habitat(occurrence_df, fileinfo_df, habitat_proportion=habitat_proportion)
# keep only relevant columns and lines, sort data
occurrence_df <- occurrence_df %>%
select(asv_id,sample,action,asv) %>%
filter(!is.na(action)) %>%
arrange(sample, action)
# define data frame for known occurrences
occurrence_df <- read_count_samples_df
# add habitat and sample_type to occurrence_df
occurrence_df <- left_join(occurrence_df, fileinfo_df, by="sample")
# add action column
occurrence_df$action <- rep(NA, nrow(occurrence_df))
# flag occurrences in negative control samples as delete
occurrence_df$action[which(occurrence_df$sample_type=="negative")] <- "delete"
# flag all expected occurrences in mock samples as "keep", NA for tolerate, and delete for all others
occurrence_df <- flag_from_mock(occurrence_df, mock_composition, fileinfo_df, sep=sep)
# flag occurrences as delete with low read count in habitat, compared to the other habitats
occurrence_df <- flag_from_habitat(occurrence_df, fileinfo_df, habitat_proportion=habitat_proportion)
# keep only relevant columns and lines, sort data
occurrence_df <- occurrence_df %>%
select(sample,action,asv_id,asv) %>%
filter(!is.na(action)) %>%
arrange(sample, action)
# write to outfile
write.table(occurrence_df, file=outfile, row.names = F, sep=sep)
outfile
# write to outfile
write.table(occurrence_df, file=known_occurrences, row.names = F, sep=sep)
# count the number of FP and expected TP
FP <- nrow(occurrence_df %>%
filter(action=="delete"))
TP <- nrow(occurrence_df %>%
filter(action=="keep"))
# read mock composition to a df
mock_comp <- read.csv(mock_composition, header=T, sep=sep) %>%
filter(action=="keep")
View(mock_comp)
# add mean_read_count to df from read_count_samples_df, and keep only if value is NA
df <- left_join(mock_comp, read_count_samples_df,  by=c("sample", "asv")) %>%
filter(is.na(mean_read_count)) %>%
select(-"mean_read_count")
View(df)
#' make_missing_occurrences
#'
#' Prepare a file that list all expected occurrences that are missing (False negatives)
#'
#' @param read_count_samples_df data frame with the following variables: asv, plate, marker, sample, read_count
#' @param mock_composition csv file with columns: sample, action (keep/tolerate), asv
#' @param sep separator used in csv files
#' @param out name of the output file
#' @export
#'
make_missing_occurrences <- function(read_count_samples_df, mock_composition="", sep=",", out=""){
# read mock composition to a df
mock_comp <- read.csv(mock_composition, header=T, sep=sep) %>%
filter(action=="keep")
# add mean_read_count to df from read_count_samples_df, and keep only if value is NA
df <- left_join(mock_comp, read_count_samples_df,  by=c("sample", "asv")) %>%
filter(is.na(mean_read_count)) %>%
select(-"mean_read_count")
# write to outfile
if(out != ""){
write.table(df, file=out, row.names = F, sep=sep)
}
FN <- nrow(df %>%
filter(action=="keep"))
return(FN)
}
# count the number of FN and write missing_occurrences, if filename is defined
FN <-make_missing_occurrences(read_count_samples_df, mock_composition=mock_composition, sep=sep, out=missing_occurrences)
# real TP is the expected occurrences - FN
TP <- TP - FN
count_df <- data.frame("TP" = c(TP),
"FP" = c(FP),
"FN" = c(FN))
return(count_df)
View(count_df)
make_known_occurrences <- function(read_count_samples_df, fileinfo="", mock_composition="", sep=",", known_occurrences="", missing_occurrences="", habitat_proportion=0.5){
# differs from original make_known_occurrences: return a DF with FP, FN, TP
# read info on samples types and keep only relevant info
fileinfo_df <- read.csv(fileinfo, header=T, sep=sep) %>%
select(sample, sample_type, habitat)
# get unique lines to avoid replicates
fileinfo_df <- unique(fileinfo_df)
# define data frame for known occurrences
occurrence_df <- read_count_samples_df
# add habitat and sample_type to occurrence_df
occurrence_df <- left_join(occurrence_df, fileinfo_df, by="sample")
# add action column
occurrence_df$action <- rep(NA, nrow(occurrence_df))
# flag occurrences in negative control samples as delete
occurrence_df$action[which(occurrence_df$sample_type=="negative")] <- "delete"
# flag all expected occurrences in mock samples as "keep", NA for tolerate, and delete for all others
occurrence_df <- flag_from_mock(occurrence_df, mock_composition, fileinfo_df, sep=sep)
# flag occurrences as delete with low read count in habitat, compared to the other habitats
occurrence_df <- flag_from_habitat(occurrence_df, fileinfo_df, habitat_proportion=habitat_proportion)
# keep only relevant columns and lines, sort data
occurrence_df <- occurrence_df %>%
select(sample,action,asv_id,asv) %>%
filter(!is.na(action)) %>%
arrange(sample, action)
# write to outfile
write.table(occurrence_df, file=known_occurrences, row.names = F, sep=sep)
# count the number of FP and expected TP
FP <- nrow(occurrence_df %>%
filter(action=="delete"))
TP <- nrow(occurrence_df %>%
filter(action=="keep"))
# count the number of FN and write missing_occurrences, if filename is defined
FN <-make_missing_occurrences(read_count_samples_df, mock_composition=mock_composition, sep=sep, out=missing_occurrences)
# real TP is the expected occurrences - FN
TP <- TP - FN
count_df <- data.frame("TP" = c(TP),
"FP" = c(FP),
"FN" = c(FN))
return(count_df)
}
###
### Make known occurrences
###
known_occurrences <- paste(outdir, "known_occurrences.csv", sep= "")
missing_occurrences <- paste(outdir, "missing_occurrences.csv", sep= "")
habitat_proportion= 0.5 # for each asv, if the proportion of reads in a habitat is below this cutoff, is is considered as an artifact in all samples of the habitat
make_known_occurrences(read_count_samples_df, fileinfo=fileinfo, mock_composition=mock_composition, sep=sep, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, habitat_proportion=habitat_proportion)
TP_df <- make_known_occurrences(read_count_samples_df, fileinfo=fileinfo, mock_composition=mock_composition, sep=sep, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, habitat_proportion=habitat_proportion)
#' make_missing_occurrences
#'
#' Prepare a file that list all expected occurrences that are missing (False negatives)
#'
#' @param read_count_samples_df data frame with the following variables: asv, plate, marker, sample, read_count
#' @param mock_composition csv file with columns: sample, action (keep/tolerate), asv
#' @param sep separator used in csv files
#' @param out name of the output file
#' @export
#'
make_missing_occurrences <- function(read_count_samples_df, mock_composition="", sep=",", out=""){
# read mock composition to a df
mock_comp <- read.csv(mock_composition, header=T, sep=sep) %>%
filter(action=="keep") %>%
select(-asv_id)
# add mean_read_count to df from read_count_samples_df, and keep only if value is NA
df <- left_join(mock_comp, read_count_samples_df,  by=c("sample", "asv")) %>%
filter(is.na(mean_read_count)) %>%
select(-"mean_read_count")
# write to outfile
if(out != ""){
write.table(df, file=out, row.names = F, sep=sep)
}
FN <- nrow(df %>%
filter(action=="keep"))
return(FN)
}
TP_df <- make_known_occurrences(read_count_samples_df, fileinfo=fileinfo, mock_composition=mock_composition, sep=sep, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, habitat_proportion=habitat_proportion)
View(TP_df)
computer <- "Bombyx" # Bombyx/Endoume/Windows
if(computer == "Bombyx"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
swarm_path <- ""
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_zfzr.csv"
outdir <- "vtamR_test/out_zfzr/"
mock_composition <- "vtamR_test/data/mock_composition_zfzr.csv"
asv_list <- "vtamR_test/data/asv_list_zfzr.csv"
#fastqdir <- "/home/meglecz/vtamR_large_files/fastq/"
#fastqinfo <- "/home/meglecz/vtamR_large_files/user_input/fastqinfo_mfzr.csv"
#outdir <- "/home/meglecz/vtamR_large_files/out/"
#mock_composition <- "local/user_input/mock_composition_mfzr_prerun.csv"
num_threads=8
compress = T
} else if (computer == "Endoume"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/emese/miniconda3/bin/"
vsearch_path = "/home/emese/miniconda3/bin/"
blast_path= "" # deactivate conda
swarm_path <- ""
db_path= "/home/emese/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "local/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out_zfzr/"
num_threads=8
compress = T
}else if (computer == "Windows"){
vtam_dir <- "C:/Users/emese/vtamR/"
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
swarm_path <- "C:/swarm-3.1.4-win-x86_64/bin/"
db_path="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "C:/Users/emese/vtamR_private/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_zfzr_gz.csv"
outdir <- "vtamR_test/out_zfzr/"
mock_composition <- "vtamR_test/data/mock_composition_zfzr_eu.csv"
num_threads=4
compress = F
}
sep=","
setwd(vtam_dir)
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files
# create the output directory and check the the slash at the end
outdir <- check_dir(dir=outdir)
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
o
sortedinfo_df <- read.csv(paste(sorted_dir, "sortedinfo.csv", sep =""), sep=sep)
# start optimize from almost unfiltered data (after eliminating ASV with low global reads count)
file <- paste(outdir, "2_Swarm.csv", sep="")
read_count_df <- read.csv(file, sep=sep)
dim(read_count_df)
###
### OptimizePCRError
###
outfile <- paste(outdir, "OptimizePCRError.csv", sep="")
OptimizePCRError_df <- OptimizePCRError(read_count_df, mock_composition=mock_composition, sep=sep, outfile=outfile, max_mismatch=1, min_read_count=10)
###
### OptimizeLFNsampleReplicate
###
outfile = paste(outdir, "OptimizeLFNsampleReplicate.csv", sep="")
OptimizeLFNsampleReplicate_df <- OptimizeLFNsampleReplicate(read_count_df, mock_composition=mock_composition, sep=sep, outfile=outfile)
###
### Make known occurrences
###
known_occurrences <- paste(outdir, "known_occurrences.csv", sep= "")
missing_occurrences <- paste(outdir, "missing_occurrences.csv", sep= "")
habitat_proportion= 0.5 # for each asv, if the proportion of reads in a habitat is below this cutoff, is is considered as an artifact in all samples of the habitat
TP_df <- make_known_occurrences(read_count_samples_df, fileinfo=fileinfo, mock_composition=mock_composition, sep=sep, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, habitat_proportion=habitat_proportion)
###
### Make known occurrences
###
fileinfo <- read.csv(paste(sorted_dir, "sortedinfo.csv", sep =""), sep=sep)
sorted_dir
###
### SortReads
###
sorted_dir <- paste(outdir, "sorted/", sep="")
###
### Make known occurrences
###
fileinfo <- read.csv(paste(sorted_dir, "sortedinfo.csv", sep =""), sep=sep)
known_occurrences <- paste(outdir, "known_occurrences.csv", sep= "")
missing_occurrences <- paste(outdir, "missing_occurrences.csv", sep= "")
habitat_proportion= 0.5 # for each asv, if the proportion of reads in a habitat is below this cutoff, is is considered as an artifact in all samples of the habitat
TP_df <- make_known_occurrences(read_count_samples_df, fileinfo=fileinfo, mock_composition=mock_composition, sep=sep, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, habitat_proportion=habitat_proportion)
missing_occurrences
TP_df <- make_known_occurrences(read_count_samples_df, fileinfo=fileinfo, mock_composition=mock_composition, sep=sep, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, habitat_proportion=habitat_proportion)
known_occurrences
missing_occurrences
fileinfo
###
### Make known occurrences
###
fileinfo <- paste(sorted_dir, "sortedinfo.csv", sep ="")
known_occurrences <- paste(outdir, "known_occurrences.csv", sep= "")
missing_occurrences <- paste(outdir, "missing_occurrences.csv", sep= "")
habitat_proportion= 0.5 # for each asv, if the proportion of reads in a habitat is below this cutoff, is is considered as an artifact in all samples of the habitat
TP_df <- make_known_occurrences(read_count_samples_df, fileinfo=fileinfo, mock_composition=mock_composition, sep=sep, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, habitat_proportion=habitat_proportion)
computer <- "Bombyx" # Bombyx/Endoume/Windows
if(computer == "Bombyx"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/" # bombyx
swarm_path <- ""
db_path="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_zfzr.csv"
outdir <- "vtamR_test/out_zfzr/"
mock_composition <- "vtamR_test/data/mock_composition_zfzr.csv"
asv_list <- "vtamR_test/data/asv_list_zfzr.csv"
#fastqdir <- "/home/meglecz/vtamR_large_files/fastq/"
#fastqinfo <- "/home/meglecz/vtamR_large_files/user_input/fastqinfo_mfzr.csv"
#outdir <- "/home/meglecz/vtamR_large_files/out/"
#mock_composition <- "local/user_input/mock_composition_mfzr_prerun.csv"
num_threads=8
compress = T
} else if (computer == "Endoume"){
vtam_dir <- "~/vtamR"
cutadapt_path="/home/emese/miniconda3/bin/"
vsearch_path = "/home/emese/miniconda3/bin/"
blast_path= "" # deactivate conda
swarm_path <- ""
db_path= "/home/emese/mkCOInr/COInr/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "local/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr_gz.csv"
outdir <- "vtamR_test/out_zfzr/"
num_threads=8
compress = T
}else if (computer == "Windows"){
vtam_dir <- "C:/Users/emese/vtamR/"
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
swarm_path <- "C:/swarm-3.1.4-win-x86_64/bin/"
db_path="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/"
#  fastqdir <- "C:/Users/emese/vtamR_private/fastq/"
fastqdir <- "vtamR_test/data/"
fastqinfo <- "vtamR_test/data/fastqinfo_zfzr_gz.csv"
outdir <- "vtamR_test/out_zfzr/"
mock_composition <- "vtamR_test/data/mock_composition_zfzr_eu.csv"
num_threads=4
compress = F
}
sep=","
setwd(vtam_dir)
taxonomy=paste(db_path, "COInr_for_vtam_taxonomy.tsv", sep="")
blast_db=paste(db_path, "COInr_for_vtam", sep="")
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c("species","species","species","genus","family","family"),
ltgres=c("species","species","species","species", "genus","genus")
)
ltg_params_df = data.frame( pid=c(100,97,95,90,85,80),
pcov=c(70,70,70,70,70,70),
phit=c(70,70,70,70,70,70),
taxn=c(1,1,2,3,4,4),
seqn=c(1,1,2,3,4,4),
refres=c(8,8,8,7,6,6),
ltgres=c(8,8,8,8,7,7)
)
#setwd("D:/vtamR")
# load local packages
load_all(".")
roxygenise() # Builds the help files
usethis::use_roxygen_md() # rebuild the help files
# create the output directory and check the the slash at the end
outdir <- check_dir(dir=outdir)
# Measure runtime using system.time()
start_time <- Sys.time()  # Record the start time
# define stat data frame that will be completed with counts after each step
stat_df <- data.frame(parameters=character(),
asv_count=integer(),
read_count=integer(),
sample_count=integer(),
sample_replicate_count=integer())
###
### SortReads
###
sorted_dir <- paste(outdir, "sorted/", sep="")
# start optimize from almost unfiltered data (after eliminating ASV with low global reads count)
file <- paste(outdir, "2_Swarm.csv", sep="")
read_count_df <- read.csv(file, sep=sep)
dim(read_count_df)
###
### Make known occurrences
###
file <- paste(outdir, "14_PoolReplicates.csv", sep="")
read_count_samples_df <- read.csv(file, sep=sep)
fileinfo <- paste(sorted_dir, "sortedinfo.csv", sep ="")
known_occurrences <- paste(outdir, "known_occurrences.csv", sep= "")
missing_occurrences <- paste(outdir, "missing_occurrences.csv", sep= "")
habitat_proportion= 0.5 # for each asv, if the proportion of reads in a habitat is below this cutoff, is is considered as an artifact in all samples of the habitat
TP_df <- make_known_occurrences(read_count_samples_df, fileinfo=fileinfo, mock_composition=mock_composition, sep=sep, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, habitat_proportion=habitat_proportion)
