---
title: "vtamR Documentation"
auhor: Emese Meglecz
date: "2024-05-23"
output:
  html_document:
    toc: true
    toc_float: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

## Summary

**vtamR** is a revised, completed version of [VTAM](https://onlinelibrary.wiley.com/doi/10.1111/1755-0998.13756) (Validation and Taxonomic Assignation of Metabarcoding Data) rewritten in R. It is a complete metabarcoding pipeline:

* Sequence analyses from raw fastq files of amplicon sequences till Amplicon Sequence Variant ([ASV](#glossary)) table of validated ASVs assigned to taxonomic groups.
* Handles technical or biological replicates of the same sample
* Uses positive and negative control samples to fine tune the filtering and reduce [false positive](#glossary) and [false negative](#glossary) occurrences.
* Can pool multiple data sets (results of earlier analyses)
* Can pool results from overlapping markers

**Novelties compared to VTAM:**

* As it is a series of R functions, vramR is highly adaptable to include/exclude and order different steps of the analyses
* Includes swarm for denoising
* Graphic options
* Include functions to get statistics of each filtering steps (read and variant count etc.)
* The notion of marker and run has been dropped to simplify the analyses


## Implementation

The heavy lifting of sequence analyses are done by the following third party programs. They should be installed on your computer:

* BLAST([Altschul et al., 1990](https://pubmed.ncbi.nlm.nih.gov/2231712/)) (v2.11.0+)
* vsearch ([Rognes et al., 2016](https://peerj.com/articles/2584/)) (v2.7.0)
* cutadapt([Martin, 2011](https://journal.embnet.org/index.php/embnetjournal/article/view/200/479)) (v4.0)
* swarm ([Mahé et al., 2015](https://peerj.com/articles/1420/)) (v2.1.12)

vtamR has been tested using the above mentioned versions, but it should work with later versions.
vtamR was tested on Windows and Linux, and should work in all operating systems.

The help is available for all vtamR function by `?function_name`.

For most functions, the input data can be either a data frame or a csv file and they return a data frame that is also written to a csv file if a file name is provided.

Follow the [installation guide](Installation_vtamR.html) for installing vtamR and its dependencies.

## Tutorial

### Getting started

**Load libraries and local packages**

```{r load_packages, warning = FALSE, message=FALSE, eval=TRUE}
library("devtools")
library("roxygen2")
library("seqinr")
library("dplyr")
library("tidyr")
library("ggplot2") 
load_all(".")
roxygenise()
usethis::use_roxygen_md()
```

**Set general parameters and working directory**

```{r set_path, eval=TRUE}
cutadapt_path <- "C:/Users/Public"
vsearch_path <- "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin"
blast_path <- "C:/Users/Public/blast-2.14.1+/bin/"
swarm_path <- "C:/Users/Public/swarm-3.1.4-win-x86_64/bin"
vtam_dir <- "C:/Users/emese/vtamR"
setwd(vtam_dir)
num_threads <- 4
sep <- ","
```

* Adapt the path to third party programs according to your installation. If the program is in your path, it can be empty (e.g. `cutadapt_path=""`)
* `num_threads` is the number of CPUs for multithreaded programs
* `sep` is the separator used in csv files

**TaxAssign reference data base**

You can download a ready to use, comprehensive COI database from [OSF](https://osf.io/vrfwz/) ([Meglécz, 2023](https://onlinelibrary.wiley.com/doi/10.1111/1755-0998.13756)). Set the path to the database and the accompanying taxonomy file.

```{r, eval=TRUE}
taxonomy <- "C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/COInr_for_vtam_taxonomy.tsv"
blast_db <- "C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/COInr_for_vtam"
```

* `taxonomy` CSV file with taxonomic information
* `blast_db` BLAST database
* Details are in [Reference database for taxonomic assignments section](#reference-database-for-taxonomic-assignments)

**Set input data**

```{r, eval=TRUE}
fastq_dir <- "vtamR_test/data/"
outdir <- "vtamR_test/out_zfzr/"
fastqinfo <- "vtamR_test/data/fastqinfo_zfzr.csv"
mock_composition <- "vtamR_test/data/mock_composition_zfzr.csv"
asv_list <- "vtamR_test/data/asv_list.csv"
```

The above file are included in the github repository.

* `fastq_dir` Directory containing the input fastq files.
* `outdir` Name of the output directory.
* [fastqinfo](#fastqinfo) CSV file with information on input files, primers, tags, samples.
* [mock_composition](#mock_composition) CSV file with expected ASVs in mock samples. Can be created later.
* [asv_list](#asv_list) CSV file with ASVs from earlier data sets. Optional.


**Check the format of input files**

The `CheckFileinfo` function tests if all obligatory columns are present, and makes some sanity checks. This can be helpful, since these files are produced by the users, and may contain errors difficult to spot by eye.

```{r}
CheckFileinfo(file=fastqinfo, dir=fastq_dir, file_type="fastqinfo")
CheckFileinfo(file=mock_composition, file_type="mock_composition")
CheckFileinfo(file=asv_list, file_type="asv_list")
```

### Merge - Demultiplex

I suppose that you have [installed third party programs](Installation_vtamR.html) and followed the [Getting started section](#getting-started).

According to your wetlab and sequencing protocol fastq files can contain one or more [sample-replicates](#glossary), and sequences may or may not contain [tags](#glossary) (for [demultiplexing](#glossary)) and primer sequences. In the following sections I show 3 different scenarios to obtain the [read_count_df](#read_count_df) data frame, which the input to the filtering steps.

#### One sample per fastq - no tag - no primer

Read pairs should be quality filtered merged and written to fasta format.
This can be done by the `Merge` function. 

See the help (`?Merge`) for setting the correct parameters for quality filtering.

* [fastqinfo](#fastqinfo) is either a csv file, or a data frame. The key information for `Merge` is the list of the fastq file pairs that should be merged. The `tag_fw`, `primer_fw`, `tag_rv`, `primer_rv` are irrelevant in this case, just fill them with `NA`.
* `fastq_dir` is the directory containing the input fastq files.
* [sortedinfo_df](#fastainfo) is the output of `Merge`. It is the updated version of fastqinfo, where fastq file names have been replaced by fasta file names and the read counts are included for each file. Since no demultiplexing and primer trimming is necessary, this data frame and the files listed in it are the input for `Dereplicate`.


```{r}
merged_dir <- paste(outdir, "merged", sep="")
sortedinfo_df <- Merge(fastqinfo, fastq_dir=fastq_dir, vsearch_path=vsearch_path, outdir=merged_dir)
```


#### One sample per fastq - primer - no tag

Read pairs should be quality filtered, merged and written to fasta format by `Merge` function as in the [previous section](#one-sample-per-fastq---no-tag---no-primer).

Then the `TrimPrimer` function will trim the primers from the reads. 
See the help (`?TrimPrimer`) for setting the correct parameters for primer trimming.

* [fastqinfo](#fastqinfo) Either a csv file, or a data frame.  The key information for `Merge` is the list of the fastq file pairs that should be merged. The `primer_fw`, `primer_rv` columns are irrelevant in this case, just fill them with `NA`.
* `fastq_dir` Directory containing the input fastq files.
* [fastainfo_df](#fastainfo) is the output of `Merge`. It is the updated version of fastqinfo, where fastq file names have been replaced by fasta file names.
* `fasta_dir` Directory containing he input fasta files for `TrimPrimer`. This directory is created by `Merge`.
* If `check_reverse` is TRUE, `TrimPrimer` checks the reverse complementary stand as well.
* `sortedinfo_df` is  updated version of fastainfo. Since no demultiplexing is necessary, this data frame and the files listed in it are the input for `Dereplicate`.

```{r}
# merge and quality filter
merged_dir <- paste(outdir, "merged/", sep="")
fastainfo_df <- Merge(fastqinfo, fastq_dir=fastq_dir, vsearch_path=vsearch_path, outdir=merged_dir)

# trim primers
sorted_dir <- paste(outdir, "sorted/", sep="")
sortedinfo_df <- TrimPrimer(fastainfo_df, fasta_dir=merged_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, check_reverse=T)
```

#### Several samples per fastq - tags - primers

Read pairs should be quality filtered, merged and written to fasta format as in the [previous sections](#one-sample-per-fastq---no-tag---no-primer).

Then the `SortReads` function will [demultiplex](#demultiplexing) the fasta files according to the [tag](#tag) combinations and [trim](#trimming) the primers from the reads. See the help (`?SortReads`) for setting the correct parameters for demultiplexing and primer trimming:

* [fastqinfo](#fastqinfo) Either a csv file, or a data frame. The key information for `Merge` is the list of the fastq file pairs that should be merged.
* `fastq_dir` Directory containing the input fastq files.
* [fastainfo_df](#fastainfo) is the output of `Merge`. It is the updated version of fastqinfo, where fastq file names have been replaced by fasta file names.
* `fasta_dir` Directory containing the input fasta files for `TrimPrimer`. This directory is created by `Merge`.
* If `check_reverse` is TRUE, `SortReads` checks the reverse complementary stand as well.
* `sortedinfo_df` is  updated version of fastainfo. This data frame and the files listed in it are the input of the `Dereplicate`.


```{r}
# merge and quality filter
merged_dir <- paste(outdir, "merged/", sep="")
fastainfo_df <- Merge(fastqinfo, fastq_dir=fastq_dir, vsearch_path=vsearch_path, outdir=merged_dir)

# demultiplex, trim tags and pimers
sorted_dir <- paste(outdir, "sorted/", sep="")
sortedinfo_df <- SortReads(fastainfo_df, fasta_dir=merged_dir, outdir=sorted_dir, check_reverse=TRUE, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path)
```


### Dereplicate

At this point, you should have one fasta file per [sample-replicate](#glossary) containing merged reads. Tags and primers have been removed from the reads.

The fasta files then should be [dereplicated](#glossary) and a numerical ID ([asv_id](#glossary)) assigned to each [ASV](#glossary). The output is a data frame ([read_count_df](#read_count_df)). This can also be written to a csv file if `outfile` is given. It is a good idea to write this file, since it is the starting point of data filtering. You can experiment with different filtering strategies starting from this file, without the need of re-running the long steps of [merging](#glossary) and [demultiplexing](#glossary) and [trimming](#glossary) reads.


If you want to keep the IDs of ASVs from earlier analyses, you can give a file containing earlier ASVs and their IDs ([asv_list](#asv_list)). The completed ASV list is written to `updated_asv_list` file.

When providing only the file name to `updated_asv_list`, but not to `asv_list`, the function will write the complete list of ASVs. This can be useful later when analyzing subsequent data sets to homogenize ASV IDs between data sets.


```{r}
outfile <- paste(outdir, "1_before_filter.csv", sep="")
updated_asv_list <- paste(outdir, "ASV_list_with_IDs.csv", sep="")

read_count_df <- Dereplicate(sortedinfo_df, dir=sorted_dir, outfile=outfile, asv_list=asv_list, updated_asv_list=updated_asv_list)
```


### Filter

vtamR has a large number of functions to filter out ASVs or occurrences of ASVs in a given sample-replicate, which are the results of technical or biological problems associated with metabarcoding. The different filters can be applied to the original data set and then the results pooled together by using the `PoolFilters` function (accepting only occurrences that pass all filters) or can be applied sequentially in the order defined according to your needs.

In this section, I give an example using most filtering options. It is up to you to construct your own pipeline.

Each function returns a data frame with the filtered output. You can also write the results to a csv file. This can be useful for tracking the presence/absence of some of the ASVs/samples throughout the analyses using the [HistoryBy](#historyby) and [SummarizeBy](#summarizeby) functions. I will start the names of the output files by a number to keep the order of the different steps and allow the [SummarizeBy](#summarizeby) function to make final statistics at each step of the analyses.

We can also follow the evolution of the number of reads and ASVs remaining in the data set after each filtering steps. Let's define a data frame (`stat_df`), that will contain this information, and we will complete it after each step. Then and add a line with information on the unfiltered data set.

* `params` String containing the major parameters of each filtering steps.
* `stage` String referring to the filtering step.

```{r}
stat_df <- data.frame(parameters=character(),
                      asv_count=integer(),
                      read_count=integer(),
                      sample_count=integer(),
                      sample_replicate_count=integer())

stat_df <- GetStat(read_count_df, stat_df, stage="Input", params=NA)
```

#### Denoising by swarm

Swarm ([Mahé et al., 2015](https://peerj.com/articles/1420/)) is a powerful and quick computer program to denoise the data set by clustering ASVs that are likely to be the result of sequencing errors to ASVs representing real biological sequences. It will considerably reduce the number of ASVs.

By default, vtamR uses 1 as swam's `d` and the fastidious algorithm. See `?Swarm` on how to change this.
It is possible to run `Swarm` separately for each sample (`by_sample=TRUE`; default) or at once for the whole data set. This second option is more efficient in reducing the number of ASVs but can cluster very similar real sequences appearing in different samples. If you are interested in intra-specific variability, I suggest `by_sample=TRUE` or skip this step.


```{r}
by_sample <- TRUE
outfile <- paste(outdir, "2_Swarm_by_sample.csv", sep="")

read_count_df <- Swarm(read_count_df, outfile=outfile, swarm_path=swarm_path, num_threads=num_threads, by_sample=by_sample)

stat_df <- GetStat(read_count_df, stat_df, stage="Swarm", params=by_sample)
```

#### LFNglobalReadCount

Although, swarm has reduced considerably the number of ASVs, there are still many ASVs with low read count. The LFNglobalReadCount function filter out all ASVs with total read count bellow a threshold.

Let's eliminate [singletons](#glossary):

```{r}
global_read_count_cutoff = 2
outfile <- paste(outdir, "3_LFNglobalReadCount.csv", sep="")

read_count_df <- LFNglobalReadCount(read_count_df, cutoff=global_read_count_cutoff, outfile=outfile)

stat_df <- GetStat(read_count_df, stat_df, stage="LFNglobalReadCount", params=global_read_count_cutoff)
```

#### FilterIndel

This filter is **applicable only for coding sequences**. The idea is that if the length of the ASV differs from the most frequent ASV length by a number that is not a multiple of 3, it is likely to be an erroneous sequence or a pseudo-gene.

```{r}
outfile <- paste(outdir, "4_FilterIndel.csv", sep="")

read_count_df <- FilterIndel(read_count_df, outfile=outfile)

stat_df <- GetStat(read_count_df, stat_df, stage="FilterIndel")
```


#### FilterCodonStop

This filter is **applicable only for coding sequences**. It checks the number of codon STOPs in all 3 reading frames, and eliminates ASVs with STOP in all three reading frames.

The numerical id of the genetic code can be chosen from [NCBI](https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi?chapter=cgencodes). By default, it is the invertebrate mitochondrial genetic code (5), since the STOP codons of this genetic codes are also STOP codons in almost all genetic codes.


```{r}
outfile <- paste(outdir, "5_FilterCodonStop.csv", sep="")
genetic_code = 5

read_count_df <- FilterCodonStop(read_count_df, outfile=outfile, genetic_code=genetic_code)

stat_df <- GetStat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
```

#### FilterChimera

This function will run the `uchime3_denovo` function implemented in vsearch. 
It can be run sample by sample (`by_sample=T`; default), or on the whole data set at once. 

FilterChimera will eliminate ASVs detected as chimeras when running on the whole dataset at once (`by_sample=F`). If `by_sample=T`, ASVs are eliminated if they have been identified as chimeras in at least `sample_prop` proportion of the samples where they are present.

* `abskew` A chimera must be at least `abskew` times less frequent that the parental ASVs.


```{r}
abskew=2
by_sample = T
sample_prop = 0.8
outfile <- paste(outdir, "6_FilterChimera.csv", sep="")

read_count_df <- FilterChimera(read_count_df, outfile=outfile, vsearch_path=vsearch_path, by_sample=by_sample, sample_prop=sample_prop, abskew=abskew)

params <- paste(abskew, by_sample, sample_prop, sep=";")
stat_df <- GetStat(read_count_df, stat_df, stage="FilterChimera", params=params)
```

#### FilterRenkonen

Let's eliminate aberrant replicates that are not similar to other replicates of the same sample. 

We will calculate the renkonen distances among all pairs of replicates within sample and plot them.

```{r}
# calculate renkonen distance among all replicates within sample 
renkonen_within_df <- MakeRenkonenDistances(read_count_df, compare_all=FALSE)
# density plot
renkonen_density_plot <- DensityPlot_RenkonenDistance(renkonen_within_df)
print(renkonen_density_plot)
# barplot
sortedinfo <- paste(sorted_dir, "sortedinfo.csv", sep="")
renkonen_barplot <- Barplot_RenkonenDistance(renkonen_within_df, sample_types=sortedinfo, x_axis_label_size=6)
print(renkonen_barplot)
```

In this example, we have only a few distances, so the density plot is not very relevant. The barplot indicates that the replicate 1 of the `tnegtag` sample (negative control) is distant from the other two replicates. Lets, establish the Renkonen cutoff value to 0.4. We will filter out replicates that have Renkonen distances above this cutoff to most other replicates of the sample. In this case, replicate 1 of the tnegtag sample will be eliminated.

```{r}
outfile <- paste(outdir, "7_FilterRenkonen.csv", sep="")
cutoff <- 0.4

read_count_df <- FilterRenkonen(read_count_df, outfile=outfile, cutoff=cutoff)

stat_df <- GetStat(read_count_df, stat_df, stage="FilerRenkonen", params=cutoff)
```

#### Make mock_composition file

If you do not know the exact amplicon sequences of the individuals in your [mock sample](#glossary), this is the time to find them among the ASVs of `read_count_df` and prepare the [mock_composition](#mock_composition) file. This will be necessary for optimizing the [Low Frequency Noise](#glossary) (LFN) and `FilterPCReror` filters, and evaluate precision and accuracy of the filtering.

**Assign taxa to ASVs**

See more details of taxonomic assignment [here](#taxassign).
```{r}
outfile <- paste(outdir, "ASV_taxa.csv", sep="")
asv_tax <- TaxAssign(asv=read_count_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, num_threads=num_threads, outfile=outfile)
```

**Pool replicates by sample**

See details of PoolReplicates [here](#poolreplicates).
```{r}
tmp_read_count_samples_df <- PoolReplicates(read_count_df, outfile=outfile, sep=sep)
```

**Make an ASV table with taxonomic assignments**

Make a data frame with ASVs and read counts in the [wide format](#glossary) and add the total number of reads for each ASV, the number of samples they are present and their taxonomic assignment. This format is easier to read for humans, than the `read_count_df`.

See details of `WriteASVtable` [here](#print-output).
```{r}
sortedinfo <- paste(sorted_dir, "sortedinfo.csv", sep ="")
tmp_asv_table <- WriteASVtable(tmp_read_count_samples_df, sortedinfo=sortedinfo, add_sums_by_asv=T, asv_tax=asv_tax)
```

If there are many samples it might be better to select only mock samples and pertinent columns.
```{r}
asv_tpos1 <- tmp_asv_table %>%
  select(tpos1, Total_number_of_reads, Number_of_samples, asv_id, phylum, class, order, family, genus, species, asv) %>%
  filter(tpos1 > 0) %>%
  arrange(desc(tpos1))
```

In this mock sample, there should be the following 6 species:

- *Caenis pusilla*
- *Rheocricotopus*
- *Phoxinus phoxinus*
- *Hydropsyche pellucidula*
- *Synorthocladius semivirens*
- *Baetis rhodani*

We can see that in spite of all the filtering we have done so far, there are still a lot of unexpected occurrences in this sample. Most of them have low read counts and could be filtered out by [Low Frequency Noise Filters](#glossary)


You can now pick the correct sequences of the expected ASVs in each mock and make the [mock_composition](#mock_composition) file.

#### FilterPCRerror

If you have done the denoising step by swarm, this function might be redundant. Let's see, if there are still potential PCR errors in mock samples. 

**OptimizePCRerror**

`OptimizePCRerror`, will find all highly similar ASV pairs (`max_mismatch=1` by default) within a mock sample, where one ASV is expected, and the other is not. Their `read_count` ratio is printed in the output file. 

The function considers only ASVs with more than `min_read_count` reads in the sample to avoid a ratio based on low read counts that are more influenced by stochastic events and will be probably filtered out anyway.

See details using the help: `?OptimizePCRerror`. 

```{r}
outfile <- paste(outdir, "OptimizePCRerror.csv", sep="")
OptimizePCRerror_df <- OptimizePCRerror(read_count_df, mock_composition=mock_composition, vsearch_path=vsearch_path, outfile=outfile, max_mismatch=2, min_read_count=5)
```

**FilterPCRerror**

It seems that Swarm has done a good job. The highest read count ratio is 0.013 in the output of `OptimizePCRerror`. This should be taken as a lower limit to `pcr_error_var_prop`. Let's use 0.05 for filtering out PCR errors sample by sample.

See the help for more detail `?FilterPCRerror`.

```{r}
pcr_error_var_prop <- 0.05
max_mismatch <- 2
outfile <- paste(outdir, "8_FilterPCRerror.csv", sep="")

read_count_df <- FilterPCRerror(read_count_df, outfile=outfile, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch)

params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sep=";")
stat_df <- GetStat(read_count_df, stat_df, stage="FilterPCRerror", params=params)
```

#### LFNsampleReplicate

`LFNsampleReplicate` will eliminate occurrences with very low read counts compared to the total number of reads in the sample-replicate. The default cutoff proportion is 0.001. 
We can have an idea of the maximum value of this cutoff, by examining the proportions of expected ASVs in the mock samples using the `OptimizeLFNsampleReplicate` function.

**OptimizeLFNsampleReplicate**

```{r}
outfile = paste(outdir, "OptimizeLFNsampleReplicate.csv", sep="")
OptimizeLFNsampleReplicate_df <- OptimizeLFNsampleReplicate(read_count=read_count_df, mock_composition=mock_composition, outfile=outfile)
```

The lowest proportion of read count of an expected ASV to the read count of its sample-relicate is 0.0049 (Sequence of *Caenis pusilla* in the replicate 3 of tpos1 mock sample). A cutoff values for `LFNsampleReplicate` lower than this will eliminate some of the expected occurrences and create false negatives. Therefore, we choose 0.004 as a cutoff.

**LFNsampleReplicate**

```{r}
lfn_sample_replicate_cutoff <- 0.004
outfile <- paste(outdir, "9_LFNsampleReplicate.csv", sep="")

read_count_df <- LFNsampleReplicate(read_count_df, cutoff=lfn_sample_replicate_cutoff, outfile=outfile)

stat_df <- GetStat(read_count_df, stat_df, stage="LFNsampleReplicate", params=lfn_sample_replicate_cutoff)
```

#### FilterMinReplicate 1

To ensure repeatability, we can accept occurrences if they are present in at least `min_replicate_number` replicates of the sample (2 by default).

```{r}
## LFNvariant
# Set parameter values
min_replicate_number <- 2
outfile <- paste(outdir, "10_FilterMinReplicate.csv", sep="")
# Run filter and get stats
read_count_df <- FilterMinReplicate(read_count_df, min_replicate_number, outfile=outfile)
stat_df <- GetStat(read_count_df, stat_df, stage="FilterMinReplicate", params=min_replicate_number)
```

#### LFNvariant and LFNreadCount

The `LFNvariant` filter will eliminate occurrences with very low read counts compared to the total number of reads of the ASV. The default cutoff proportion is 0.001. This filter is designed to filter out occurrences present in the data set due to tag-jump or light inter sample contamination.

The `LFNreadCount` filter simply eliminates occurrences with read counts bellow a `cutoff` (10 by default). 

To find the best cutoff values for these two filters, the `OptimizeLFNreadCountLFNvariant` we will count the number of [false positive](#glossary), [false negative](#glossary) and [true positive](#glossary) occurrences using a series of combination of the cutoff values of these two filters. To run this function, we need a [known_occurrences_df](#known_occurrences_df) data frame that lists all occurrences that are known to be a FP, TP.

**MakeKnownOccurrences**

The `MakeKnownOccurrences` function will identify [false positive](#glossary), [false negative](#glossary) and [true positive](#glossary) occurrences in controls samples (mock and negative). Some false positives can also be identified in real samples if samples of different habitats are included in the data sets. 

The false positive and the true positive occurrences are written to the `known_occurrences` data frame (or file), false negatives to the `missing_occurrences` data frame (or file), and a `performance_metrics` data frame is also produced with the count of these occurrences. For details see `?MakeKnownOccurrences`.

This function takes a `read_count_samples` data frame as an input, where the replicates of the same sample have been pooled (see `?PoolReplicates`)


```{r}
# Pool replicates
read_count_samples_df <- PoolReplicates(read_count_df)

# Detect known occurrences
results <- MakeKnownOccurrences(read_count_samples = read_count_samples_df, sortedinfo=sortedinfo, mock_composition=mock_composition)

# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
```


**OptimizeLFNreadCountLFNvariant**

The `LFNreadCount` and `LFNvariant` functions are run for a series of cutoff value combinations of the two filters, followed by `FilterMinReplicate`. For each parameter combination, the number of FN, TP, and FP is reported. Chose the parameter setting that minimizes, FN and FP.

Here we will use the default range of cutoffs to test, but you can set the minimum, the maximum and the increment for the cutoff values for both filters. (see `?OptimizeLFNreadCountLFNvariant`). We set `min_replicate_number` to 2, to eliminate non-repeatable occurrences among the three replicates of each sample.


```{r}
outfile = paste(outdir, "OptimizeLFNreadCountLFNvariant.csv", sep="")

OptimizeLFNreadCountLFNvariant_df <- OptimizeLFNreadCountLFNvariant(read_count_df, known_occurrences=known_occurrences_df, outfile= outfile, min_replicate_number=2)
```

**LFNvariant, LFNreadCount**

From the output, we choose 0.001 for the cutoff of `LFNvariant` and 10 for `LFNreadCount`, since this is the less stringent combination the keeps all expected occurrences (6 TP, no FN), and has less FP.

We will run the two filters on the same input data frame (for which the parameters has been optimized), and pool the results by accepting only occurrences that pass both filters by the `PoolFilters` function.

See `?LFNvariant, ?LFNreadCount` and `?PoolFilters` for details.

```{r}
## LFNvariant
# Set parameter values
lnf_variant_cutoff = 0.001
outfile <- paste(outdir, "11_LFNvariant.csv", sep="")
# Run filter and get stats
read_count_df_lnf_variant <- LFNvariant(read_count_df, cutoff=lnf_variant_cutoff, outfile=outfile)
stat_df <- GetStat(read_count_df_lnf_variant, stat_df, stage="LFNvariant", params=lnf_variant_cutoff)

## LFNreadCount
# Set parameter values
lfn_read_count_cutoff <- 10
outfile <- paste(outdir, "12_LFNreadCount.csv", sep="")
# Run filter and get stats
read_count_df_lfn_read_count <- LFNreadCount(read_count_df, cutoff=lfn_read_count_cutoff, outfile=outfile)
stat_df <- GetStat(read_count_df_lfn_read_count, stat_df, stage="LFNreadCount", params=lfn_read_count_cutoff)

## Combine results
# Set parameter values
outfile <- paste(outdir, "13_poolLFN.csv", sep="")
# Combine results and get stats
read_count_df <- PoolFilters(read_count_df_lfn_read_count, read_count_df_lnf_variant, outfile=outfile)
stat_df <- GetStat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
rm(read_count_df_lfn_read_count)
rm(read_count_df_lnf_variant)
```

#### FilterMinReplicate 2

Let's run again `FilterMinReplicate` to ensure repeatability among replicates of the sample (2 by default).

```{r}
## LFNvariant
# Set parameter values
min_replicate_number <- 2
outfile <- paste(outdir, "14_FilterMinReplicate.csv", sep="")
# Run filter and get stats
read_count_df <- FilterMinReplicate(read_count_df, min_replicate_number, outfile=outfile)
stat_df <- GetStat(read_count_df, stat_df, stage="FilterMinReplicate", params=min_replicate_number)
```

### Pool, TaxAssign and document

#### PoolReplicates

Let's pool replicates of the same sample. `PoolReplicates` function will take the mean non-zero read counts of each ASV over replicates of the same sample.

```{r}
# Set parameter values
outfile <- paste(outdir, "15_PoolReplicates.csv", sep="")
read_count_samples_df <- PoolReplicates(read_count_df, outfile=outfile)
# Run function and get stats
stat_df <- GetStat(read_count_samples_df, stat_df, stage="PoolReplicates")
```

#### Get performance metrics

Run `MakeKnownOccurrences` again to get performance metrics (FP, FN, FP). 
This time we will write the output data frames to files as well. The `performance_metrics` file will give you the count of FP, FN, TP, accuracy and sensitivity, you can find false negatives in `missing_occurrences`, and true and false positives in `known_occurrences`.

```{r}
# Set parameter values
missing_occurrences <- paste(outdir, "missing_occurrences.csv", sep= "")
performance_metrics <- paste(outdir, "performance_metrics.csv", sep= "")
known_occurrences <- paste(outdir, "known_occurrences.csv", sep= "")
sortedinfo <- paste(sorted_dir, "sortedinfo.csv", sep ="")
# Run function
results <- MakeKnownOccurrences(read_count_samples_df, sortedinfo=sortedinfo, mock_composition=mock_composition, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, performance_metrics=performance_metrics)
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
```

#### TaxAssign

If you haven't assigned ASV to taxa yet (before the optimization step), it is time to do it. If you have already done it, you can use the same output as previously (`asv_tax` data frame) and skip this step.

For the format of `taxonomy` and `blast_db` check the [Reference database for taxonomic assignments](#reference-database-for-taxonomic-assignments) section. See the brief description of the algorithm with `?TaxAssign`.


```{r}
# Set parameter values
outfile <- paste(outdir, "TaxAssign.csv", sep="")
# Run function
asv_tax <- TaxAssign(asv=read_count_samples_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outfile=outfile, num_threads=num_threads)
```

### Print output

`WriteASVtable` will reorganize the `read_count_samples_df` data frame, with samples in columns and ASV in lines (from [long format](#glossary) to [wide format](#glossary)).

It is possible to add supplementary informations as well:

* Taxonomic assignment (`asv_tax`)
* Total number of reads and samples for each ASV (`add_sums_by_asv`)
* Total number of reads and ASVs in each sample (`add_sums_by_sample`)
* Supplementary column for each mock sample with expected occurrences in each of them (`add_expected_asv`)
* Supplementary column for each sample that has been filtered out (`add_empty_samples`)

For more information see `?WriteAsVtable`

```{r}
# write ASV table completed by taxonomic assignments
outfile=paste(outdir, "Final_asvtable_with_TaxAssign.csv", sep="")

asv_table_df <- WriteASVtable(read_count_samples_df, outfile=outfile, asv_tax=asv_tax, sortedinfo=sortedinfo, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition)
```

Print out the number of reads, ASVs, samples and replicates after each step.

```{r}
write.csv(stat_df, file = paste(outdir, "stat_steps.csv", sep=""))
```

## Suppelmentary functions

### RandomSeq

Random select `n` sequences from each input fasta file. It can be used before or after demultiplexing (`SortReads`). However, if using `LFNvariant` and `LFNreadCount` which are partially based on the number of reads in negative controls, standardizing the number of reads among samples does not make sense. Thus I use `RandomSeq` after `Merge`, to get the same number of reads for each replicate series (same samples, different replicates in each fasta file).


* [fastainfo](#fastainfo) Either a csv file, or a data frame, with the following columns: tag_fw, primer_fw, tag_rv, primer_rv, sample, sample_type, habitat, replicate, fasta.
* `fasta_dir` Directory containing the input fasta files.
* `n` Number of sequences to be taken randomly (without replacement).


```{r}
randomseq_dir = paste(outdir, "random_seq/", sep="")
fastainfo_df <- RandomSeq(fastainfo_df, fasta_dir=merged_dir, outdir=randomseq_dir, vsearch_path=vsearch_path, n=10000)
```


###  HistoryBy

This function scans all files in the `dir` that starts by a number. (See file names of the output files of the different [filtering steps](#filter)). It will select all lines were the `feature` (asv_id/asv/sample/replicate) has a `value` we are looking for.

**Examples**

*Get the history of asv_id (`feature`) 27 (`value`).*
```{r}
tmp_ASV_27 <- HistoryBy(dir=outdir, feature="asv_id", value="27")
```

*Same search by using the sequence of the ASV  (`feature`).*
```{r}
tmp_replicate_1 <- HistoryBy(dir=outdir, feature="asv", value="CCTTTATTTTATTTTCGGTATCTGGTCAGGTCTCGTAGGATCATCACTTAGATTTATTATTCGAATAGAATTAAGAACTCCTGGTAGATTTATTGGCAACGACCAAATTTATAACGTAATTGTTACATCTCATGCATTTATTATAATTTTTTTTATAGTTATACCAATCATAATT")
```

*Get the history of the sample (`feature`) tpos1 (`value`).*
```{r}
tmp_sample_tpos1 <- HistoryBy(dir=outdir, feature="sample", value="tpos1")
```


### SummarizeBy

This function scans all files in the `dir` that starts by a number. See file names of the output files of the different [filtering steps](#filter)). 

It will group each file by a variable (asv/asv_id/sample/replicate) and summarize a `feature` (asv/asv_id/sample/replicate/read_count). 
If the `feature` is `read_count`, it will give the sum of the read counts for each value of the variable in each file. Otherwise, it returns the number of distinct values of the `feature` for each value of the variable in each file.

**Examples**

*Get the number of reads of each sample after each filtering steps.*

From this data frame, we can see that the negative control sample become "clean" after the LFN filters, and there is a considerable variation among the number of reads of different real samples.
```{r}
read_count_by_sample <- SummarizeBy(dir=outdir, feature="read_count", grouped_by="sample")
```


*Get the number asv for each sample after each filtering steps.*

From this data frame, we can see that the negative control sample become "clean" after the LFN filters, and the mock samples has 10 ASVs at the end.
```{r}
asv_by_sample <- SummarizeBy(dir=outdir, feature="asv", grouped_by="sample")
```


*Get the number asv_id for each replicate after each filtering steps.*

We can see that number of ASVs are comparable in different replicates.
```{r}
asvid_by_replicate <- SummarizeBy(dir=outdir, feature="asv_id", grouped_by="replicate")
```

### UpdateASVlist

Pools unique `asv` - `asv_id` combinations from the input data frame (`read_count_df`) and from the input file (`asv_list`).

The input file is typically a csv file containing ASVs seen in earlier data sets with their `asv_id`s.
If there is a conflict within or between the input data the function quits with a error message. Otherwise writes the `updated_asv_list` to the outfile.

The safest option of avoiding incoherence between `asv_ids` of earlier and present runs is to use the `asv_list` and `updated_asv_list` parameters in the `Dereplicate` function as it is done in this [tutorial](#dereplicate). This will synchronize the `asv_id`s in this run with earlier ones, and writes an updated file with all ASV from earlier and the present data set. In this case, the `UpdateASVlist` function is not necessary, since it is automatically called from `Dereplicate`. 

However, if you are analyzing a large number of very large data sets, the complete `asv_list` will grow quickly, and might cause memory issues. Most of the ASVs in this list are singletons, and filtered out during the analyses. Therefore, you can opt for homogenizing the `asv_id`s with earlier runs by the `Dereplicate` function, without writing an `updated_asv_list`. Then update the `asv_list` after the first steps of filtering (e.g. `Swarm`, `LFNglobalReadCount`) to keep only ASVs that are more frequent and more likely to appear in future data sets. It is still a quite safe option, and reduces greatly the number of ASVs kept in this file.


```{r}
updated_asv_list <- paste(outdir, "updated_ASV_list.csv", sep="")
UpdateASVlist(read_count_df, asv_list=asv_list, outfile=updated_asv_list)
```

### PoolDatasets

**More than one overlapping marker**

This function pools different data sets and it is particularly useful, if the results should be pooled from more than one overlapping markers. In that case, ASVs identical on their overlapping regions are pooled into groups, and different ASVs of the same group are represented by their centroid (longest ASV of the group). Pooling can take the mean read counts of the ASVs (`mean_over_markers=T`; default) or their sum (`mean_over_markers=F`).

The function takes several input CSV files, each in [long format](#glossary) containing `asv_id`, `sample`, `read_count` and `asv` columns. The file names should be organized in data frame, with the marker names for each file.

* `outfile` Name of the output CSV file with the pooled data set (`asv_id`, `sample`, `read_count`, `asv`). ASVs are grouped to the same line if identical in their overlapping region, and only the centroids appear in the `asv` column.
* `asv_with_centroids` Name of the output CSV file containing each of the the original ASVs (with samples, markers, and read_count) as well as their centroids.
* The data frame returned by the function corresponds to the `outfile`.

```{r}
files <- data.frame(file=c("vtamR_test/out_mfzr/15_PoolReplicates.csv", "vtamR_test/test/15_PoolReplicates_ZFZR.csv"),
                    marker=c("MFZR", "ZFZR"))

outfile <- paste(outdir, "Pooled_datasets.csv", sep="") 
asv_with_centroids <- paste(outdir, "Pooled_datasets_asv_with_centroids.csv", sep="") 

read_count_pool <- PoolDatasets(files, outfile=outfile, asv_with_centroids=asv_with_centroids, mean_over_markers=T, vsearch_path=vsearch_path)
```

**Only one marker**

Pooling the results of different data sets of the same marker is very simple. Basically, the input files (in [long format](#glossary) with `asv`, `asv_id`, `sample`, `read_count` columns) are concatenated. The `PoolDatasets` function also  checks if sample names are unique. If not, it sums the read count of the same sample and same ASV, but returns a warning.

The output file or data frame can be rearranged to [wide format](#glossary) by the [WriteASVtable](#print-output) function.

### CountReadsDir

Count the number of reads in `fasta` or `fastq` files found in the input directory.
Input files can be gz compressed or uncompressed, but zip files are not supported.

The [fastainfo](#fastainfo) and the [sortedinfo](#sortedinfo) files contain the number of reads after `Merge` or `SortReads`, so no need to run `CountReadsDir` separately. This function can be useful for counting the number of reads in the input `fastq` files.

* `dir` Input directory containing the fasta of fastq files
* `file_type` [fasta/fastq]
* `pattern` Regular expression; Check only files for `pattern` in the file name

```{r}
dir <- "vtamR_test/data"
df <- CountReadsDir(dir, pattern="_fw.fastq.gz", file_type="fastq")
```
 
### Barplot_ReadCountBySample

Make a bar plot of read counts by [sample](#glossary) (`sample_replicate=F`) or [sample-replicate](#glossary) (`sample_replicate=T`).
Can use different colors for different sample types ([real/mock/negative](#glossary))

* [read_count_df](#read_count_df) Input data frame
* `sample_types` data frame or CSV file containing info on sample types for each sample

```{r}
sortedinfo <- paste(sorted_dir, "sortedinfo.csv", sep ="")
Barplot_ReadCountBySample(read_count_df=read_count_df, sample_replicate=F, sample_types=sortedinfo)
```


### Histogram_ReadCountByVariant

Histogram of read counts by ASV

* [read_count_df](#read_count_df) Input data frame
* `min_read_count` Ignore variants with read count bellow this value
* `binwidth` Width of bins

```{r}
Histogram_ReadCountByVariant(read_count_df, min_read_count=10, binwidth=1000)
```


### Renkonen distances

Calculate the Renkonen distances among all replicates (compare_all=TRUE) or among replicates of the same sample (compare_all=FALSE) and plot them.

* [read_count_df](#read_count_df) Input data frame.
* `sample_types` Data frame or CSV file containing info on sample types for each sample.

```{r}
renkonen_within_df <- MakeRenkonenDistances(read_count_df, compare_all=FALSE)
Barplot_RenkonenDistance(renkonen_within_df, sample_types=sortedinfo)
DensityPlot_RenkonenDistance(renkonen_within_df)

renkonen_all_df <- MakeRenkonenDistances(read_count_df, compare_all=TRUE)
DensityPlot_RenkonenDistance(renkonen_all_df)
```

 

## I/O files and data frames

### fastqinfo

CSV file with information on input fastq files, primers, tags, samples with the following columns. Each line corresponds to a sample-replicate combination.

 * tag_fw: Sequence tag on the 5' of the fw read (NA if file is already demultiplexed)
 * primer_fw: Forward primer (NA if primer has been trimmed)
 * tag_rv: Sequence tag on the 3' of the rv read (NA if file is already demultiplexed)
 * primer_rv: Reverse primer (NA if primer has been trimmed)
 * sample: Name of the sample (alpha-numerical)
 * sample_type: [real/mock/negative](#glossary)
 * habitat: If real or mock samples are from different habitats that cannot contain the same type of organisms (e.g. terrestrial vs. marine), this information is used for detecting false positives. Use NA otherwise. Use NA for negative controls.
 * replicate: Numerical id of a replicate within sample (e.g. Sample1 can have replicate 1, 2 or 3)
 * fastq_fw: Forward fastq file
 * fastq_rv: Reverse fastq file


### fastainfo

CSV file with information on input fasta files, primers, tags, samples with the following columns. Each line corresponds to a sample-replicate combination.

 * tag_fw: Sequence tag on the 5' of the fw read (NA if file is already demultiplexed)
 * primer_fw: Forward primer (NA if primer has been trimmed)
 * tag_rv: Sequence tag on the 3' of the rv read (NA if file is already demultiplexed)
 * primer_rv: Reverse primer (NA if primer has been trimmed)
 * sample: Name of the sample (alpha-numerical)
 * sample_type: [real/mock/negative](#glossary)
 * habitat: If real or mock samples are from different habitats that cannot contain the same type of organisms (e.g. terrestrial vs. marine), this information is used for detecting false positives. Use NA otherwise. Use NA for negative controls.
 * replicate: Numerical id of a replicate (e.g. Sample1 can have replicate 1, 2 or 3)
 * fasta: Fasta file
 * Read_count: Number of reads in the fasta file. Optional.

### sortedinfo

CSV file with information on demultiplexed and primer trimmed fasta files and samples with the following columns. Each line corresponds to a sample-replicate combination.

 * sample: Name of the sample (alpha-numerical)
 * sample_type: [real/mock/negative](#glossary)
 * habitat: If real or mock samples are from different habitats that cannot contain the same type of organisms (e.g. terrestrial vs. marine), this information is used for detecting false positives. Use NA otherwise. Use NA for negative controls.
 * replicate: Numerical id of a replicate (e.g. Sample1 can have replicate 1, 2 or 3)
 * fasta: Fasta file
 * Read_count: Number of reads in the fasta file. Optional.

### mock_composition

CSV file with the following columns.

 * sample: Name of the [mock](#glossary) sample
 * action: 
      * keep: Expected ASV in the mock, that should be kept in the data set
      * tolerate: ASV that can be present in a mock, but it is not essential to keep it in the data set (e.g. badly amplified organism)
 * [asv](#glossary): sequence of the ASV
 * taxon: Optional; Name of the organism
 * [asv_id](#glossary): Optional; If there is a conflict between asv and asv_id, the asv_id is ignored

### known_occurrences
CSV file or data frame with the following columns.

 * [sample](#glossary): Name of the sample
 * action:
      * keep: Expected ASVs in a mock sample (corresponds to True Positives)
      * delete: False Positive occurrences: unexpected ASV in a mock sample; all occurrences in negative controls; occurrences in real samples corresponding to an incompatible habitats (e.g. an ASV mostly present in marine samples is unexpected in a freshwater sample)
 * [asv_id](#glossary)
 * [asv](#glossary): sequence of the ASV
 
### asv_list

This file lists ASVs and their IDs from earlier data sets. When provided (optional), identical ASVs in the present and earlier data sets have the same ID. New ASVs (not present in asv_list) will get unique IDs not present in asv_list. It is a CSV file with the following columns:

 * [asv_id](#glossary): Unique numerical ID of the ASV
 * [asv](#glossary): ASV sequence
 

### read_count_df

Data frame with the following columns:

 * [asv](#glossary): Sequence of the ASV
 * [asv_id](#glossary): Numerical ID of the ASV
 * [sample](#glossary): Sample name
 * [replicate](#glossary): Replicate within sample (Numerical)
 * read_count: Number of reads of the ASV in the Sample-Replicate

### Reference database for taxonomic assignments

A data base is composed of two elements. A BLAST database (`blast_db`) and a `taxonomy` file.

`blast_db` can be produced using the `makeblastdb` command of BLAST:

```{bash}
makeblastdb -dbtype nucl -in [FASTA_FILE] -parse_seqids -taxid_map [TAXID_FILE] -out [DB_NAME]
```

 * FASTA_FILE is a fasta file containing reference sequences.
 * TAXID_FILE is a tab separated file with sequence IDs and the corresponding numerical [taxIDs](#glossary).
 * DB_NAME is the name of the newly created BLAST database.

`taxonomy` is a **tab separated** csv file with the following columns:

 * tax_id: Numerical Taxonomic ID. It can be a valid [NCBI taxID](https://www.ncbi.nlm.nih.gov/taxonomy), or arbitrary negative numbers for taxa not in NCBI.
 * parent_tax_id: taxID of the closest parent of tax_id.
 * rank: taxonomic rank (e.g. species, genus, subgenus, no_rank).
 * name_txt: Scientifc name of the taxon.
 * old_tax_id: taxIDs that have been merged to the tax_id by NCBI; if there is more than one for a given tax_id, make one line for each old_tax_id.
 * taxlevel: Integer associated to each major taxonomic rank. (0 => root, 1=> superkingdom, 2=> kingdom, 3=> phylum, 4=> class, 5=> order, 6=> family, 7=> genus, 8=> species). Levels in between have 0.5 added to the next highest level (e.g. 5.5 for infraorder and for superfamily).


**A ready to use COI database** in BLAST format and the associated taxonomy file can be downloaded from [https://osf.io/vrfwz/](https://osf.io/vrfwz/). It was created using [mkCOInr](https://github.com/meglecz/mkCOInr). It is also possible to make a [customized database](https://mkcoinr.readthedocs.io/en/latest/content/tutorial.html#customize-database) using mkCOInr. 



## Options

* `compress`: If TRUE, output files of the `Merge`, `RandomSeq` and `SortReads` functions are compressed. This saves space, but compressing/decompressing increases run time in some cases. I suggest to avoid compressing intermediate files and delete/compress them as soon as the analyses are finished. (See more in the [troubleshooting section](#troubleshooting)). The input fasta and fastq files of these functions can be compressed (`.gz`, `.bz2`, but NOT .zip) or uncompressed files and compression is automatically detected based on the file extension.


* `delete_tmp`: Delete automatically intermediate files. TRUE by default.

* `sep`: Separator used in sv files. Use the same separate in all CSV files except for the [taxonomy](#reference-database-for-taxonomic-assignments) file of the reference database, which is tag separated.

* `quiet`: If TRUE print as little information to the terminal as possible. TRUE by default.



## Glossary


* **ASV**: Amplicon Sequence Variant. Unique sequnece, caracterized by the number of reads in each sample-replicate.
* **asv_id**: Unique numerical ID of an ASV.
* **demultiplexing**: Sorting reads in a fasta (or fastq) file to different sample-replicates according to the tags present at their extremities.
* **dereplication**: The merged reads contain many identical sequenes. The dereplication reduces the dataset to unique sequences (ASV), and count the number of reads for each ASV in each sample-replicate.
* **data frame (or csv) in long format**: The read_count_df contains one line for each occurrence with asv_id, asv, sample, replicate, read_count columns. This is the standard format of keeping occurrences throughout the analyses, and it is smaller than the wide format, if there are many ASVs present in only one or few samples.
* **data frame (or csv) in wide format**: The read_count_df can be rearranged in wide format, where lines are ASVs, columns are sample(-replicates) and cells contain read counts. It is a more human friendly format and it is the base of writing an ASV table, where this information can be completed by taxonomic assignments and other informations.
* **false negative occurrence**: An expected ASV in a mock sample that is not found in the data.
* **false positive occurrence**: Un expected presence an ASV in a Sample.
    * all occurrences in all negative contols, 
    * unexpected occurrences in mock samples
    * presence of an ASV in an incompatible habitat (e.g. ASV with high read count in samples of habitat 1 and low read count in habitat 2 is considered as FP in habitat 2).
* **habitat**: Habitat type of the organisms in real or mock samples. Use this only if organisms of the different habitats cannot appear in another haditat of the dataset. Use NA otherwise.
* **Low Frequency Noise (LFN)**: ASVs present in low frequencies, likely to be do to errors.
* **merge**: Assemble a forward and reverse read pair to one single sequence.
* **mock sample**: An artificial mix of DNA of know organisms.
* **negative sample**: Negative control.
* **real sample**: An environmental sample.
* **replicate**: Technical or biological replicate of a sample. Replicates must have numerical identifiers. (e.g. sample tops1 have replicate 1, 2 and 3).
* **sample** Name of the environmental or control sample.
* **sample-replicate** Each sample can have technical of biological replicates. sample-replicate refers to one replicate of a given sample.
* **sample_type**: [real/mock/negative](#glossary)
* **singleton**: ASV with a single read in the whole data set.
* **tag** Short sequence at the extremity of the amplicon. It is used at the demultiplexing step to identify the sample-replicate, where the read comes from.
* **taxIDs**: Numerical taxonomic identifier. It can be a valid [NCBI taxID](https://www.ncbi.nlm.nih.gov/taxonomy), or arbitrary negative numbers for taxa not in NCBI.
* **trimming**: Cut the extremities of the sequences. It can be based of sequences quality, or on the detection of a tag or primer.
* **true positive occurrence**: An expected occurrence in a mock sample.




## Troubleshooting

### tmp_FunctionName_######## in vtamR directory

These are temporary directories, that are automatically deleted at the end of the function. In some cases, if the function ends prematurely due to an error, the directory is not deleted. Just delete them manually.

### LFNvariant eliminate most occurrences of a frequent ASV

`LFNvariant` will filter out all occurrences where

`(number of reads of the ASV in a sample-replicate) / (total number of read of the ASV) < cutoff` 

As a consequence, if an ASV is present in most samples, there are many samples in the data set and the cutoff is relatively high, most occurrences can fall bellow the cutoff and the total read count of the ASV will decrease strongly at this step.

If the proportion of the read count of an ASV in the output compared to the input is less than `min_read_count_prop`, vtamR prints out a Warning. Take a close look at the variant. It can be the result of a general contamination, and in this case, the whole ASV could/should be eliminated from the data set. Otherwise, you might want to reduce the cutoff value for `LFNvariant`.

### Memory issues for running swarm on the whole dataset

Possible workarounds:

* Try to run Swarm sample by sample (`by_sample=T`). 
* Run `global_read_count` first to eliminate singletons then swarm. 

Both solutions make swarm a bit less efficient, but at least your analyses can get through.

### Memory issues in Merge on Windows

If input fastq files are very large and do not go through `Merge`, try working with uncompressed input files.


### Files to keep

At the first steps (Merge, RandomSeq, Sortreads), many files are produced, and some of them can be very large, especially before the dereplication (before `Dereplicate`).

Compressing and decompressing between the different steps can take a while and can have memory issues in some cases (especially on Windows). Therefore, vtamR uses by default uncompressed files. This behavior can be changed by the [compress](#options) option.

Here is a list of files I would tend to keep, delete or compress.

* `Merge`: Delete the output fasta files, but keep `fastainfo.csv` for information on read counts after `Merge`.
* `RandomSeq`: Compress fasta files, but keep them to ensure reproducibility in case of re-analyse.
* `SortReads`: Delete the output fasta files, but keep `sortedinfo.csv` for information on read counts after `SortReads`. 
* `Dereplicate`: Write the dereplicated info to a file (`1_before_filter.csv` in the tutorial) and compress it after the analyses. This is an important file, since you can restart the analyses from here, without re-doing the longest merging, and demultiplexing steps. Compress and keep the updated ASV list (`ASV_list_with_IDs.csv` in the tutorial), since this can be used when analyzing subsequent data sets to synchronyse the asv_ids.
* Output of different filtering steps: Compress them if they are too large. They are useful for tracing back the history of a sample or an ASV (see [HistoryBy](#historyby)) or making summary files (see [SummarizeBy](#summarizeby)).

### No or few sequneces passing merge

If the amplicon is shorter than the reads, use `fastq_allowmergestagger=T`.


