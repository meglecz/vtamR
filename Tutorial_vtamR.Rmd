---
title: "vtamR Documentation"
auhor: Emese Meglecz
date: "2024-03-21"
output:
  html_document:
    toc: true
    toc_float: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```


## Summary

**vtamR** is a revised, completed version of [VTAM](https://onlinelibrary.wiley.com/doi/10.1111/1755-0998.13756) (Validation and Taxonomic Assignation of Metabarcoding Data) rewritten in R. It is a complete metabarcoding pipeline:

* Sequence analyses from raw fastq files of amplicon sequences till Amplicon Sequence Variant (ASV or variant) table of validated variants assigned to taxonomic groups.
* Handles technical or biological replicates of the same sample
* Uses positive and negative control samples to fine tune the filtering and reduce false positive and false negative occurrences.
* Can pool multiple data sets (resultas or earlier analyses)
* Can pool results from overlapping markers

**Novelties compared to VTAM:**

* As it is a series of R functions, vramR is highly adaptable to include, exclude and order different steps of tha analyses
* Includes swarm for denoising
* Graphic options
* Include functions to get statistics of each filtering steps (read and variant count etc.)
* The notion of marker and run has been dropped to simplify the analyses


## Implementation

The heavy lifting of sequence analyses are done by the following third party programs. They should be installed on your computer:

* BLAST([Altschul et al., 1990](https://pubmed.ncbi.nlm.nih.gov/2231712/)) (v2.11.0+)
* vsearch ([Rognes et al., 2016](https://peerj.com/articles/2584/)) (v2.7.0)
* cutadapt([Martin, 2011](https://journal.embnet.org/index.php/embnetjournal/article/view/200/479)) (v4.0)
* swarm ([Mahé et al., 2015](https://peerj.com/articles/1420/)) (v2.1.12)

vtamR has been tested using the above mentioned versions. It should work with later versions, and it is also likely to work with older ones.
vtamR was tested in Windows and Linux, and should work in all operating systems.

The help is available for all vtamR function by `?function_name`
In most cases, the input data can be either a data frame or a csv file and most function return a data frame and can also write them into a file if a file name is provided.

## Installation

### Install third party programs on Linux

* `vsearch` [https://github.com/torognes/vsearch](https://github.com/torognes/swarm)
* `BLAST` [https://www.ncbi.nlm.nih.gov/books/NBK52640/](https://www.ncbi.nlm.nih.gov/books/NBK52640/)
* `cutadapt` [https://cutadapt.readthedocs.io/en/stable/installation.html](https://cutadapt.readthedocs.io/en/stable/installation.html)
* `swarm` [https://github.com/torognes/swarm](https://github.com/torognes/swarm)

### Install third party programs on Windows

**vsearch**

* Download binaries from https://github.com/torognes/vsearch/releases/tag/v2.23.0
* Decompress the zip file
* The executable are found in `vsearch-x.xx.x-win-x86_64/bin/`

**BLAST**

* Detailed instructions: [https://www.ncbi.nlm.nih.gov/books/NBK52637/](https://www.ncbi.nlm.nih.gov/books/NBK52637/)
* Download executable (ncbi-blast-x.xx.x+-win64.exe) from [https://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/](https://ftp.ncbi.nlm.nih.gov/blast/executables/LATEST/)
* Double click on the .exe file, accept the license agreement and specify the install location when prompted. Attention! Do not accept the standard location (C:/Program Files/...) since it contains a space. Chose a directory with a path without space.
* The executables are found in `blast-x.xx.x+/bin/`

**cutadapt**

* Download `cutadapt.exe` from [https://github.com/marcelm/cutadapt/releases](https://github.com/marcelm/cutadapt/releases)  and save it a convenient place on your computer (path without space, e.g. `C:/Users/Public/`)

**swarm**

* Download binaries from [https://github.com/torognes/swarm/releases](https://github.com/torognes/swarm/releases) and save it to a convenient place on your computer (path without space, e.g. `C:/Users/Public/`)
* Unzip file
* The executable is in the `swarm-x.x.x-win-x86_64/bin` directory


### Install R packages

TODO: make an R package for vtamR and adapt the installation procedure

```{r install_pck}
install.packages("devtools")
install.packages("roxygen2")
install.packages("seqinr")
install.packages("dplyr")
install.packages("tidyr")
install.packages("ggplot2")
#devtools::install_github("meglecz/vtamR", build_vignettes = TRUE)
```

### Test your installation

**Set path to third party programs and specify the number of CPUs**
```{r set_path, eval=TRUE}
cutadapt_path <- "C:/Users/Public/"
vsearch_path <- "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path <- "C:/Users/Public/blast-2.14.1+/bin/"
swarm_path <- "C:/swarm-3.1.4-win-x86_64/bin/"
num_threads <- 4
```

**Load libraries**
```{r load_libraries}
library("devtools")
library("roxygen2")
library("seqinr")
library("dplyr")
library("tidyr")
library("ggplot2") 
#library("vtamR")
load_all(".")
roxygenise()
usethis::use_roxygen_md()
```

**Run test functions on test data**
```{r test_vtamR}
test_Merge_and_SortReads(vsearch_path=vsearch_path, cutadapt_path=cutadapt_path)
test_Filters(vsearch_path=vsearch_path, swarm_path=swarm_path)
test_MakeKnownOccurrences()
test_Optimize(vsearch_path=vsearch_path)
test_TaxAssign(blast_path=blast_path, num_threads=num_threads)
```

You should see "PASS" after the tested function names.

## Getting Started

Load libraries
```{r load_packages, warning = FALSE, message=FALSE, eval=TRUE}
library("devtools")
library("roxygen2")
library("seqinr")
library("dplyr")
library("tidyr")
library("ggplot2") 
```

Load local packages
```{r load_local_packages, eval=TRUE, message=FALSE}
load_all(".")
roxygenise()
usethis::use_roxygen_md()
```

**Set general parameters Windows**

```{r, eval=TRUE}
cutadapt_path="C:/Users/Public/"
vsearch_path = "C:/Users/Public/vsearch-2.23.0-win-x86_64/bin/"
blast_path="C:/Users/Public/blast-2.14.1+/bin/"
swarm_path <- "C:/swarm-3.1.4-win-x86_64/bin/"
num_threads <- 4
sep=","
```

**Set general parameters Bombyx**
```{r, eval=TRUE}
cutadapt_path="/home/meglecz/miniconda3/envs/vtam_2/bin/"
vsearch_path = ""
blast_path="~/ncbi-blast-2.11.0+/bin/"
swarm_path <- ""
num_threads <- 8
sep=","
```


* Adapt the path to third party programs according to your installation. If the program is in your path, it can be empty (e.g. `cutadapt_path=""`)
* `num_threads` is the number of CPUs for multithreaded programs
* `sep` is the separator used in csv files


**Taxassign reference data base**

You can download a ready to use, comprehensive COI database from [OSF](https://osf.io/vrfwz/) ([Meglécz, 2023](https://onlinelibrary.wiley.com/doi/10.1111/1755-0998.13756)). Set the path to the database and the accompanying taxonomy file.

```{r, eval=TRUE}
taxonomy="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/COInr_for_vtam_taxonomy.tsv"
blast_db="C:/Users/Public/COInr_for_vtam_2023_05_03_dbV5/COInr_for_vtam"
```

**Taxassign reference data base Bombyx**
```{r, eval=TRUE}
taxonomy="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/COInr_for_vtam_taxonomy.tsv"
blast_db="~/mkLTG/COInr_for_vtam_2022_05_06_dbV5/COInr_for_vtam"
```

* `taxonomy` CSV file file with taxonomic information
* `blast_db` BLAST database
* Details are in [Input and output files section](#input-and-output-files)


**Set input data**

```{r, eval=TRUE}
fastq_dir <- "vtamR_test/data/"
outdir <- "vtamR_test/out_mfzr/"
fastqinfo <- "vtamR_test/data/fastqinfo_mfzr.csv"
mock_composition <- "vtamR_test/data/mock_composition_mfzr.csv"
asv_list <- "vtamR_test/data/asv_list.csv"
```

* `fastq_dir` Directory containing the input fastq files
* `outdir` Name of the output directory
* `fastqinfo` CSV file file with information on input files, primers, tags, samples
* `mock_composition` CSV file file with expected ASVs in mock samples
* `asv_list` CSV file file with ASVs from earlier data sets (if any)
* Details are in [Input and output files section](#input-and-output-files)


**Check the format of input files**

The `check_fileinfo` function tests if all obligatory columns are present, and makes some sanity checks. This can be helpful, since these files are produced by the users, and may contain errors difficult to spot by eye.

```{r}
check_fileinfo(file=fastqinfo, dir=fastq_dir, file_type="fastqinfo")
check_fileinfo(file=mock_composition, file_type="mock_composition")
check_fileinfo(file=asv_list, file_type="asv_list")
```

## Merge - Demultiplex

I suppose that you have [installed third party programs and set path to them](#installation) and followed the [Getting started section](#getting-started)

According to your wetlab and sequencing protocol fastq files can contain one or more sample-replicate, and sequences may or may not contain tags (for demultiplexing) and primer sequences. In the following sections I show 3 different scenarios to obtain the [read_count_df](#input-and-output-files), the input data frame to the filtering steps.

### One sample per fastq - no tag - no primer

Read pairs should be quality filtered merged and written to fasta format.
This can be done by the `Merge` function. 
See the help (`?Merge`) for setting the correct parameters for quality filtering.

* [fastqinfo](#input-and-output-files) is either a csv file, or a data frame, with the following columns: tag_fw, primer_fw, tag_rv, primer_rv, sample, sample_type, habitat, replicate, fastq_fw, fastq_rv. The tag_fw, primer_fw, tag_rv, primer_rv are irrelevant in this case, just fill them with NA.
* `fastq_dir` is the directory containing the input fastq files

```{r}
merged_dir <- paste(outdir, "merged/", sep="")
fastainfo_df <- Merge(fastqinfo, fastq_dir=fastq_dir, vsearch_path=vsearch_path, outdir=merged_dir)
```


### One sample per fastq - primer - no tag

Read pairs should be quality filtered merged and written to fasta format as in the [previous section](#one-sample-per-fastq---no-tag---no-primer). The only difference is that you should fill the `primer_fw` and the `primer_rv` columns of the [fastqinfo](#input-and-output-files) file.

Then the `TrimPrimer` function will trim the primers from the reads. 
See the help (`?TrimPrimer`) for setting the correct parameters for primer trimming.

* [fastainfo_df](#input-and-output-files) is the output of `Merge`
* `fasta_dir` is the directory containing he input fasta files
* If `check_reverse` is TRUE, `TrimPrimer` checks the reverse complementary stand as well (FALSE by default)

```{r}
# merge an quality filter
merged_dir <- paste(outdir, "merged/", sep="")
fastainfo_df <- Merge(fastqinfo, fastq_dir=fastq_dir, vsearch_path=vsearch_path, outdir=merged_dir)

# trim primers
sorted_dir <- paste(outdir, "sorted/", sep="")
sortedinfo_df <- TrimPrimer(fastainfo_df, fasta_dir=merged_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path, check_reverse=T)
```

### Several samples per fastq - tags - primers

Read pairs should be quality filtered, merged and written to fasta format as in the [previous sections](#one-sample-per-fastq---no-tag---no-primer). The only difference is that you should fill the `tag_fw`, `primer_fw`, `tag_rv` and `primer_rv` columns of the [fastqinfo](#input-and-output-files) file.

Then the `SortReads` function will demultiplex the fasta files according to the tag combinations and trim the primers from the reads. See the help (`?SortReads`) for setting the correct parameters for demultiplexing and primer trimming:.

* [fastainfo_df](#input-and-output-files) is the output of `Merge`
* `fasta_dir` is the directory containing he input fasta files
* If `check_reverse` is TRUE, `SortReads` checks the reverse complementary stand as well (FALSE by default)

```{r}
# merge and quality filter
merged_dir <- paste(outdir, "merged/", sep="")
fastainfo_df <- Merge(fastqinfo, fastq_dir=fastq_dir, vsearch_path=vsearch_path, outdir=merged_dir)

# demultiplex, trim tags and pimers
sorted_dir <- paste(outdir, "sorted/", sep="")
sortedinfo_df <- SortReads(fastainfo_df, fasta_dir=merged_dir, outdir=sorted_dir, cutadapt_path=cutadapt_path, vsearch_path=vsearch_path)
```


## Dereplicate

I suppose that you have [installed third party programs and set path to them](#installation) and followed the [Getting started section](#getting-started), and you have one fasta file per sample-replicate containing merged reads. Tags and primers have been removed from the reads.

The fasta files then should be dereplicated and a numerical ID (asv_id) assigned to each ASV. The output is a data frame ([read_count_df](#input-and-output-files)). This can also be written to a csv file if `outfile` is given. It is a good idea to write this file, since it is the starting point of data filtering. You can experiment with different filtering strategies starting from this file, without the need of re-running the long steps of merging and filtering.


If you want to keep the IDs of ASVs from earlier analyses, you can give a file containing earlier ASVs and their IDs ([asv_list](#input-and-output-files)). The completed ASV list is written to `updated_asv_list` file.

Giving only the file name to `updated_asv_list`, but not to `asv_list`, the function will write the complete list of ASVs. This can be useful later when analyzing subsequent data sets.


```{r}
outfile <- paste(outdir, "1_before_filter.csv", sep="")
updated_asv_list <- paste(outdir, "ASV_list_with_IDs.csv", sep="")

read_count_df <- read_fastas_from_sortedinfo(sortedinfo_df, dir=sorted_dir, outfile=outfile, asv_list=asv_list, updated_asv_list=updated_asv_list)
```


## Filter

vtamR has a large number of functions to filter out ASVs or occurrences of ASVs in a given sample-replicate, which are the results of technical or biological problems associated with metabarcoding. The different filters can be applied to the original data set and then the results pooled together (accepting only occurrences that pass all filters) or can be applied sequentially in the order defined according to your needs or constraints (like computing capacity).

In this section I give an example using most filtering options. It is up to you to construct your own pipeline.

Each function returns a data frame with the filtered output. You can also write the results to a csv file. This can be useful of tracking the presence/absence of some of the ASVs/samples throughout the analyses using different [helper functions](#helper-functions). I will start the names of the output files by a number to keep the order of the different steps and allow the `summarize_by` [helper function](#helper-functions) to make final statistics at each step of the analyses.

We can also follow the evolution of the number of reads and ASVs remaining in the data set after each filtering steps. Let's define a data frame (`stat_df`), that will contain this information, and we will complete after each step, and add line with information on the unfiltered data set.

`params` is a string containing the major parameters of each filtering steps.

`stage` is a string referring to the filtering step.

```{r}
stat_df <- data.frame(parameters=character(),
                      asv_count=integer(),
                      read_count=integer(),
                      sample_count=integer(),
                      sample_replicate_count=integer())

stat_df <- get_stat(read_count_df, stat_df, stage="Input", params=NA)
```

### Denoising by swarm

Swarm ([Mahé et al., 2015](https://peerj.com/articles/1420/)) is a powerful and quick computer program to denoise the data set by clustering variants that are likely to be the result of sequencing errors to ASVs representing real biological sequences. It will considerably reduce the number of ASVs.

By default, vtamR uses 1 as swam's d and the fastidious algorithm. See `?Swarm` on how to change this.
It is possible to run Swarm separately for each sample (`by_sample=TRUE`; default) or at once for the whole data set. This second option is more efficient in reducing the number of ASVs but can cluster very similar real sequences appearing in different samples. Is you are interested in intra-specific variability, I suggest `by_sample=TRUE`.


```{r}
by_sample <- TRUE
outfile <- paste(outdir, "2_Swarm_by_sample.csv", sep="")

read_count_df <- Swarm(read_count_df, outfile=outfile, swarm_path=swarm_path, num_threads=num_threads, by_sample=by_sample)

stat_df <- get_stat(read_count_df, stat_df, stage="Swarm", params=by_sample)
```

### LFN_global_read_count

Although, swarm has reduced considerably the number of ASVs, there are still many ASVs with low read count. The LFN_global_read_count function filter out all ASVs with total read count bellow a threshold.

Let's eliminate singletons:

```{r}
global_read_count_cutoff = 2
outfile <- paste(outdir, "3_LFN_global_read_count.csv", sep="")

read_count_df <- LFN_global_read_count(read_count_df, cutoff=global_read_count_cutoff, outfile=outfile)

stat_df <- get_stat(read_count_df, stat_df, stage="LFN_global_read_count", params=global_read_count_cutoff)
```

### FilerIndel

This filter is applicable only for coding sequences. The idea is that if the length of the ASV differs from the most frequent ASV length by a number that is not a multiple of 3, it is likely an erroneous sequence or a pseudo-gene.

```{r}
outfile <- paste(outdir, "4_FilterIndel.csv", sep="")

read_count_df <- FilterIndel(read_count_df, outfile=outfile)

stat_df <- get_stat(read_count_df, stat_df, stage="FilterIndel")
```


### FilterCodonStop

This filter is applicable only for coding sequences. It checks the number of codon STOPs in all 3 reading frames, and eliminates ASV with STOP in all of them.

The Genetic code can be chosen from [here](https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi?chapter=cgencodes). By default, it is the invertebrate mitochondrial genetic code (5)


```{r}
outfile <- paste(outdir, "5_FilterCodonStop.csv", sep="")
genetic_code = 5

read_count_df <- FilterCodonStop(read_count_df, outfile=outfile, genetic_code=genetic_code)

stat_df <- get_stat(read_count_df, stat_df, stage="FilerCodonStop", params=genetic_code)
```

### FilterChimera

This function will run the `uchime3_denovo` function implemented in vsearch. 
It can be run sample by sample (`by_sample=T`; default), or once in the whole data set. If `by_sample=T`, ASVs are eliminated if they have been identified as chimeras in `sample_prop` proportion of the samples where they are present.

`abskew` A chimera must be at least abskew times less frequent that the parental ASVs


```{r}
abskew=2
by_sample = T
sample_prop = 0.8
outfile <- paste(outdir, "6_FilterChimera.csv", sep="")

read_count_df <- FilterChimera(read_count_df, outfile=outfile, vsearch_path=vsearch_path, by_sample=by_sample, sample_prop=sample_prop, abskew=abskew)

params <- paste(abskew, by_sample, sample_prop, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilterChimera", params=params)
```

### FilterRenkonen

Let's eliminate aberrant replicates that are not similar to other replicates of the same sample. 

We will calculate the renkonen distance among all replicates within sample and plot them.

```{r}
# calculate renkonen distance among all replicates within sample 
renkonen_within_df <- make_renkonen_distances(read_count_df, compare="within")
# density plot
renkonen_density_plot <- density_plot_renkonen_distance(renkonen_within_df)
print(renkonen_density_plot)
# barplot
sortedinfo <- paste(sorted_dir, "sortedinfo.csv", sep="")
renkonen_barplot <- barplot_renkonen_distance(renkonen_within_df, sample_types=sortedinfo, x_axis_label_size=6)
print(renkonen_barplot)
```

In this example, we have only a few distances, so the density plot is not very relevant. The barplot indicates that the replicate 1 of the tnegtag sample (negative control) is distant from the other two replicates. Lets, establish the Renkonen cutoff value to 0.4. We will filter out replicates that have Renkonen distances above this cutoff to most other replicates of the sample. In this case replicate 1 on the tnegtag sample will be eliminated

```{r}
outfile <- paste(outdir, "7_FilterRenkonen.csv", sep="")
cutoff <- 0.4

read_count_df <- FilterRenkonen(read_count_df, outfile=outfile, cutoff=cutoff)

stat_df <- get_stat(read_count_df, stat_df, stage="FilerRenkonen", params=cutoff)
```

### Make mock_composition file

If you do not know the exact amplicon sequences of the individuals in your mock sample, this is the time to find them among the ASVs of `read_count_df`. This will be necessary for optimizing the Low Frequency Noise filters, and evaluate precision and accuracy of the filtering.

**Assign taxa to ASVs**
See details of taxonomic assignment [here](#taxassign)
```{r}
outfile <- paste(outdir, "ASV_taxa.csv", sep="")
asv_tax <- TaxAssign(asv=read_count_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, num_threads=num_threads, outfile=outfile)
```

**Pool replicates by sample**
```{r}
tmp_read_count_samples_df <- PoolReplicates(read_count_df, digits=0, outfile=outfile, sep=sep)
```

**Make an ASV table with taxonomic assignments**

We will add to the data frame the total number of reads for each ASV and the number of samples they are present.
See details of `WriteAsVtable` [here](#writeasvtable)
```{r}
sortedinfo <- paste(sorted_dir, "sortedinfo.csv", sep ="")
tmp_asv_table <- WriteASVtable(tmp_read_count_samples_df, sortedinfo=sortedinfo, add_sums_by_asv=T, asv_tax=asv_tax)
```

If there are many samples it might be better to select only mock samples and pertinent columns.
```{r}
asv_tpos1 <- tmp_asv_table %>%
  select(tpos1, Total_number_of_reads, Number_of_samples, asv_id, phylum, class, order, family, genus, species, asv) %>%
  filter(tpos1 > 0) %>%
  arrange(desc(tpos1))
```

Yous can now pick the correct sequences of the expected ASVs in each mock and make the [mock_composition](#input-and-output-files) file

### FilterPCRerror

If you have done the denoising step by swarm, this function is probably redundant. Let's see, if there are still potential PCR errors in mock samples. 

**OptimizePCRError**

`OptimizePCRError`, will find all highly similar ASV pairs (`max_mismatch=1` by default) within a mock sample, where one ASV is expected, and the other is not. Their read_count ratio is printed in the output file. 

The function considers only ASVs with more then `min_read_count` reads in the sample to avoid count a ratio based on low read counts that are more influenced by stochastic events and will be probably filtered out anyway. See details using the help: `?OptimizePCRError`

```{r}
outfile <- paste(outdir, "OptimizePCRError.csv", sep="")
OptimizePCRError_df <- OptimizePCRError(read_count_df, mock_composition=mock_composition, outfile=outfile, max_mismatch=2, min_read_count=5)
```

**FilterPCRerror**

It seems that Swarm has done a good job. The highest read count ratio is 0.03 which is way bellow the default value of `FilterPCRerror` (0.1). Let's run `FilterPCRerror`. See the help for more detail `?FilterPCRerror`.

```{r}
pcr_error_var_prop <- 0.1
max_mismatch <- 2
outfile <- paste(outdir, "8_FilterPCRerror.csv", sep="")

read_count_df <- FilterPCRerror(read_count_df, outfile=outfile, vsearch_path=vsearch_path, pcr_error_var_prop=pcr_error_var_prop, max_mismatch=max_mismatch)

params <- paste(pcr_error_var_prop, max_mismatch, by_sample, sep=";")
stat_df <- get_stat(read_count_df, stat_df, stage="FilerPCRerror", params=params)
```

### LFN_sample_replicate

`LFN_sample_replicate` will eliminate occurrences with very low read counts compared to the total number of reads in the sample-replicate. The default cutoff proportion is 0.001. 
We can have an idea of the maximum value of this cutoff, by examining the proportions of expected ASVs in the mock samples using the `OptimizeLFNsampleReplicate` function

**OptimizeLFNsampleReplicate**

```{r}
outfile = paste(outdir, "OptimizeLFNsampleReplicate.csv", sep="")
OptimizeLFNsampleReplicate_df <- OptimizeLFNsampleReplicate(read_count=read_count_df, mock_composition=mock_composition, outfile=outfile)
```

**LFN_sample_replicate**

We can see in the output, that the lowest proportion of an expected ASV is 0.005. Let's use this cutoff for the `LFN_sample_replicate` filter

```{r}
lfn_sample_replicate_cutoff <- 0.005
outfile <- paste(outdir, "9_LFN_sample_replicate.csv", sep="")

read_count_df_lnf_sample_replicate <- LFN_sample_replicate(read_count_df, cutoff=lfn_sample_replicate_cutoff, outfile=outfile)

stat_df <- get_stat(read_count_df_lnf_sample_replicate, stat_df, stage="LFN_sample_replicate", params=lfn_sample_replicate_cutoff)
```

### FilterMinReplicateNumber_1

To ensure repeatability, we can accept occurrences if they are present in at least `min_replicate_number` replicates of the sample (2 by default).

```{r}
## LFN_variant
# Set parameter values
min_replicate_number <- 2
outfile <- paste(outdir, "10_FilterMinReplicateNumber.csv", sep="")
# Run filter and get stats
read_count_df <- FilterMinReplicateNumber(read_count_df, min_replicate_number, outfile=outfile)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterMinReplicateNumber", params=min_replicate_number)
```

### LFN_variant and LFN_read_count

The `LFN_variant` filter will eliminate occurrences with very low read counts compared to the total number of reads of the ASV. The default cutoff proportion is 0.001. This is an important filter, to filter out occurrences present in the data set due to tag-jump.

The `LFN_read_count` filter simply eliminates occurrences with less then `lfn_read_count_cutoff` reads (10 by default). 

To find the best cutoff values for these filters, we will count the number of [false positive](#glossary), [false negative](#glossary) and [true positive](#glossary) occurrences using a series of combination of the cutoff values of these two filters.

**MakeKnownOccurrences**

The MakeKnownOccurrences function will identify [false positive](#glossary), [false negative](#glossary) and [true positive](#glossary) occurrences in controls samples (mock and negative). Some false positives can also be identified in real samples if samples of different habitats are included in the data sets. 

The false positive and the true positive occurrences are written to the known_occurrences data frame (or file), false negatives to the missing_occurrences data frame (or file), and a performance_metrics data frame is also produced with the count of these occurrences. For details see `?MakeKnownOccurrences`.

This function takes a read_count_samples data frame as an input, where the replicates of the same sample have been pooled (see `?PoolReplicates`)


```{r}
# Pool replicates
read_count_samples_df <- PoolReplicates(read_count_df)

# Detect known occurrences
results <- MakeKnownOccurrences(read_count_samples = read_count_samples_df, sortedinfo=sortedinfo, mock_composition=mock_composition)

# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
```


**OptimizeLFNReadCountAndLFNvariant**

The the `LFN_read_count` and `LFN_variant` is run for a series of cutoff value combinations of the two filters, followed by FilterMinReplicateNumber. For each parameter combination the number of FN, TP, and FP is reported. Chose the parameter setting that minimizes, FN and FP.

Here we will use the default range of cutoffs to test, but you can set the minimum, the maximum and the increment for the cutoff values for both filters. (see `?OptimizeLFNReadCountAndLFNvariant`). We set `min_replicate_number` to 2 (1 by default), to eliminate non-repeatable occurrences among replicates of the sample.


```{r}
outfile = paste(outdir, "OptimizeLFNReadCountAndLFNvariant.csv", sep="")

OptimizeLFNReadCountAndLFNvariant_df <- OptimizeLFNReadCountAndLFNvariant(read_count_df, known_occurrences=known_occurrences_df, outfile= outfile, min_replicate_number=2)
```

**LFN_variant, LFN_read_count**

From the output, it we choose 0.001 for the cutoff of `LFN_variant` and 85 for `LFN_read_count`.

We will run the two filters on the same input data frame (for which the parameters has been optimized), and pool the results by accepting only occurrences that pass both filters. 

See `?LFN_variant` for details.

```{r}
## LFN_variant
# Set parameter values
lnf_variant_cutoff = 0.001
outfile <- paste(outdir, "11_LFN_variant.csv", sep="")
# Run filter and get stats
read_count_df_lnf_variant <- LFN_variant(read_count_df, cutoff=lnf_variant_cutoff, outfile=outfile)
stat_df <- get_stat(read_count_df_lnf_variant, stat_df, stage="LFN_variant", params=lnf_variant_cutoff)

## LFN_read_count
# Set parameter values
lfn_read_count_cutoff <- 85
outfile <- paste(outdir, "12_LFN_read_count.csv", sep="")
# Run filter and get stats
read_count_df_lfn_read_count <- LFN_read_count(read_count_df, cutoff=lfn_read_count_cutoff, outfile=outfile)
stat_df <- get_stat(read_count_df_lfn_read_count, stat_df, stage="LFN_read_count", params=lfn_read_count_cutoff)

## Combine results
# Set parameter values
outfile <- paste(outdir, "13_pool_LFN.csv", sep="")
# Combine results and get stats
read_count_df <- pool_LFN(read_count_df_lfn_read_count, read_count_df_lnf_variant, outfile=outfile)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterLFN")
# delete temporary data frames
rm(read_count_df_lfn_read_count)
rm(read_count_df_lnf_variant)
```

### FilterMinReplicateNumber_2

Let's run again FilterMinReplicateNumber to ensure repeatability among replicates of the sample (2 by default).

```{r}
## LFN_variant
# Set parameter values
min_replicate_number <- 2
outfile <- paste(outdir, "14_FilterMinReplicateNumber.csv", sep="")
# Run filter and get stats
read_count_df <- FilterMinReplicateNumber(read_count_df, min_replicate_number, outfile=outfile)
stat_df <- get_stat(read_count_df, stat_df, stage="FilterMinReplicateNumber", params=min_replicate_number)
```

### PoolReplicates

Let's pool replicates of the same sample. `PoolReplicates` function will take the mean non-zero read counts of each ASV over replicates of the same sample.

```{r}
# Set parameter values
outfile <- paste(outdir, "15_PoolReplicates.csv", sep="")
read_count_samples_df <- PoolReplicates(read_count_df, outfile=outfile)
# Run function and get stats
stat_df <- get_stat(read_count_samples_df, stat_df, stage="PoolReplicates")
```

### Get performance metrics

Run MakeKnownOccurrences again to get performance metrics (FP, FN, FP). 
This time we will write the output data frames to files as well. The `performance_metrics` file will give you the count of FP, FN, TP, accuracy and sensitivity, you can found false negatives in `missing_occurrences`, and true and false positives in `known_occurrences`.

```{r}
# Set parameter values
missing_occurrences <- paste(outdir, "missing_occurrences.csv", sep= "")
performance_metrics <- paste(outdir, "performance_metrics.csv", sep= "")
known_occurrences <- paste(outdir, "known_occurrences.csv", sep= "")
sortedinfo <- paste(sorted_dir, "sortedinfo.csv", sep ="")
# Run function
results <- MakeKnownOccurrences(read_count_samples_df, sortedinfo=sortedinfo, mock_composition=mock_composition, known_occurrences=known_occurrences, missing_occurrences=missing_occurrences, performance_metrics=performance_metrics)
# give explicit names to the 3 output data frames
known_occurrences_df <- results[[1]]
missing_occurrences_df <- results[[2]]
performance_metrics_df <- results[[3]]
```

### TaxAssign

If you haven't assigned ASV to taxa yet (before the optimization step), it is time to do it. If you have already done it, you can use the same output as previously and skip this step.

For the format of `taxonomy` and `blast_db` check the [Input and output files](#input-and-output-files) section
taxonomy
blast_db

```{r}
# Set parameter values
outfile <- paste(outdir, "TaxAssign.csv", sep="")
# Run function
asv_tax <- TaxAssign(asv=read_count_samples_df, taxonomy=taxonomy, blast_db=blast_db, blast_path=blast_path, outfile=outfile, num_threads=num_threads)
```

### Print output

`WriteAsVtable` will reorganize the `read_count_samples_df` data frame, with samples in columns and ASV in lines. 

It is possible to add supplementary informations as well:

* Taxonomic assignment (`asv_tax`)
* Total number of reads and samples for each ASV (`add_sums_by_asv`)
* Total number of reads and ASVs in each sample (`add_sums_by_sample`)
* Supplementary column per mock samples with expected occurrences in each of them (`add_expected_asv`)
* Supplementary column for samples that have been filtered out (`add_empty_samples`)

For more information see `?WriteAsVtable`

```{r}
# write ASV table completed by taxonomic assignments
outfile=paste(outdir, "Final_asvtable_with_taxassign.csv", sep="")

asv_table_df <- WriteASVtable(read_count_samples_df, outfile=outfile, asv_tax=asv_tax, sortedinfo=sortedinfo, add_empty_samples=T, add_sums_by_sample=T, add_sums_by_asv=T, add_expected_asv=T, mock_composition=mock_composition)
```

Print out the number of reads, ASVs, samples and replicates after each step.

```{r}
write.csv(stat_df, file = paste(outdir, "stat_steps.csv", sep=""))
```

## Helper functions

 RandomSeq
 history_by
 summarize_by
 pool_datasets
 count_reads_dir
 
 barplot_read_count_by_sample
 histogram_read_count_by_variant
 barplot_renkonen_distance
 

## Input and output files

### Reference database for taxonomic assignments

A data base is composed of two elements. A BLAST database (`blast_db`) and a `taxonomy` file.

`blast_db` can produced by a `makeblastdb` of BLAST:

```{bash}
makeblastdb -dbtype nucl -in [FASTA_FILE] -parse_seqids -taxid_map [TAXID_FILE] -out [DB_NAME]
```

FASTA_FILE is a fasta fie containing reference sequences
TAXID_FILE is a tab separated file with sequence IDs and the corresponding numerical taxIDs
DB_NAME is the name of the newly created BLAST database.

`taxonomy` is a tab separated csv file with the following columns:

* tax_id: tax_id: can be correct NCBI taxID, or arbitrary negative numbers for taxa not in NCBI
* parent_tax_id: taxiID of the closest parent of tax_id
* rank: taxonomic rank (e.g. species, genus, subgenus, no_rank) 
* name_txt: Scientifc name of the taxon 
* old_tax_id: taxIDs that have been merged to the tax_id by NCBI; if more than one for a given tax_id, make one line for each old_tax_id
* taxlevel: Integer associated to a major taxonomic rank. (0 => root, 1=> superkingdom, 2=> kingdom, 3=> phylum, 4=> class, 5=> order, 6=> family, 7=> genus, 8=> species). Levels in between have 0.5 added to the next highest level (e.g. 5.5 for infraorder and for superfamily).


A ready to use COI database and the associated taxonomy file can be downloaded from [https://osf.io/vrfwz/](https://osf.io/vrfwz/). It was created usin [mkCOInr](https://github.com/meglecz/mkCOInr).



* `fastqinfo` CSV file file with information on input files, primers, tags, samples tag_fw,primer_fw,tag_rv,primer_rv,sample,sample_type,habitat,replicate,fastq_fw,fastq_rv (see details xxxx)
* `mock_composition` CSV file file with the following columns: sample,action,asv,taxon,asv_id (see details xxxx)
* `asv_list` CSV file file with the following columns: asv_id,asv (see details xxxx)

* read_count_df

* `compress`: If TRUE, output files of the `Merge`, `RandomSeq` and `SortReads` functions are compressed. This saves space, but compressing/decompressing increases run time in some cases. I suggest to avoid compressing intermediate files and delete/compress them as soon as the analyses are finished. (See more in the [troubleshooting section](#troubleshooting)). The input fasta and fastq files of these functions can be compressed (`.gz`, `.bz2`, but NOT .zip) or uncompressed files and compression is automatically detected based on the file extension.

## Glossary

* False positive occurrence
* False negative occurrence
* True positive occurrence


## Troubleshooting
Explain min_read_count_prop for LFN_variant

If pb of running swarm for the whole dataset try to run it sample by sample. If still problem, do global_read_count first, then swarm


